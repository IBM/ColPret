model_name,steps,timing/compilation_seconds,accuracy,cross_ent_loss,cross_ent_loss_per_all_target_tokens,effective_batch_size/decoder,effective_batch_size/encoder,loss,loss_per_all_target_tokens,loss_per_nonpadding_target_token,non_padding_fraction/decoder,non_padding_fraction/encoder,non_padding_fraction/loss_weights,non_padding_fraction/overall,timing/seconds,timing/seqs,timing/seqs_per_second,timing/seqs_per_second_per_core,timing/steps_per_second,timing/target_tokens_per_second,timing/target_tokens_per_second_per_core,z_loss,z_loss_per_all_target_tokens,val_perplexity,config,timing/init_or_restore_seconds,timing/evaluate_seconds,timing/train_iter_warmup,learning_rate,learning_rate/current,timing/uptime,timing/checkpoint_seconds
xxl,0,52.162994,1.8657654e-05,2534267.2,10.854694,2048.0,2048.0,2537024.0,10.866502,10.88159,0.9986135,0.99859595,0.9986135,0.9985992,100.6149,204800.0,2035.4838,7.9511085,0.99388856,232045.14,906.42633,2756.624,0.011807086,1.8308900845712672,"b""    from __gin__ import dynamic_registration\n    import __main__ as train_script\n    import data.pile.tasks\n    from models.scalable_t5 import network\n    import seqio\n    from t5x import adafactor\n    from t5x import gin_utils\n    from t5x import models\n    from t5x import partitioning\n    from t5x import trainer\n    from t5x import utils\n    \n#### Macros:\n\n    BATCH_SIZE = 2048\n    DROPOUT_RATE = 0.0\n    EVAL_BATCH_SIZE = 256\n    LABEL_SMOOTHING = 0.0\n    LOSS_NORMALIZING_FACTOR = None\n    MIXTURE_OR_TASK_MODULE = None\n    MIXTURE_OR_TASK_NAME = 'pile_r_denoiser'\n    MODEL = @models.EncoderDecoderModel()\n    MODEL_DIR = 'gs://improved-t5/ckpts/v2_xxl_mlm/'\n    OPTIMIZER = @adafactor.Adafactor()\n    RANDOM_SEED = None\n    SAVING_PERIOD = 10000\n    SHUFFLE_TRAIN_EXAMPLES = True\n    TASK_FEATURE_LENGTHS = {'inputs': 512, 'targets': 114}\n    TRAIN_STEPS = 1000000\n    USE_CACHED_TASKS = False\n    USE_HARDWARE_RNG = False\n    VOCABULARY = @seqio.SentencePieceVocabulary()\n    Z_LOSS = 0.0001\n    \n#### Parameters for adafactor.Adafactor:\n\n    adafactor.Adafactor.decay_rate = 0.8\n    adafactor.Adafactor.logical_factor_rules = \\\n        @adafactor.standard_logical_factor_rules()\n    adafactor.Adafactor.step_offset = 0\n    \n#### Parameters for utils.CheckpointConfig:\n\n    utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()\n    utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()\n    \n#### Parameters for utils.create_learning_rate_scheduler:\n\n    utils.create_learning_rate_scheduler.base_learning_rate = 1.0\n    utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'\n    utils.create_learning_rate_scheduler.warmup_steps = 10000\n    \n#### Parameters for infer_eval/utils.DatasetConfig:\n\n    infer_eval/utils.DatasetConfig.batch_size = %EVAL_BATCH_SIZE\n    infer_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    infer_eval/utils.DatasetConfig.seed = 0\n    infer_eval/utils.DatasetConfig.shuffle = False\n    infer_eval/utils.DatasetConfig.split = 'validation'\n    infer_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    \n#### Parameters for train/utils.DatasetConfig:\n\n    train/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train/utils.DatasetConfig.pack = True\n    train/utils.DatasetConfig.seed = None\n    train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES\n    train/utils.DatasetConfig.split = 'train'\n    train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for train_eval/utils.DatasetConfig:\n\n    train_eval/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train_eval/utils.DatasetConfig.pack = True\n    train_eval/utils.DatasetConfig.seed = 42\n    train_eval/utils.DatasetConfig.shuffle = False\n    train_eval/utils.DatasetConfig.split = 'validation'\n    train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for models.EncoderDecoderModel:\n\n    models.EncoderDecoderModel.input_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.label_smoothing = %LABEL_SMOOTHING\n    models.EncoderDecoderModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR\n    models.EncoderDecoderModel.module = @network.Transformer()\n    models.EncoderDecoderModel.optimizer_def = %OPTIMIZER\n    models.EncoderDecoderModel.output_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.z_loss = %Z_LOSS\n    \n#### Parameters for seqio.Evaluator:\n\n    seqio.Evaluator.logger_cls = \\\n        [@seqio.PyLoggingLogger, @seqio.TensorBoardLogger, @seqio.JSONLogger]\n    seqio.Evaluator.num_examples = 2048\n    \n#### Parameters for partitioning.PjitPartitioner:\n\n    partitioning.PjitPartitioner.logical_axis_rules = \\\n        @partitioning.standard_logical_axis_rules()\n    partitioning.PjitPartitioner.model_parallel_submesh = (1, 1, 4, 1)\n    partitioning.PjitPartitioner.num_partitions = 1\n    \n#### Parameters for utils.RestoreCheckpointConfig:\n\n    utils.RestoreCheckpointConfig.path = []\n    \n#### Parameters for utils.SaveCheckpointConfig:\n\n    utils.SaveCheckpointConfig.dtype = 'float32'\n    utils.SaveCheckpointConfig.keep = None\n    utils.SaveCheckpointConfig.period = %SAVING_PERIOD\n    utils.SaveCheckpointConfig.save_dataset = False\n    \n#### Parameters for seqio.SentencePieceVocabulary:\n\n    seqio.SentencePieceVocabulary.extra_ids = 100\n    seqio.SentencePieceVocabulary.sentencepiece_model_file = \\\n        'gs://improved-t5/vocabs/tokenizer.model'\n    \n#### Parameters for network.T5Config:\n\n    network.T5Config.dropout_rate = %DROPOUT_RATE\n    network.T5Config.dtype = 'bfloat16'\n    network.T5Config.emb_dim = 4096\n    network.T5Config.head_dim = 64\n    network.T5Config.logits_via_embedding = False\n    network.T5Config.mlp_activations = ('gelu', 'linear')\n    network.T5Config.mlp_dim = 10240\n    network.T5Config.num_decoder_layers = 24\n    network.T5Config.num_encoder_layers = 24\n    network.T5Config.num_heads = 64\n    network.T5Config.remat_policy = 'minimal'\n    network.T5Config.scan_layers = True\n    network.T5Config.vocab_size = 32128\n    \n#### Parameters for train_script.train:\n\n    train_script.train.checkpoint_cfg = @utils.CheckpointConfig()\n    train_script.train.eval_period = %SAVING_PERIOD\n    train_script.train.eval_steps = 100\n    train_script.train.infer_eval_dataset_cfg = @infer_eval/utils.DatasetConfig()\n    train_script.train.inference_evaluator_cls = @seqio.Evaluator\n    train_script.train.model = %MODEL\n    train_script.train.model_dir = %MODEL_DIR\n    train_script.train.partitioner = @partitioning.PjitPartitioner()\n    train_script.train.random_seed = %RANDOM_SEED\n    train_script.train.run_eval_before_training = True\n    train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config\n    train_script.train.total_steps = %TRAIN_STEPS\n    train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()\n    train_script.train.train_eval_dataset_cfg = @train_eval/utils.DatasetConfig()\n    train_script.train.trainer_cls = @trainer.Trainer\n    train_script.train.use_hardware_rng = %USE_HARDWARE_RNG\n    \n#### Parameters for trainer.Trainer:\n\n    trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()\n    trainer.Trainer.num_microbatches = None\n    \n#### Parameters for network.Transformer:\n\n    network.Transformer.config = @network.T5Config()""",39.670086,7.410404,2.0742416e-05,,,,
xxl,1170000,21.832785,-0.9984468,148373.33,-0.75691134,2048.0076,2048.0076,148383.17,-0.7569615,-0.7556853,1.0016888,0.99206066,1.0016888,1.2850294,13959.778,20480000.0,1467.0719,2.8653748,0.7163437,-140421.08,326.65274,9.992647,-5.0976465e-05,1.0340469455583143,"b""    from __gin__ import dynamic_registration\n    import __main__ as train_script\n    import data.pile.tasks\n    from models.scalable_t5 import network\n    import seqio\n    from t5x import adafactor\n    from t5x import gin_utils\n    from t5x import models\n    from t5x import partitioning\n    from t5x import trainer\n    from t5x import utils\n    \n#### Macros:\n\n    BATCH_SIZE = 2048\n    DROPOUT_RATE = 0.0\n    EVAL_BATCH_SIZE = 256\n    LABEL_SMOOTHING = 0.0\n    LOSS_NORMALIZING_FACTOR = None\n    MIXTURE_OR_TASK_MODULE = None\n    MIXTURE_OR_TASK_NAME = 'pile_r_denoiser'\n    MODEL = @models.EncoderDecoderModel()\n    MODEL_DIR = 'gs://improved-t5/ckpts/v2_xxl_mlm/'\n    OPTIMIZER = @adafactor.Adafactor()\n    RANDOM_SEED = None\n    SAVING_PERIOD = 10000\n    SHUFFLE_TRAIN_EXAMPLES = True\n    TASK_FEATURE_LENGTHS = {'inputs': 512, 'targets': 114}\n    TRAIN_STEPS = 2000000\n    USE_CACHED_TASKS = False\n    USE_HARDWARE_RNG = False\n    VOCABULARY = @seqio.SentencePieceVocabulary()\n    Z_LOSS = 0.0001\n    \n#### Parameters for adafactor.Adafactor:\n\n    adafactor.Adafactor.decay_rate = 0.8\n    adafactor.Adafactor.logical_factor_rules = \\\n        @adafactor.standard_logical_factor_rules()\n    adafactor.Adafactor.step_offset = 0\n    \n#### Parameters for utils.CheckpointConfig:\n\n    utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()\n    utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()\n    \n#### Parameters for utils.create_learning_rate_scheduler:\n\n    utils.create_learning_rate_scheduler.base_learning_rate = 1.0\n    utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'\n    utils.create_learning_rate_scheduler.warmup_steps = 10000\n    \n#### Parameters for infer_eval/utils.DatasetConfig:\n\n    infer_eval/utils.DatasetConfig.batch_size = %EVAL_BATCH_SIZE\n    infer_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    infer_eval/utils.DatasetConfig.seed = 0\n    infer_eval/utils.DatasetConfig.shuffle = False\n    infer_eval/utils.DatasetConfig.split = 'validation'\n    infer_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    \n#### Parameters for train/utils.DatasetConfig:\n\n    train/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train/utils.DatasetConfig.pack = True\n    train/utils.DatasetConfig.seed = None\n    train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES\n    train/utils.DatasetConfig.split = 'train'\n    train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for train_eval/utils.DatasetConfig:\n\n    train_eval/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train_eval/utils.DatasetConfig.pack = True\n    train_eval/utils.DatasetConfig.seed = 42\n    train_eval/utils.DatasetConfig.shuffle = False\n    train_eval/utils.DatasetConfig.split = 'validation'\n    train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for models.EncoderDecoderModel:\n\n    models.EncoderDecoderModel.input_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.label_smoothing = %LABEL_SMOOTHING\n    models.EncoderDecoderModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR\n    models.EncoderDecoderModel.module = @network.Transformer()\n    models.EncoderDecoderModel.optimizer_def = %OPTIMIZER\n    models.EncoderDecoderModel.output_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.z_loss = %Z_LOSS\n    \n#### Parameters for seqio.Evaluator:\n\n    seqio.Evaluator.logger_cls = \\\n        [@seqio.PyLoggingLogger, @seqio.TensorBoardLogger, @seqio.JSONLogger]\n    seqio.Evaluator.num_examples = 2048\n    \n#### Parameters for partitioning.PjitPartitioner:\n\n    partitioning.PjitPartitioner.logical_axis_rules = \\\n        @partitioning.standard_logical_axis_rules()\n    partitioning.PjitPartitioner.model_parallel_submesh = None\n    partitioning.PjitPartitioner.num_partitions = 4\n    \n#### Parameters for utils.RestoreCheckpointConfig:\n\n    utils.RestoreCheckpointConfig.path = []\n    \n#### Parameters for utils.SaveCheckpointConfig:\n\n    utils.SaveCheckpointConfig.dtype = 'float32'\n    utils.SaveCheckpointConfig.keep = None\n    utils.SaveCheckpointConfig.period = %SAVING_PERIOD\n    utils.SaveCheckpointConfig.save_dataset = False\n    \n#### Parameters for seqio.SentencePieceVocabulary:\n\n    seqio.SentencePieceVocabulary.extra_ids = 100\n    seqio.SentencePieceVocabulary.sentencepiece_model_file = \\\n        'gs://improved-t5/vocabs/tokenizer.model'\n    \n#### Parameters for network.T5Config:\n\n    network.T5Config.dropout_rate = %DROPOUT_RATE\n    network.T5Config.dtype = 'bfloat16'\n    network.T5Config.emb_dim = 4096\n    network.T5Config.head_dim = 64\n    network.T5Config.logits_via_embedding = False\n    network.T5Config.mlp_activations = ('gelu', 'linear')\n    network.T5Config.mlp_dim = 10240\n    network.T5Config.num_decoder_layers = 24\n    network.T5Config.num_encoder_layers = 24\n    network.T5Config.num_heads = 64\n    network.T5Config.remat_policy = 'minimal'\n    network.T5Config.scan_layers = True\n    network.T5Config.vocab_size = 32128\n    \n#### Parameters for train_script.train:\n\n    train_script.train.checkpoint_cfg = @utils.CheckpointConfig()\n    train_script.train.eval_period = %SAVING_PERIOD\n    train_script.train.eval_steps = 100\n    train_script.train.infer_eval_dataset_cfg = @infer_eval/utils.DatasetConfig()\n    train_script.train.inference_evaluator_cls = @seqio.Evaluator\n    train_script.train.model = %MODEL\n    train_script.train.model_dir = %MODEL_DIR\n    train_script.train.partitioner = @partitioning.PjitPartitioner()\n    train_script.train.random_seed = %RANDOM_SEED\n    train_script.train.run_eval_before_training = True\n    train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config\n    train_script.train.total_steps = %TRAIN_STEPS\n    train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()\n    train_script.train.train_eval_dataset_cfg = @train_eval/utils.DatasetConfig()\n    train_script.train.trainer_cls = @trainer.Trainer\n    train_script.train.use_hardware_rng = %USE_HARDWARE_RNG\n    \n#### Parameters for trainer.Trainer:\n\n    trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()\n    trainer.Trainer.num_microbatches = None\n    \n#### Parameters for network.Transformer:\n\n    network.Transformer.config = @network.T5Config()""",135.37999,0.97273064,1.168251e-05,0.00092649757,0.0009245007,28150.834,372.76468
xxl,1180000,,-0.9980544,148875.3,-0.7594721,2048.0063,2048.0063,148885.33,-0.7595232,-0.75823516,1.0016987,0.99201345,1.0016987,1.2867212,13958.407,20480000.0,1467.2162,2.8656566,0.71641415,-140434.89,326.68484,10.072699,-5.138484e-05,1.0340366545187978,,,0.96362686,,0.00092254323,0.00092057505,14088.034,50.04694
xxl,1190000,,-0.99769443,149177.97,-0.7610161,2048.0063,2048.0063,149188.05,-0.76106745,-0.7597756,1.0017004,0.99200714,1.0017004,1.2869554,13957.282,20480000.0,1467.3344,2.8658874,0.71647185,-140446.2,326.71115,10.02351,-5.1133906e-05,1.034022849215917,,,0.9728339,,0.0009186377,0.0009166989,28131.057,372.8215
xxl,1200000,21.37903,-0.99802107,148871.64,-0.75945336,2048.005,2048.005,148881.5,-0.75950366,-0.75821334,1.0017017,0.9920005,1.0017017,1.2871927,13957.995,20480000.0,1467.2594,2.865741,0.71643525,-140439.03,326.69446,9.989674,-5.096129e-05,1.0340195448051004,"b""    from __gin__ import dynamic_registration\n    import __main__ as train_script\n    import data.pile.tasks\n    from models.scalable_t5 import network\n    import seqio\n    from t5x import adafactor\n    from t5x import gin_utils\n    from t5x import models\n    from t5x import partitioning\n    from t5x import trainer\n    from t5x import utils\n    \n#### Macros:\n\n    BATCH_SIZE = 2048\n    DROPOUT_RATE = 0.0\n    EVAL_BATCH_SIZE = 256\n    LABEL_SMOOTHING = 0.0\n    LOSS_NORMALIZING_FACTOR = None\n    MIXTURE_OR_TASK_MODULE = None\n    MIXTURE_OR_TASK_NAME = 'pile_r_denoiser'\n    MODEL = @models.EncoderDecoderModel()\n    MODEL_DIR = 'gs://improved-t5/ckpts/v2_xxl_mlm/'\n    OPTIMIZER = @adafactor.Adafactor()\n    RANDOM_SEED = None\n    SAVING_PERIOD = 10000\n    SHUFFLE_TRAIN_EXAMPLES = True\n    TASK_FEATURE_LENGTHS = {'inputs': 512, 'targets': 114}\n    TRAIN_STEPS = 2000000\n    USE_CACHED_TASKS = False\n    USE_HARDWARE_RNG = False\n    VOCABULARY = @seqio.SentencePieceVocabulary()\n    Z_LOSS = 0.0001\n    \n#### Parameters for adafactor.Adafactor:\n\n    adafactor.Adafactor.decay_rate = 0.8\n    adafactor.Adafactor.logical_factor_rules = \\\n        @adafactor.standard_logical_factor_rules()\n    adafactor.Adafactor.step_offset = 0\n    \n#### Parameters for utils.CheckpointConfig:\n\n    utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()\n    utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()\n    \n#### Parameters for utils.create_learning_rate_scheduler:\n\n    utils.create_learning_rate_scheduler.base_learning_rate = 1.0\n    utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'\n    utils.create_learning_rate_scheduler.warmup_steps = 10000\n    \n#### Parameters for infer_eval/utils.DatasetConfig:\n\n    infer_eval/utils.DatasetConfig.batch_size = %EVAL_BATCH_SIZE\n    infer_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    infer_eval/utils.DatasetConfig.seed = 0\n    infer_eval/utils.DatasetConfig.shuffle = False\n    infer_eval/utils.DatasetConfig.split = 'validation'\n    infer_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    \n#### Parameters for train/utils.DatasetConfig:\n\n    train/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train/utils.DatasetConfig.pack = True\n    train/utils.DatasetConfig.seed = None\n    train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES\n    train/utils.DatasetConfig.split = 'train'\n    train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for train_eval/utils.DatasetConfig:\n\n    train_eval/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train_eval/utils.DatasetConfig.pack = True\n    train_eval/utils.DatasetConfig.seed = 42\n    train_eval/utils.DatasetConfig.shuffle = False\n    train_eval/utils.DatasetConfig.split = 'validation'\n    train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for models.EncoderDecoderModel:\n\n    models.EncoderDecoderModel.input_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.label_smoothing = %LABEL_SMOOTHING\n    models.EncoderDecoderModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR\n    models.EncoderDecoderModel.module = @network.Transformer()\n    models.EncoderDecoderModel.optimizer_def = %OPTIMIZER\n    models.EncoderDecoderModel.output_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.z_loss = %Z_LOSS\n    \n#### Parameters for seqio.Evaluator:\n\n    seqio.Evaluator.logger_cls = \\\n        [@seqio.PyLoggingLogger, @seqio.TensorBoardLogger, @seqio.JSONLogger]\n    seqio.Evaluator.num_examples = 2048\n    \n#### Parameters for partitioning.PjitPartitioner:\n\n    partitioning.PjitPartitioner.logical_axis_rules = \\\n        @partitioning.standard_logical_axis_rules()\n    partitioning.PjitPartitioner.model_parallel_submesh = None\n    partitioning.PjitPartitioner.num_partitions = 4\n    \n#### Parameters for utils.RestoreCheckpointConfig:\n\n    utils.RestoreCheckpointConfig.path = []\n    \n#### Parameters for utils.SaveCheckpointConfig:\n\n    utils.SaveCheckpointConfig.dtype = 'float32'\n    utils.SaveCheckpointConfig.keep = None\n    utils.SaveCheckpointConfig.period = %SAVING_PERIOD\n    utils.SaveCheckpointConfig.save_dataset = False\n    \n#### Parameters for seqio.SentencePieceVocabulary:\n\n    seqio.SentencePieceVocabulary.extra_ids = 100\n    seqio.SentencePieceVocabulary.sentencepiece_model_file = \\\n        'gs://improved-t5/vocabs/tokenizer.model'\n    \n#### Parameters for network.T5Config:\n\n    network.T5Config.dropout_rate = %DROPOUT_RATE\n    network.T5Config.dtype = 'bfloat16'\n    network.T5Config.emb_dim = 4096\n    network.T5Config.head_dim = 64\n    network.T5Config.logits_via_embedding = False\n    network.T5Config.mlp_activations = ('gelu', 'linear')\n    network.T5Config.mlp_dim = 10240\n    network.T5Config.num_decoder_layers = 24\n    network.T5Config.num_encoder_layers = 24\n    network.T5Config.num_heads = 64\n    network.T5Config.remat_policy = 'minimal'\n    network.T5Config.scan_layers = True\n    network.T5Config.vocab_size = 32128\n    \n#### Parameters for train_script.train:\n\n    train_script.train.checkpoint_cfg = @utils.CheckpointConfig()\n    train_script.train.eval_period = %SAVING_PERIOD\n    train_script.train.eval_steps = 100\n    train_script.train.infer_eval_dataset_cfg = @infer_eval/utils.DatasetConfig()\n    train_script.train.inference_evaluator_cls = @seqio.Evaluator\n    train_script.train.model = %MODEL\n    train_script.train.model_dir = %MODEL_DIR\n    train_script.train.partitioner = @partitioning.PjitPartitioner()\n    train_script.train.random_seed = %RANDOM_SEED\n    train_script.train.run_eval_before_training = True\n    train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config\n    train_script.train.total_steps = %TRAIN_STEPS\n    train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()\n    train_script.train.train_eval_dataset_cfg = @train_eval/utils.DatasetConfig()\n    train_script.train.trainer_cls = @trainer.Trainer\n    train_script.train.use_hardware_rng = %USE_HARDWARE_RNG\n    \n#### Parameters for trainer.Trainer:\n\n    trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()\n    trainer.Trainer.num_microbatches = None\n    \n#### Parameters for network.Transformer:\n\n    network.Transformer.config = @network.T5Config()""",119.619194,5.855516,1.6212463e-05,0.0009147812,0.0009128713,42497.62,372.2535
xxl,1250000,21.914232,-0.9984352,148510.23,-0.75760967,2048.0063,2048.0063,148519.67,-0.7576579,-0.7563856,1.001682,0.99209243,1.001682,1.2838873,13965.797,20480000.0,1466.4398,2.8641403,0.71603507,-140360.58,326.512,9.559414,-4.8766367e-05,1.0340079114710954,"b""    from __gin__ import dynamic_registration\n    import __main__ as train_script\n    import data.pile.tasks\n    from models.scalable_t5 import network\n    import seqio\n    from t5x import adafactor\n    from t5x import gin_utils\n    from t5x import models\n    from t5x import partitioning\n    from t5x import trainer\n    from t5x import utils\n    \n#### Macros:\n\n    BATCH_SIZE = 2048\n    DROPOUT_RATE = 0.0\n    EVAL_BATCH_SIZE = 256\n    LABEL_SMOOTHING = 0.0\n    LOSS_NORMALIZING_FACTOR = None\n    MIXTURE_OR_TASK_MODULE = None\n    MIXTURE_OR_TASK_NAME = 'pile_r_denoiser'\n    MODEL = @models.EncoderDecoderModel()\n    MODEL_DIR = 'gs://improved-t5/ckpts/v2_xxl_mlm/'\n    OPTIMIZER = @adafactor.Adafactor()\n    RANDOM_SEED = None\n    SAVING_PERIOD = 10000\n    SHUFFLE_TRAIN_EXAMPLES = True\n    TASK_FEATURE_LENGTHS = {'inputs': 512, 'targets': 114}\n    TRAIN_STEPS = 2000000\n    USE_CACHED_TASKS = False\n    USE_HARDWARE_RNG = False\n    VOCABULARY = @seqio.SentencePieceVocabulary()\n    Z_LOSS = 0.0001\n    \n#### Parameters for adafactor.Adafactor:\n\n    adafactor.Adafactor.decay_rate = 0.8\n    adafactor.Adafactor.logical_factor_rules = \\\n        @adafactor.standard_logical_factor_rules()\n    adafactor.Adafactor.step_offset = 0\n    \n#### Parameters for utils.CheckpointConfig:\n\n    utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()\n    utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()\n    \n#### Parameters for utils.create_learning_rate_scheduler:\n\n    utils.create_learning_rate_scheduler.base_learning_rate = 1.0\n    utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'\n    utils.create_learning_rate_scheduler.warmup_steps = 10000\n    \n#### Parameters for infer_eval/utils.DatasetConfig:\n\n    infer_eval/utils.DatasetConfig.batch_size = %EVAL_BATCH_SIZE\n    infer_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    infer_eval/utils.DatasetConfig.seed = 0\n    infer_eval/utils.DatasetConfig.shuffle = False\n    infer_eval/utils.DatasetConfig.split = 'validation'\n    infer_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    \n#### Parameters for train/utils.DatasetConfig:\n\n    train/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train/utils.DatasetConfig.pack = True\n    train/utils.DatasetConfig.seed = None\n    train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES\n    train/utils.DatasetConfig.split = 'train'\n    train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for train_eval/utils.DatasetConfig:\n\n    train_eval/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train_eval/utils.DatasetConfig.pack = True\n    train_eval/utils.DatasetConfig.seed = 42\n    train_eval/utils.DatasetConfig.shuffle = False\n    train_eval/utils.DatasetConfig.split = 'validation'\n    train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for models.EncoderDecoderModel:\n\n    models.EncoderDecoderModel.input_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.label_smoothing = %LABEL_SMOOTHING\n    models.EncoderDecoderModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR\n    models.EncoderDecoderModel.module = @network.Transformer()\n    models.EncoderDecoderModel.optimizer_def = %OPTIMIZER\n    models.EncoderDecoderModel.output_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.z_loss = %Z_LOSS\n    \n#### Parameters for seqio.Evaluator:\n\n    seqio.Evaluator.logger_cls = \\\n        [@seqio.PyLoggingLogger, @seqio.TensorBoardLogger, @seqio.JSONLogger]\n    seqio.Evaluator.num_examples = 2048\n    \n#### Parameters for partitioning.PjitPartitioner:\n\n    partitioning.PjitPartitioner.logical_axis_rules = \\\n        @partitioning.standard_logical_axis_rules()\n    partitioning.PjitPartitioner.model_parallel_submesh = None\n    partitioning.PjitPartitioner.num_partitions = 4\n    \n#### Parameters for utils.RestoreCheckpointConfig:\n\n    utils.RestoreCheckpointConfig.path = []\n    \n#### Parameters for utils.SaveCheckpointConfig:\n\n    utils.SaveCheckpointConfig.dtype = 'float32'\n    utils.SaveCheckpointConfig.keep = None\n    utils.SaveCheckpointConfig.period = %SAVING_PERIOD\n    utils.SaveCheckpointConfig.save_dataset = False\n    \n#### Parameters for seqio.SentencePieceVocabulary:\n\n    seqio.SentencePieceVocabulary.extra_ids = 100\n    seqio.SentencePieceVocabulary.sentencepiece_model_file = \\\n        'gs://improved-t5/vocabs/tokenizer.model'\n    \n#### Parameters for network.T5Config:\n\n    network.T5Config.dropout_rate = %DROPOUT_RATE\n    network.T5Config.dtype = 'bfloat16'\n    network.T5Config.emb_dim = 4096\n    network.T5Config.head_dim = 64\n    network.T5Config.logits_via_embedding = False\n    network.T5Config.mlp_activations = ('gelu', 'linear')\n    network.T5Config.mlp_dim = 10240\n    network.T5Config.num_decoder_layers = 24\n    network.T5Config.num_encoder_layers = 24\n    network.T5Config.num_heads = 64\n    network.T5Config.remat_policy = 'minimal'\n    network.T5Config.scan_layers = True\n    network.T5Config.vocab_size = 32128\n    \n#### Parameters for train_script.train:\n\n    train_script.train.checkpoint_cfg = @utils.CheckpointConfig()\n    train_script.train.eval_period = %SAVING_PERIOD\n    train_script.train.eval_steps = 100\n    train_script.train.infer_eval_dataset_cfg = @infer_eval/utils.DatasetConfig()\n    train_script.train.inference_evaluator_cls = @seqio.Evaluator\n    train_script.train.model = %MODEL\n    train_script.train.model_dir = %MODEL_DIR\n    train_script.train.partitioner = @partitioning.PjitPartitioner()\n    train_script.train.random_seed = %RANDOM_SEED\n    train_script.train.run_eval_before_training = True\n    train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config\n    train_script.train.total_steps = %TRAIN_STEPS\n    train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()\n    train_script.train.train_eval_dataset_cfg = @train_eval/utils.DatasetConfig()\n    train_script.train.trainer_cls = @trainer.Trainer\n    train_script.train.use_hardware_rng = %USE_HARDWARE_RNG\n    \n#### Parameters for trainer.Trainer:\n\n    trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()\n    trainer.Trainer.num_microbatches = None\n    \n#### Parameters for network.Transformer:\n\n    network.Transformer.config = @network.T5Config()""",126.59879,0.98282385,1.4305115e-05,0.0008962217,0.0008944276,70948.516,44.111973
xxl,1260000,,-0.9989256,147961.22,-0.7548089,2048.008,2048.008,147971.36,-0.7548607,-0.753592,1.0016835,0.9920854,1.0016835,1.2841402,13962.093,20480000.0,1466.8287,2.8648999,0.71622497,-140397.81,326.59857,9.934556,-5.068012e-05,1.03399180012745,,,0.9745407,,0.0008926151,0.00089087116,14118.827,48.709892
xxl,1270000,,-0.9985199,148293.75,-0.75650537,2048.0076,2048.0076,148303.3,-0.756554,-0.7552798,1.0016872,0.992068,1.0016872,1.284762,13961.167,20480000.0,1466.926,2.86509,0.7162725,-140407.12,326.62024,9.911892,-5.05645e-05,1.0339699825780047,,,0.9834943,,0.00088913785,0.0008873568,28164.5,374.2907
xxl,1280000,,-0.9988665,147972.88,-0.7548684,2048.007,2048.007,147982.88,-0.7549194,-0.75365216,1.0016814,0.99209434,1.0016814,1.2838142,13961.295,20480000.0,1466.9126,2.8650637,0.7162659,-140405.83,326.61728,9.890819,-5.0456994e-05,1.0339676016794654,,,0.96946764,,0.00088561507,0.00088388385,42535.92,372.96252
xxl,1290000,,-0.99879503,148139.36,-0.75571775,2048.0068,2048.0068,148149.16,-0.7557677,-0.75449395,1.0016882,0.9920634,1.0016882,1.2849306,13961.215,20480000.0,1466.9211,2.8650804,0.7162701,-140406.66,326.61914,9.771166,-4.9846596e-05,1.0339861102947445,,,0.98567367,,0.00088214036,0.0008804512,56905.88,45.72824
xxl,1300000,,-0.9979674,149000.16,-0.76010895,2048.0068,2048.0068,149009.34,-0.76015586,-0.75887436,1.0016886,0.99206173,1.0016886,1.2849928,13961.18,20480000.0,1466.9247,2.8650873,0.7162718,-140406.98,326.61996,9.573517,-4.8838312e-05,1.033951737033231,,,0.96853805,,0.0008787652,0.00087705837,70948.49,374.7282
xxl,1310000,,-0.99811655,148865.39,-0.7594215,2048.0076,2048.0076,148874.64,-0.75946873,-0.7581928,1.0016829,0.9920883,1.0016829,1.2840339,13961.318,20480000.0,1466.9103,2.8650591,0.7162648,-140405.61,326.61673,9.535014,-4.8641894e-05,1.033948755969415,,,1.0082645,,0.0008753525,0.0008737044,85320.43,374.84296
xxl,1320000,,-0.9981693,148819.12,-0.7591855,2048.0063,2048.0063,148828.12,-0.7592313,-0.757955,1.001684,0.99208355,1.001684,1.2842066,13961.244,20480000.0,1466.918,2.8650742,0.71626854,-140406.34,326.61847,9.5376005,-4.865509e-05,1.0339464787342503,,,0.96725297,,0.00087205716,0.0008703886,99692.25,372.267
xxl,1330000,,-0.9981376,148818.98,-0.7591847,2048.0063,2048.0063,148828.05,-0.75923103,-0.7579544,1.0016843,0.99208194,1.0016843,1.2842643,13961.201,20480000.0,1466.9225,2.865083,0.71627074,-140406.78,326.61945,9.51446,-4.8537036e-05,1.0339372059425591,,,0.975014,,0.0008687168,0.00086711027,114061.43,373.10736
xxl,1340000,,-0.9981218,148846.06,-0.7593229,2048.0076,2048.0076,148855.55,-0.7593713,-0.7580956,1.0016828,0.9920893,1.0016828,1.2840006,13961.312,20480000.0,1466.9109,2.8650603,0.7162651,-140405.67,326.61685,9.502503,-4.8476046e-05,1.0339293291612175,,,0.9679091,,0.00086548785,0.0008638687,128431.56,373.95193
xxl,1350000,,-0.99834275,148612.86,-0.75813323,2048.008,2048.008,148622.31,-0.75818145,-0.75689965,1.0016935,0.99203956,1.0016935,1.2857912,13961.25,20480000.0,1466.9174,2.865073,0.71626824,-140406.3,326.61832,9.511024,-4.8519516e-05,1.0339127548220333,,,0.96668077,,0.000862281,0.00086066325,142802.44,44.924324
xxl,1360000,,-0.9984899,148428.28,-0.75719166,2048.0076,2048.0076,148437.92,-0.75724083,-0.75596744,1.0016844,0.9920812,1.0016844,1.2842902,13961.225,20480000.0,1466.92,2.8650782,0.71626955,-140406.55,326.61893,9.47621,-4.834191e-05,1.0338893285514716,,,0.9767182,,0.00085906,0.00085749326,156844.38,372.3647
xxl,1370000,,-0.99857134,148397.89,-0.75703657,2048.0083,2048.0083,148407.25,-0.75708437,-0.75581825,1.0016751,0.9921243,1.0016751,1.2827389,13961.633,20480000.0,1466.8772,2.8649945,0.71624863,-140402.44,326.60938,9.477697,-4.83495e-05,1.0338988159352305,,,0.97329044,,0.0008559233,0.000854358,171214.11,43.66931
xxl,1380000,,-0.99871373,148214.25,-0.75609976,2048.0073,2048.0073,148223.8,-0.7561485,-0.7548751,1.0016869,0.99207026,1.0016869,1.2846866,13961.244,20480000.0,1466.918,2.8650742,0.71626854,-140406.34,326.61847,9.448549,-4.8200804e-05,1.033896568553984,,,0.97193384,,0.0008528227,0.00085125683,185254.77,44.341812
xxl,1390000,,-0.998737,148188.77,-0.7559698,2048.0083,2048.0083,148197.84,-0.7560161,-0.75474393,1.0016855,0.99207574,1.0016855,1.2844852,13961.205,20480000.0,1466.9221,2.8650823,0.71627057,-140406.75,326.6194,9.470356,-4.831205e-05,1.0338790031316234,,,0.9818671,,0.0008497159,0.00084818923,199296.05,43.987247
xxl,1400000,,-0.99860936,148386.89,-0.7569805,2048.0056,2048.0056,148396.7,-0.75703055,-0.7557534,1.0016899,0.9920553,1.0016899,1.2852194,13961.316,20480000.0,1466.9104,2.8650594,0.71626484,-140405.62,326.61676,9.554577,-4.8741695e-05,1.0338784811269988,,,0.9823582,,0.00084665354,0.0008451545,213337.05,44.25493
xxl,1410000,,-0.99617875,150895.17,-0.76977617,2048.008,2048.008,150905.05,-0.7698266,-0.7685001,1.001726,0.991886,1.001726,1.2913011,13962.199,20480000.0,1466.8177,2.8648784,0.7162196,-140396.75,326.59613,9.740224,-4.968875e-05,1.0338779109917247,,,0.96740127,,0.00084364426,0.0008421522,227379.28,371.55994
xxl,1420000,,-0.99636185,150713.2,-0.7688479,2048.007,2048.007,150722.45,-0.7688951,-0.7675761,1.0017184,0.9919211,1.0017184,1.2900369,13961.195,20480000.0,1466.9232,2.8650844,0.7162711,-140406.84,326.61963,9.598816,-4.8967373e-05,1.0338856882186385,,,0.97035336,,0.0008406639,0.00083918165,241747.78,45.647697
xxl,1430000,,-0.9979923,148991.84,-0.76006657,2048.0076,2048.0076,149001.27,-0.76011467,-0.75882334,1.0017017,0.99200004,1.0017017,1.2872051,13961.327,20480000.0,1466.9092,2.865057,0.71626425,-140405.5,326.61652,9.665524,-4.9307673e-05,1.0338688310016433,,,0.97027993,,0.0008377118,0.00083624234,255790.55,43.748783
xxl,1440000,,-0.9991013,147734.81,-0.753654,2048.0068,2048.0068,147744.53,-0.7537036,-0.75243527,1.0016856,0.99207485,1.0016856,1.2845134,13961.252,20480000.0,1466.9172,2.8650727,0.7162682,-140406.28,326.6183,9.75487,-4.976347e-05,1.0338645220265328,,,0.9773402,,0.0008347885,0.0008333336,269831.25,372.22186
xxl,1450000,,-0.9991585,147633.36,-0.75313646,2048.0078,2048.0078,147643.34,-0.7531873,-0.75191385,1.0016936,0.9920381,1.0016936,1.2858407,13962.044,20480000.0,1466.834,2.8649101,0.71622753,-140398.31,326.59973,9.736449,-4.9669496e-05,1.03384354005056,,,0.970207,,0.00083189586,0.0008304551,284201.25,49.002136
xxl,1460000,,-0.9993865,147483.61,-0.75237244,2048.008,2048.008,147493.73,-0.7524241,-0.75115913,1.0016841,0.99208313,1.0016841,1.2842232,13961.3125,20480000.0,1466.9109,2.8650603,0.7162651,-140405.67,326.61685,9.693656,-4.945119e-05,1.0338455221126404,,,0.9623568,,0.0008290311,0.0008276061,298247.28,44.779995
xxl,1470000,21.497837,-0.998778,148145.9,-0.75575113,2048.0063,2048.0063,148155.8,-0.75580156,-0.7545304,1.0016848,0.99207985,1.0016848,1.2843399,13961.238,20480000.0,1466.9186,2.8650753,0.71626884,-140406.4,326.6186,9.562284,-4.8781014e-05,1.0338261380729816,"b""    from __gin__ import dynamic_registration\n    import __main__ as train_script\n    import data.pile.tasks\n    from models.scalable_t5 import network\n    import seqio\n    from t5x import adafactor\n    from t5x import gin_utils\n    from t5x import models\n    from t5x import partitioning\n    from t5x import trainer\n    from t5x import utils\n    \n#### Macros:\n\n    BATCH_SIZE = 2048\n    DROPOUT_RATE = 0.0\n    EVAL_BATCH_SIZE = 256\n    LABEL_SMOOTHING = 0.0\n    LOSS_NORMALIZING_FACTOR = None\n    MIXTURE_OR_TASK_MODULE = None\n    MIXTURE_OR_TASK_NAME = 'pile_r_denoiser'\n    MODEL = @models.EncoderDecoderModel()\n    MODEL_DIR = 'gs://improved-t5/ckpts/v2_xxl_mlm/'\n    OPTIMIZER = @adafactor.Adafactor()\n    RANDOM_SEED = None\n    SAVING_PERIOD = 10000\n    SHUFFLE_TRAIN_EXAMPLES = True\n    TASK_FEATURE_LENGTHS = {'inputs': 512, 'targets': 114}\n    TRAIN_STEPS = 2000000\n    USE_CACHED_TASKS = False\n    USE_HARDWARE_RNG = False\n    VOCABULARY = @seqio.SentencePieceVocabulary()\n    Z_LOSS = 0.0001\n    \n#### Parameters for adafactor.Adafactor:\n\n    adafactor.Adafactor.decay_rate = 0.8\n    adafactor.Adafactor.logical_factor_rules = \\\n        @adafactor.standard_logical_factor_rules()\n    adafactor.Adafactor.step_offset = 0\n    \n#### Parameters for utils.CheckpointConfig:\n\n    utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()\n    utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()\n    \n#### Parameters for utils.create_learning_rate_scheduler:\n\n    utils.create_learning_rate_scheduler.base_learning_rate = 1.0\n    utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'\n    utils.create_learning_rate_scheduler.warmup_steps = 10000\n    \n#### Parameters for infer_eval/utils.DatasetConfig:\n\n    infer_eval/utils.DatasetConfig.batch_size = %EVAL_BATCH_SIZE\n    infer_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    infer_eval/utils.DatasetConfig.seed = 0\n    infer_eval/utils.DatasetConfig.shuffle = False\n    infer_eval/utils.DatasetConfig.split = 'validation'\n    infer_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    \n#### Parameters for train/utils.DatasetConfig:\n\n    train/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train/utils.DatasetConfig.pack = True\n    train/utils.DatasetConfig.seed = None\n    train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES\n    train/utils.DatasetConfig.split = 'train'\n    train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for train_eval/utils.DatasetConfig:\n\n    train_eval/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train_eval/utils.DatasetConfig.pack = True\n    train_eval/utils.DatasetConfig.seed = 42\n    train_eval/utils.DatasetConfig.shuffle = False\n    train_eval/utils.DatasetConfig.split = 'validation'\n    train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for models.EncoderDecoderModel:\n\n    models.EncoderDecoderModel.input_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.label_smoothing = %LABEL_SMOOTHING\n    models.EncoderDecoderModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR\n    models.EncoderDecoderModel.module = @network.Transformer()\n    models.EncoderDecoderModel.optimizer_def = %OPTIMIZER\n    models.EncoderDecoderModel.output_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.z_loss = %Z_LOSS\n    \n#### Parameters for seqio.Evaluator:\n\n    seqio.Evaluator.logger_cls = \\\n        [@seqio.PyLoggingLogger, @seqio.TensorBoardLogger, @seqio.JSONLogger]\n    seqio.Evaluator.num_examples = 2048\n    \n#### Parameters for partitioning.PjitPartitioner:\n\n    partitioning.PjitPartitioner.logical_axis_rules = \\\n        @partitioning.standard_logical_axis_rules()\n    partitioning.PjitPartitioner.model_parallel_submesh = None\n    partitioning.PjitPartitioner.num_partitions = 4\n    \n#### Parameters for utils.RestoreCheckpointConfig:\n\n    utils.RestoreCheckpointConfig.path = []\n    \n#### Parameters for utils.SaveCheckpointConfig:\n\n    utils.SaveCheckpointConfig.dtype = 'float32'\n    utils.SaveCheckpointConfig.keep = None\n    utils.SaveCheckpointConfig.period = %SAVING_PERIOD\n    utils.SaveCheckpointConfig.save_dataset = False\n    \n#### Parameters for seqio.SentencePieceVocabulary:\n\n    seqio.SentencePieceVocabulary.extra_ids = 100\n    seqio.SentencePieceVocabulary.sentencepiece_model_file = \\\n        'gs://improved-t5/vocabs/tokenizer.model'\n    \n#### Parameters for network.T5Config:\n\n    network.T5Config.dropout_rate = %DROPOUT_RATE\n    network.T5Config.dtype = 'bfloat16'\n    network.T5Config.emb_dim = 4096\n    network.T5Config.head_dim = 64\n    network.T5Config.logits_via_embedding = False\n    network.T5Config.mlp_activations = ('gelu', 'linear')\n    network.T5Config.mlp_dim = 10240\n    network.T5Config.num_decoder_layers = 24\n    network.T5Config.num_encoder_layers = 24\n    network.T5Config.num_heads = 64\n    network.T5Config.remat_policy = 'minimal'\n    network.T5Config.scan_layers = True\n    network.T5Config.vocab_size = 32128\n    \n#### Parameters for train_script.train:\n\n    train_script.train.checkpoint_cfg = @utils.CheckpointConfig()\n    train_script.train.eval_period = %SAVING_PERIOD\n    train_script.train.eval_steps = 100\n    train_script.train.infer_eval_dataset_cfg = @infer_eval/utils.DatasetConfig()\n    train_script.train.inference_evaluator_cls = @seqio.Evaluator\n    train_script.train.model = %MODEL\n    train_script.train.model_dir = %MODEL_DIR\n    train_script.train.partitioner = @partitioning.PjitPartitioner()\n    train_script.train.random_seed = %RANDOM_SEED\n    train_script.train.run_eval_before_training = True\n    train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config\n    train_script.train.total_steps = %TRAIN_STEPS\n    train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()\n    train_script.train.train_eval_dataset_cfg = @train_eval/utils.DatasetConfig()\n    train_script.train.trainer_cls = @trainer.Trainer\n    train_script.train.use_hardware_rng = %USE_HARDWARE_RNG\n    \n#### Parameters for trainer.Trainer:\n\n    trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()\n    trainer.Trainer.num_microbatches = None\n    \n#### Parameters for network.Transformer:\n\n    network.Transformer.config = @network.T5Config()""",136.46434,6.089959,1.7166138e-05,0.0008261957,0.0008247863,312289.03,381.97232
xxl,310000,51.3547,-0.98368853,163628.8,-0.8347355,2048.0068,2048.0068,163661.61,-0.8349029,-0.8334959,1.001688,0.99206424,1.001688,1.2848994,24620.176,20480000.0,831.8381,3.2493675,0.40617093,-79619.55,370.42792,32.755173,-0.00016709714,1.0372307385374335,"b""    from __gin__ import dynamic_registration\n    import __main__ as train_script\n    import data.pile.tasks\n    from models.scalable_t5 import network\n    import seqio\n    from t5x import adafactor\n    from t5x import gin_utils\n    from t5x import models\n    from t5x import partitioning\n    from t5x import trainer\n    from t5x import utils\n    \n#### Macros:\n\n    BATCH_SIZE = 2048\n    DROPOUT_RATE = 0.0\n    EVAL_BATCH_SIZE = 256\n    LABEL_SMOOTHING = 0.0\n    LOSS_NORMALIZING_FACTOR = None\n    MIXTURE_OR_TASK_MODULE = None\n    MIXTURE_OR_TASK_NAME = 'pile_r_denoiser'\n    MODEL = @models.EncoderDecoderModel()\n    MODEL_DIR = 'gs://improved-t5/ckpts/v2_xxl_mlm/'\n    OPTIMIZER = @adafactor.Adafactor()\n    RANDOM_SEED = None\n    SAVING_PERIOD = 10000\n    SHUFFLE_TRAIN_EXAMPLES = True\n    TASK_FEATURE_LENGTHS = {'inputs': 512, 'targets': 114}\n    TRAIN_STEPS = 1000000\n    USE_CACHED_TASKS = False\n    USE_HARDWARE_RNG = False\n    VOCABULARY = @seqio.SentencePieceVocabulary()\n    Z_LOSS = 0.0001\n    \n#### Parameters for adafactor.Adafactor:\n\n    adafactor.Adafactor.decay_rate = 0.8\n    adafactor.Adafactor.logical_factor_rules = \\\n        @adafactor.standard_logical_factor_rules()\n    adafactor.Adafactor.step_offset = 0\n    \n#### Parameters for utils.CheckpointConfig:\n\n    utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()\n    utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()\n    \n#### Parameters for utils.create_learning_rate_scheduler:\n\n    utils.create_learning_rate_scheduler.base_learning_rate = 1.0\n    utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'\n    utils.create_learning_rate_scheduler.warmup_steps = 10000\n    \n#### Parameters for infer_eval/utils.DatasetConfig:\n\n    infer_eval/utils.DatasetConfig.batch_size = %EVAL_BATCH_SIZE\n    infer_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    infer_eval/utils.DatasetConfig.seed = 0\n    infer_eval/utils.DatasetConfig.shuffle = False\n    infer_eval/utils.DatasetConfig.split = 'validation'\n    infer_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    \n#### Parameters for train/utils.DatasetConfig:\n\n    train/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train/utils.DatasetConfig.pack = True\n    train/utils.DatasetConfig.seed = None\n    train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES\n    train/utils.DatasetConfig.split = 'train'\n    train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for train_eval/utils.DatasetConfig:\n\n    train_eval/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train_eval/utils.DatasetConfig.pack = True\n    train_eval/utils.DatasetConfig.seed = 42\n    train_eval/utils.DatasetConfig.shuffle = False\n    train_eval/utils.DatasetConfig.split = 'validation'\n    train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for models.EncoderDecoderModel:\n\n    models.EncoderDecoderModel.input_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.label_smoothing = %LABEL_SMOOTHING\n    models.EncoderDecoderModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR\n    models.EncoderDecoderModel.module = @network.Transformer()\n    models.EncoderDecoderModel.optimizer_def = %OPTIMIZER\n    models.EncoderDecoderModel.output_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.z_loss = %Z_LOSS\n    \n#### Parameters for seqio.Evaluator:\n\n    seqio.Evaluator.logger_cls = \\\n        [@seqio.PyLoggingLogger, @seqio.TensorBoardLogger, @seqio.JSONLogger]\n    seqio.Evaluator.num_examples = 2048\n    \n#### Parameters for partitioning.PjitPartitioner:\n\n    partitioning.PjitPartitioner.logical_axis_rules = \\\n        @partitioning.standard_logical_axis_rules()\n    partitioning.PjitPartitioner.model_parallel_submesh = None\n    partitioning.PjitPartitioner.num_partitions = 1\n    \n#### Parameters for utils.RestoreCheckpointConfig:\n\n    utils.RestoreCheckpointConfig.path = []\n    \n#### Parameters for utils.SaveCheckpointConfig:\n\n    utils.SaveCheckpointConfig.dtype = 'float32'\n    utils.SaveCheckpointConfig.keep = None\n    utils.SaveCheckpointConfig.period = %SAVING_PERIOD\n    utils.SaveCheckpointConfig.save_dataset = False\n    \n#### Parameters for seqio.SentencePieceVocabulary:\n\n    seqio.SentencePieceVocabulary.extra_ids = 100\n    seqio.SentencePieceVocabulary.sentencepiece_model_file = \\\n        'gs://improved-t5/vocabs/tokenizer.model'\n    \n#### Parameters for partitioning.standard_logical_axis_rules:\n\n    partitioning.standard_logical_axis_rules.activation_partitioning_dims = 1\n    partitioning.standard_logical_axis_rules.parameter_partitioning_dims = 2\n    \n#### Parameters for network.T5Config:\n\n    network.T5Config.dropout_rate = %DROPOUT_RATE\n    network.T5Config.dtype = 'bfloat16'\n    network.T5Config.emb_dim = 4096\n    network.T5Config.head_dim = 64\n    network.T5Config.logits_via_embedding = False\n    network.T5Config.mlp_activations = ('gelu', 'linear')\n    network.T5Config.mlp_dim = 10240\n    network.T5Config.num_decoder_layers = 24\n    network.T5Config.num_encoder_layers = 24\n    network.T5Config.num_heads = 64\n    network.T5Config.remat_policy = 'minimal'\n    network.T5Config.scan_layers = True\n    network.T5Config.vocab_size = 32128\n    \n#### Parameters for train_script.train:\n\n    train_script.train.checkpoint_cfg = @utils.CheckpointConfig()\n    train_script.train.eval_period = %SAVING_PERIOD\n    train_script.train.eval_steps = 100\n    train_script.train.infer_eval_dataset_cfg = @infer_eval/utils.DatasetConfig()\n    train_script.train.inference_evaluator_cls = @seqio.Evaluator\n    train_script.train.model = %MODEL\n    train_script.train.model_dir = %MODEL_DIR\n    train_script.train.partitioner = @partitioning.PjitPartitioner()\n    train_script.train.random_seed = %RANDOM_SEED\n    train_script.train.run_eval_before_training = True\n    train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config\n    train_script.train.total_steps = %TRAIN_STEPS\n    train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()\n    train_script.train.train_eval_dataset_cfg = @train_eval/utils.DatasetConfig()\n    train_script.train.trainer_cls = @trainer.Trainer\n    train_script.train.use_hardware_rng = %USE_HARDWARE_RNG\n    \n#### Parameters for trainer.Trainer:\n\n    trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()\n    trainer.Trainer.num_microbatches = None\n    \n#### Parameters for network.Transformer:\n\n    network.Transformer.config = @network.T5Config()""",7.6904726,23.716951,1.835823e-05,0.0018107702,0.0017960559,469406.72,
xxl,320000,,-0.9852148,162027.33,-0.82656574,2048.007,2048.007,162059.02,-0.82672745,-0.8253346,1.0016876,0.9920665,1.0016876,1.2848202,24630.596,20480000.0,831.4862,3.247993,0.40599912,-79585.87,370.2712,31.4856,-0.00016062055,1.0371650249220328,,,4.136305,,0.001781797,0.0017677698,24793.56,19.28592
xxl,330000,,-0.9850616,162139.62,-0.82713866,2048.007,2048.007,162170.45,-0.8272959,-0.82590175,1.001688,0.9920636,1.001688,1.2849195,24627.795,20480000.0,831.58075,3.2483623,0.4060453,-79594.92,370.3133,30.768312,-0.00015696138,1.037048666022923,,,4.094818,,0.0017541745,0.0017407791,49501.297,18.304264
xxl,340000,,-0.9854929,161731.5,-0.8250567,2048.0078,2048.0078,161761.47,-0.8252095,-0.82381976,1.0016869,0.99206895,1.0016869,1.2847279,24627.963,20480000.0,831.575,3.24834,0.4060425,-79594.37,370.31076,30.19524,-0.00015403792,1.036972191800808,,,4.102884,,0.0017277872,0.0017149884,74208.305,18.430613
xxl,350000,52.45878,-0.9860522,161158.36,-0.8221328,2048.008,2048.008,161187.5,-0.8222815,-0.82089037,1.0016947,0.9920336,1.0016947,1.2860042,24627.791,20480000.0,831.5808,3.2483625,0.40604532,-79594.92,370.31335,29.577427,-0.0001508862,1.0368051829151836,"b""    from __gin__ import dynamic_registration\n    import __main__ as train_script\n    import data.pile.tasks\n    from models.scalable_t5 import network\n    import seqio\n    from t5x import adafactor\n    from t5x import gin_utils\n    from t5x import models\n    from t5x import partitioning\n    from t5x import trainer\n    from t5x import utils\n    \n#### Macros:\n\n    BATCH_SIZE = 2048\n    DROPOUT_RATE = 0.0\n    EVAL_BATCH_SIZE = 256\n    LABEL_SMOOTHING = 0.0\n    LOSS_NORMALIZING_FACTOR = None\n    MIXTURE_OR_TASK_MODULE = None\n    MIXTURE_OR_TASK_NAME = 'pile_r_denoiser'\n    MODEL = @models.EncoderDecoderModel()\n    MODEL_DIR = 'gs://improved-t5/ckpts/v2_xxl_mlm/'\n    OPTIMIZER = @adafactor.Adafactor()\n    RANDOM_SEED = None\n    SAVING_PERIOD = 10000\n    SHUFFLE_TRAIN_EXAMPLES = True\n    TASK_FEATURE_LENGTHS = {'inputs': 512, 'targets': 114}\n    TRAIN_STEPS = 1000000\n    USE_CACHED_TASKS = False\n    USE_HARDWARE_RNG = False\n    VOCABULARY = @seqio.SentencePieceVocabulary()\n    Z_LOSS = 0.0001\n    \n#### Parameters for adafactor.Adafactor:\n\n    adafactor.Adafactor.decay_rate = 0.8\n    adafactor.Adafactor.logical_factor_rules = \\\n        @adafactor.standard_logical_factor_rules()\n    adafactor.Adafactor.step_offset = 0\n    \n#### Parameters for utils.CheckpointConfig:\n\n    utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()\n    utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()\n    \n#### Parameters for utils.create_learning_rate_scheduler:\n\n    utils.create_learning_rate_scheduler.base_learning_rate = 1.0\n    utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'\n    utils.create_learning_rate_scheduler.warmup_steps = 10000\n    \n#### Parameters for infer_eval/utils.DatasetConfig:\n\n    infer_eval/utils.DatasetConfig.batch_size = %EVAL_BATCH_SIZE\n    infer_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    infer_eval/utils.DatasetConfig.seed = 0\n    infer_eval/utils.DatasetConfig.shuffle = False\n    infer_eval/utils.DatasetConfig.split = 'validation'\n    infer_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    \n#### Parameters for train/utils.DatasetConfig:\n\n    train/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train/utils.DatasetConfig.pack = True\n    train/utils.DatasetConfig.seed = None\n    train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES\n    train/utils.DatasetConfig.split = 'train'\n    train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for train_eval/utils.DatasetConfig:\n\n    train_eval/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train_eval/utils.DatasetConfig.pack = True\n    train_eval/utils.DatasetConfig.seed = 42\n    train_eval/utils.DatasetConfig.shuffle = False\n    train_eval/utils.DatasetConfig.split = 'validation'\n    train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for models.EncoderDecoderModel:\n\n    models.EncoderDecoderModel.input_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.label_smoothing = %LABEL_SMOOTHING\n    models.EncoderDecoderModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR\n    models.EncoderDecoderModel.module = @network.Transformer()\n    models.EncoderDecoderModel.optimizer_def = %OPTIMIZER\n    models.EncoderDecoderModel.output_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.z_loss = %Z_LOSS\n    \n#### Parameters for seqio.Evaluator:\n\n    seqio.Evaluator.logger_cls = \\\n        [@seqio.PyLoggingLogger, @seqio.TensorBoardLogger, @seqio.JSONLogger]\n    seqio.Evaluator.num_examples = 2048\n    \n#### Parameters for partitioning.PjitPartitioner:\n\n    partitioning.PjitPartitioner.logical_axis_rules = \\\n        @partitioning.standard_logical_axis_rules()\n    partitioning.PjitPartitioner.model_parallel_submesh = None\n    partitioning.PjitPartitioner.num_partitions = 1\n    \n#### Parameters for utils.RestoreCheckpointConfig:\n\n    utils.RestoreCheckpointConfig.path = []\n    \n#### Parameters for utils.SaveCheckpointConfig:\n\n    utils.SaveCheckpointConfig.dtype = 'float32'\n    utils.SaveCheckpointConfig.keep = None\n    utils.SaveCheckpointConfig.period = %SAVING_PERIOD\n    utils.SaveCheckpointConfig.save_dataset = False\n    \n#### Parameters for seqio.SentencePieceVocabulary:\n\n    seqio.SentencePieceVocabulary.extra_ids = 100\n    seqio.SentencePieceVocabulary.sentencepiece_model_file = \\\n        'gs://improved-t5/vocabs/tokenizer.model'\n    \n#### Parameters for partitioning.standard_logical_axis_rules:\n\n    partitioning.standard_logical_axis_rules.activation_partitioning_dims = 1\n    partitioning.standard_logical_axis_rules.parameter_partitioning_dims = 2\n    \n#### Parameters for network.T5Config:\n\n    network.T5Config.dropout_rate = %DROPOUT_RATE\n    network.T5Config.dtype = 'bfloat16'\n    network.T5Config.emb_dim = 4096\n    network.T5Config.head_dim = 64\n    network.T5Config.logits_via_embedding = False\n    network.T5Config.mlp_activations = ('gelu', 'linear')\n    network.T5Config.mlp_dim = 10240\n    network.T5Config.num_decoder_layers = 24\n    network.T5Config.num_encoder_layers = 24\n    network.T5Config.num_heads = 64\n    network.T5Config.remat_policy = 'minimal'\n    network.T5Config.scan_layers = True\n    network.T5Config.vocab_size = 32128\n    \n#### Parameters for train_script.train:\n\n    train_script.train.checkpoint_cfg = @utils.CheckpointConfig()\n    train_script.train.eval_period = %SAVING_PERIOD\n    train_script.train.eval_steps = 100\n    train_script.train.infer_eval_dataset_cfg = @infer_eval/utils.DatasetConfig()\n    train_script.train.inference_evaluator_cls = @seqio.Evaluator\n    train_script.train.model = %MODEL\n    train_script.train.model_dir = %MODEL_DIR\n    train_script.train.partitioner = @partitioning.PjitPartitioner()\n    train_script.train.random_seed = %RANDOM_SEED\n    train_script.train.run_eval_before_training = True\n    train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config\n    train_script.train.total_steps = %TRAIN_STEPS\n    train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()\n    train_script.train.train_eval_dataset_cfg = @train_eval/utils.DatasetConfig()\n    train_script.train.trainer_cls = @trainer.Trainer\n    train_script.train.use_hardware_rng = %USE_HARDWARE_RNG\n    \n#### Parameters for trainer.Trainer:\n\n    trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()\n    trainer.Trainer.num_microbatches = None\n    \n#### Parameters for network.Transformer:\n\n    network.Transformer.config = @network.T5Config()""",7.3315444,23.815756,2.1219254e-05,0.001702562,0.0016903109,98915.164,21.961903
xxl,10000,,-0.7213511,489996.53,-2.499667,2048.0066,2048.0066,492511.28,-2.5124958,-2.5082557,1.0016904,0.99205285,1.0016904,1.2853075,24447.102,20480000.0,837.7271,3.2723715,0.40904644,-80183.22,373.05035,2512.1572,-0.012815512,1.0671144781907203,,,4.080433,,0.010000295,0.01,24616.72,19.261759
xxl,20000,,-0.8945217,263591.3,-1.344684,2048.0068,2048.0068,264172.8,-1.3476505,-1.3453674,1.001697,0.99202234,1.001697,1.2864038,24444.297,20480000.0,837.82324,3.272747,0.40909338,-80192.414,373.09317,581.3112,-0.0029654996,1.058158309505484,,,4.1126857,,0.008284418,0.0070712445,49140.906,18.201525
xxl,770000,58.10289,-0.99599624,151030.36,-0.77046585,2048.0083,2048.0083,151041.08,-0.7705205,-0.769218,1.0016932,0.9920394,1.0016932,1.2857898,24045.807,20480000.0,851.70776,3.3269835,0.41587293,-81521.375,379.27612,10.679648,-5.448113e-05,1.034337449897423,"b""    from __gin__ import dynamic_registration\n    import __main__ as train_script\n    import data.pile.tasks\n    from models.scalable_t5 import network\n    import seqio\n    from t5x import adafactor\n    from t5x import gin_utils\n    from t5x import models\n    from t5x import partitioning\n    from t5x import trainer\n    from t5x import utils\n    \n#### Macros:\n\n    BATCH_SIZE = 2048\n    DROPOUT_RATE = 0.0\n    EVAL_BATCH_SIZE = 256\n    LABEL_SMOOTHING = 0.0\n    LOSS_NORMALIZING_FACTOR = None\n    MIXTURE_OR_TASK_MODULE = None\n    MIXTURE_OR_TASK_NAME = 'pile_r_denoiser'\n    MODEL = @models.EncoderDecoderModel()\n    MODEL_DIR = 'gs://improved-t5/ckpts/v2_xxl_mlm/'\n    OPTIMIZER = @adafactor.Adafactor()\n    RANDOM_SEED = None\n    SAVING_PERIOD = 10000\n    SHUFFLE_TRAIN_EXAMPLES = True\n    TASK_FEATURE_LENGTHS = {'inputs': 512, 'targets': 114}\n    TRAIN_STEPS = 1000000\n    USE_CACHED_TASKS = False\n    USE_HARDWARE_RNG = False\n    VOCABULARY = @seqio.SentencePieceVocabulary()\n    Z_LOSS = 0.0001\n    \n#### Parameters for adafactor.Adafactor:\n\n    adafactor.Adafactor.decay_rate = 0.8\n    adafactor.Adafactor.logical_factor_rules = \\\n        @adafactor.standard_logical_factor_rules()\n    adafactor.Adafactor.step_offset = 0\n    \n#### Parameters for utils.CheckpointConfig:\n\n    utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()\n    utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()\n    \n#### Parameters for utils.create_learning_rate_scheduler:\n\n    utils.create_learning_rate_scheduler.base_learning_rate = 1.0\n    utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'\n    utils.create_learning_rate_scheduler.warmup_steps = 10000\n    \n#### Parameters for infer_eval/utils.DatasetConfig:\n\n    infer_eval/utils.DatasetConfig.batch_size = %EVAL_BATCH_SIZE\n    infer_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    infer_eval/utils.DatasetConfig.seed = 0\n    infer_eval/utils.DatasetConfig.shuffle = False\n    infer_eval/utils.DatasetConfig.split = 'validation'\n    infer_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    \n#### Parameters for train/utils.DatasetConfig:\n\n    train/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train/utils.DatasetConfig.pack = True\n    train/utils.DatasetConfig.seed = None\n    train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES\n    train/utils.DatasetConfig.split = 'train'\n    train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for train_eval/utils.DatasetConfig:\n\n    train_eval/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train_eval/utils.DatasetConfig.pack = True\n    train_eval/utils.DatasetConfig.seed = 42\n    train_eval/utils.DatasetConfig.shuffle = False\n    train_eval/utils.DatasetConfig.split = 'validation'\n    train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for models.EncoderDecoderModel:\n\n    models.EncoderDecoderModel.input_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.label_smoothing = %LABEL_SMOOTHING\n    models.EncoderDecoderModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR\n    models.EncoderDecoderModel.module = @network.Transformer()\n    models.EncoderDecoderModel.optimizer_def = %OPTIMIZER\n    models.EncoderDecoderModel.output_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.z_loss = %Z_LOSS\n    \n#### Parameters for seqio.Evaluator:\n\n    seqio.Evaluator.logger_cls = \\\n        [@seqio.PyLoggingLogger, @seqio.TensorBoardLogger, @seqio.JSONLogger]\n    seqio.Evaluator.num_examples = 2048\n    \n#### Parameters for partitioning.PjitPartitioner:\n\n    partitioning.PjitPartitioner.logical_axis_rules = \\\n        @partitioning.standard_logical_axis_rules()\n    partitioning.PjitPartitioner.model_parallel_submesh = None\n    partitioning.PjitPartitioner.num_partitions = 1\n    \n#### Parameters for utils.RestoreCheckpointConfig:\n\n    utils.RestoreCheckpointConfig.path = []\n    \n#### Parameters for utils.SaveCheckpointConfig:\n\n    utils.SaveCheckpointConfig.dtype = 'float32'\n    utils.SaveCheckpointConfig.keep = None\n    utils.SaveCheckpointConfig.period = %SAVING_PERIOD\n    utils.SaveCheckpointConfig.save_dataset = False\n    \n#### Parameters for seqio.SentencePieceVocabulary:\n\n    seqio.SentencePieceVocabulary.extra_ids = 100\n    seqio.SentencePieceVocabulary.sentencepiece_model_file = \\\n        'gs://improved-t5/vocabs/tokenizer.model'\n    \n#### Parameters for partitioning.standard_logical_axis_rules:\n\n    partitioning.standard_logical_axis_rules.activation_partitioning_dims = 1\n    partitioning.standard_logical_axis_rules.parameter_partitioning_dims = 2\n    \n#### Parameters for network.T5Config:\n\n    network.T5Config.dropout_rate = %DROPOUT_RATE\n    network.T5Config.dtype = 'bfloat16'\n    network.T5Config.emb_dim = 4096\n    network.T5Config.head_dim = 64\n    network.T5Config.logits_via_embedding = False\n    network.T5Config.mlp_activations = ('gelu', 'linear')\n    network.T5Config.mlp_dim = 10240\n    network.T5Config.num_decoder_layers = 24\n    network.T5Config.num_encoder_layers = 24\n    network.T5Config.num_heads = 64\n    network.T5Config.remat_policy = 'minimal'\n    network.T5Config.scan_layers = True\n    network.T5Config.vocab_size = 32128\n    \n#### Parameters for train_script.train:\n\n    train_script.train.checkpoint_cfg = @utils.CheckpointConfig()\n    train_script.train.eval_period = %SAVING_PERIOD\n    train_script.train.eval_steps = 100\n    train_script.train.infer_eval_dataset_cfg = @infer_eval/utils.DatasetConfig()\n    train_script.train.inference_evaluator_cls = @seqio.Evaluator\n    train_script.train.model = %MODEL\n    train_script.train.model_dir = %MODEL_DIR\n    train_script.train.partitioner = @partitioning.PjitPartitioner()\n    train_script.train.random_seed = %RANDOM_SEED\n    train_script.train.run_eval_before_training = True\n    train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config\n    train_script.train.total_steps = %TRAIN_STEPS\n    train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()\n    train_script.train.train_eval_dataset_cfg = @train_eval/utils.DatasetConfig()\n    train_script.train.trainer_cls = @trainer.Trainer\n    train_script.train.use_hardware_rng = %USE_HARDWARE_RNG\n    \n#### Parameters for trainer.Trainer:\n\n    trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()\n    trainer.Trainer.num_microbatches = 2\n    \n#### Parameters for network.Transformer:\n\n    network.Transformer.config = @network.T5Config()""",16.966244,4.088131,6.914139e-06,0.0011433268,0.0011396065,145630.86,374.02542
xxl,980000,34.52894,-0.99238706,154813.33,-0.7897643,2048.006,2048.006,154823.77,-0.7898175,-0.7884482,1.0017368,0.99183464,1.0017368,1.2931385,23886.625,20480000.0,857.3836,3.3491547,0.41864434,-82064.64,381.80365,10.408651,-5.3098665e-05,1.034168479539605,"b""    from __gin__ import dynamic_registration\n    import __main__ as train_script\n    import data.pile.tasks\n    from models.scalable_t5 import network\n    import seqio\n    from t5x import adafactor\n    from t5x import gin_utils\n    from t5x import models\n    from t5x import partitioning\n    from t5x import trainer\n    from t5x import utils\n    \n#### Macros:\n\n    BATCH_SIZE = 2048\n    DROPOUT_RATE = 0.0\n    EVAL_BATCH_SIZE = 256\n    LABEL_SMOOTHING = 0.0\n    LOSS_NORMALIZING_FACTOR = None\n    MIXTURE_OR_TASK_MODULE = None\n    MIXTURE_OR_TASK_NAME = 'pile_r_denoiser'\n    MODEL = @models.EncoderDecoderModel()\n    MODEL_DIR = 'gs://improved-t5/ckpts/v2_xxl_mlm/'\n    OPTIMIZER = @adafactor.Adafactor()\n    RANDOM_SEED = None\n    SAVING_PERIOD = 10000\n    SHUFFLE_TRAIN_EXAMPLES = True\n    TASK_FEATURE_LENGTHS = {'inputs': 512, 'targets': 114}\n    TRAIN_STEPS = 1000000\n    USE_CACHED_TASKS = False\n    USE_HARDWARE_RNG = False\n    VOCABULARY = @seqio.SentencePieceVocabulary()\n    Z_LOSS = 0.0001\n    \n#### Parameters for adafactor.Adafactor:\n\n    adafactor.Adafactor.decay_rate = 0.8\n    adafactor.Adafactor.logical_factor_rules = \\\n        @adafactor.standard_logical_factor_rules()\n    adafactor.Adafactor.step_offset = 0\n    \n#### Parameters for utils.CheckpointConfig:\n\n    utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()\n    utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()\n    \n#### Parameters for utils.create_learning_rate_scheduler:\n\n    utils.create_learning_rate_scheduler.base_learning_rate = 1.0\n    utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'\n    utils.create_learning_rate_scheduler.warmup_steps = 10000\n    \n#### Parameters for infer_eval/utils.DatasetConfig:\n\n    infer_eval/utils.DatasetConfig.batch_size = %EVAL_BATCH_SIZE\n    infer_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    infer_eval/utils.DatasetConfig.seed = 0\n    infer_eval/utils.DatasetConfig.shuffle = False\n    infer_eval/utils.DatasetConfig.split = 'validation'\n    infer_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    \n#### Parameters for train/utils.DatasetConfig:\n\n    train/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train/utils.DatasetConfig.pack = True\n    train/utils.DatasetConfig.seed = None\n    train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES\n    train/utils.DatasetConfig.split = 'train'\n    train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for train_eval/utils.DatasetConfig:\n\n    train_eval/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train_eval/utils.DatasetConfig.pack = True\n    train_eval/utils.DatasetConfig.seed = 42\n    train_eval/utils.DatasetConfig.shuffle = False\n    train_eval/utils.DatasetConfig.split = 'validation'\n    train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for models.EncoderDecoderModel:\n\n    models.EncoderDecoderModel.input_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.label_smoothing = %LABEL_SMOOTHING\n    models.EncoderDecoderModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR\n    models.EncoderDecoderModel.module = @network.Transformer()\n    models.EncoderDecoderModel.optimizer_def = %OPTIMIZER\n    models.EncoderDecoderModel.output_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.z_loss = %Z_LOSS\n    \n#### Parameters for seqio.Evaluator:\n\n    seqio.Evaluator.logger_cls = \\\n        [@seqio.PyLoggingLogger, @seqio.TensorBoardLogger, @seqio.JSONLogger]\n    seqio.Evaluator.num_examples = 2048\n    \n#### Parameters for partitioning.PjitPartitioner:\n\n    partitioning.PjitPartitioner.logical_axis_rules = \\\n        @partitioning.standard_logical_axis_rules()\n    partitioning.PjitPartitioner.model_parallel_submesh = None\n    partitioning.PjitPartitioner.num_partitions = 1\n    \n#### Parameters for utils.RestoreCheckpointConfig:\n\n    utils.RestoreCheckpointConfig.path = []\n    \n#### Parameters for utils.SaveCheckpointConfig:\n\n    utils.SaveCheckpointConfig.dtype = 'float32'\n    utils.SaveCheckpointConfig.keep = None\n    utils.SaveCheckpointConfig.period = %SAVING_PERIOD\n    utils.SaveCheckpointConfig.save_dataset = False\n    \n#### Parameters for seqio.SentencePieceVocabulary:\n\n    seqio.SentencePieceVocabulary.extra_ids = 100\n    seqio.SentencePieceVocabulary.sentencepiece_model_file = \\\n        'gs://improved-t5/vocabs/tokenizer.model'\n    \n#### Parameters for partitioning.standard_logical_axis_rules:\n\n    partitioning.standard_logical_axis_rules.activation_partitioning_dims = 1\n    partitioning.standard_logical_axis_rules.parameter_partitioning_dims = 2\n    \n#### Parameters for network.T5Config:\n\n    network.T5Config.dropout_rate = %DROPOUT_RATE\n    network.T5Config.dtype = 'bfloat16'\n    network.T5Config.emb_dim = 4096\n    network.T5Config.head_dim = 64\n    network.T5Config.logits_via_embedding = False\n    network.T5Config.mlp_activations = ('gelu', 'linear')\n    network.T5Config.mlp_dim = 10240\n    network.T5Config.num_decoder_layers = 24\n    network.T5Config.num_encoder_layers = 24\n    network.T5Config.num_heads = 64\n    network.T5Config.remat_policy = 'minimal'\n    network.T5Config.scan_layers = True\n    network.T5Config.vocab_size = 32128\n    \n#### Parameters for train_script.train:\n\n    train_script.train.checkpoint_cfg = @utils.CheckpointConfig()\n    train_script.train.eval_period = %SAVING_PERIOD\n    train_script.train.eval_steps = 100\n    train_script.train.infer_eval_dataset_cfg = @infer_eval/utils.DatasetConfig()\n    train_script.train.inference_evaluator_cls = @seqio.Evaluator\n    train_script.train.model = %MODEL\n    train_script.train.model_dir = %MODEL_DIR\n    train_script.train.partitioner = @partitioning.PjitPartitioner()\n    train_script.train.random_seed = %RANDOM_SEED\n    train_script.train.run_eval_before_training = True\n    train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config\n    train_script.train.total_steps = %TRAIN_STEPS\n    train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()\n    train_script.train.train_eval_dataset_cfg = @train_eval/utils.DatasetConfig()\n    train_script.train.trainer_cls = @trainer.Trainer\n    train_script.train.use_hardware_rng = %USE_HARDWARE_RNG\n    \n#### Parameters for trainer.Trainer:\n\n    trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()\n    trainer.Trainer.num_microbatches = None\n    \n#### Parameters for network.Transformer:\n\n    network.Transformer.config = @network.T5Config()""",14.367246,4.069985,2.026558e-05,0.0010127369,0.001010153,384598.2,25.604137
xxl,120000,50.75834,-0.96525615,183290.36,-0.93503696,2048.0059,2048.0059,183351.25,-0.93534756,-0.9337674,1.0016923,0.9920442,1.0016923,1.2856188,24439.643,20480000.0,837.9827,3.27337,0.40917125,-80207.69,373.16418,60.483044,-0.00030854804,1.04195084221189,"b""    from __gin__ import dynamic_registration\n    import __main__ as train_script\n    import data.pile.tasks\n    from models.scalable_t5 import network\n    import seqio\n    from t5x import adafactor\n    from t5x import gin_utils\n    from t5x import models\n    from t5x import partitioning\n    from t5x import trainer\n    from t5x import utils\n    \n#### Macros:\n\n    BATCH_SIZE = 2048\n    DROPOUT_RATE = 0.0\n    EVAL_BATCH_SIZE = 256\n    LABEL_SMOOTHING = 0.0\n    LOSS_NORMALIZING_FACTOR = None\n    MIXTURE_OR_TASK_MODULE = None\n    MIXTURE_OR_TASK_NAME = 'pile_r_denoiser'\n    MODEL = @models.EncoderDecoderModel()\n    MODEL_DIR = 'gs://improved-t5/ckpts/v2_xxl_mlm/'\n    OPTIMIZER = @adafactor.Adafactor()\n    RANDOM_SEED = None\n    SAVING_PERIOD = 10000\n    SHUFFLE_TRAIN_EXAMPLES = True\n    TASK_FEATURE_LENGTHS = {'inputs': 512, 'targets': 114}\n    TRAIN_STEPS = 1000000\n    USE_CACHED_TASKS = False\n    USE_HARDWARE_RNG = False\n    VOCABULARY = @seqio.SentencePieceVocabulary()\n    Z_LOSS = 0.0001\n    \n#### Parameters for adafactor.Adafactor:\n\n    adafactor.Adafactor.decay_rate = 0.8\n    adafactor.Adafactor.logical_factor_rules = \\\n        @adafactor.standard_logical_factor_rules()\n    adafactor.Adafactor.step_offset = 0\n    \n#### Parameters for utils.CheckpointConfig:\n\n    utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()\n    utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()\n    \n#### Parameters for utils.create_learning_rate_scheduler:\n\n    utils.create_learning_rate_scheduler.base_learning_rate = 1.0\n    utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'\n    utils.create_learning_rate_scheduler.warmup_steps = 10000\n    \n#### Parameters for infer_eval/utils.DatasetConfig:\n\n    infer_eval/utils.DatasetConfig.batch_size = %EVAL_BATCH_SIZE\n    infer_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    infer_eval/utils.DatasetConfig.seed = 0\n    infer_eval/utils.DatasetConfig.shuffle = False\n    infer_eval/utils.DatasetConfig.split = 'validation'\n    infer_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    \n#### Parameters for train/utils.DatasetConfig:\n\n    train/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train/utils.DatasetConfig.pack = True\n    train/utils.DatasetConfig.seed = None\n    train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES\n    train/utils.DatasetConfig.split = 'train'\n    train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for train_eval/utils.DatasetConfig:\n\n    train_eval/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train_eval/utils.DatasetConfig.pack = True\n    train_eval/utils.DatasetConfig.seed = 42\n    train_eval/utils.DatasetConfig.shuffle = False\n    train_eval/utils.DatasetConfig.split = 'validation'\n    train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for models.EncoderDecoderModel:\n\n    models.EncoderDecoderModel.input_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.label_smoothing = %LABEL_SMOOTHING\n    models.EncoderDecoderModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR\n    models.EncoderDecoderModel.module = @network.Transformer()\n    models.EncoderDecoderModel.optimizer_def = %OPTIMIZER\n    models.EncoderDecoderModel.output_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.z_loss = %Z_LOSS\n    \n#### Parameters for seqio.Evaluator:\n\n    seqio.Evaluator.logger_cls = \\\n        [@seqio.PyLoggingLogger, @seqio.TensorBoardLogger, @seqio.JSONLogger]\n    seqio.Evaluator.num_examples = 2048\n    \n#### Parameters for partitioning.PjitPartitioner:\n\n    partitioning.PjitPartitioner.logical_axis_rules = \\\n        @partitioning.standard_logical_axis_rules()\n    partitioning.PjitPartitioner.model_parallel_submesh = None\n    partitioning.PjitPartitioner.num_partitions = 1\n    \n#### Parameters for utils.RestoreCheckpointConfig:\n\n    utils.RestoreCheckpointConfig.path = []\n    \n#### Parameters for utils.SaveCheckpointConfig:\n\n    utils.SaveCheckpointConfig.dtype = 'float32'\n    utils.SaveCheckpointConfig.keep = None\n    utils.SaveCheckpointConfig.period = %SAVING_PERIOD\n    utils.SaveCheckpointConfig.save_dataset = False\n    \n#### Parameters for seqio.SentencePieceVocabulary:\n\n    seqio.SentencePieceVocabulary.extra_ids = 100\n    seqio.SentencePieceVocabulary.sentencepiece_model_file = \\\n        'gs://improved-t5/vocabs/tokenizer.model'\n    \n#### Parameters for partitioning.standard_logical_axis_rules:\n\n    partitioning.standard_logical_axis_rules.activation_partitioning_dims = 1\n    partitioning.standard_logical_axis_rules.parameter_partitioning_dims = 2\n    \n#### Parameters for network.T5Config:\n\n    network.T5Config.dropout_rate = %DROPOUT_RATE\n    network.T5Config.dtype = 'bfloat16'\n    network.T5Config.emb_dim = 4096\n    network.T5Config.head_dim = 64\n    network.T5Config.logits_via_embedding = False\n    network.T5Config.mlp_activations = ('gelu', 'linear')\n    network.T5Config.mlp_dim = 10240\n    network.T5Config.num_decoder_layers = 24\n    network.T5Config.num_encoder_layers = 24\n    network.T5Config.num_heads = 64\n    network.T5Config.remat_policy = 'minimal'\n    network.T5Config.scan_layers = True\n    network.T5Config.vocab_size = 32128\n    \n#### Parameters for train_script.train:\n\n    train_script.train.checkpoint_cfg = @utils.CheckpointConfig()\n    train_script.train.eval_period = %SAVING_PERIOD\n    train_script.train.eval_steps = 100\n    train_script.train.infer_eval_dataset_cfg = @infer_eval/utils.DatasetConfig()\n    train_script.train.inference_evaluator_cls = @seqio.Evaluator\n    train_script.train.model = %MODEL\n    train_script.train.model_dir = %MODEL_DIR\n    train_script.train.partitioner = @partitioning.PjitPartitioner()\n    train_script.train.random_seed = %RANDOM_SEED\n    train_script.train.run_eval_before_training = True\n    train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config\n    train_script.train.total_steps = %TRAIN_STEPS\n    train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()\n    train_script.train.train_eval_dataset_cfg = @train_eval/utils.DatasetConfig()\n    train_script.train.trainer_cls = @trainer.Trainer\n    train_script.train.use_hardware_rng = %USE_HARDWARE_RNG\n    \n#### Parameters for trainer.Trainer:\n\n    trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()\n    trainer.Trainer.num_microbatches = None\n    \n#### Parameters for network.Transformer:\n\n    network.Transformer.config = @network.T5Config()""",5.241361,23.446936,1.5258789e-05,0.0029495438,0.0028867633,73650.79,17.696238
xxl,130000,,-0.9673388,181070.77,-0.9237139,2048.0073,2048.0073,181129.45,-0.9240133,-0.92243683,1.001709,0.9919657,1.001709,1.288438,24622.572,20480000.0,831.75714,3.2490513,0.40613142,-79611.8,370.39185,58.268723,-0.0002972519,1.0415308742038185,,,4.1117706,,0.0028289973,0.002773512,24809.557,22.455696
xxl,140000,,-0.96865994,179560.5,-0.9160095,2048.0059,2048.0059,179616.11,-0.91629314,-0.91474265,1.001695,0.99203104,1.001695,1.2860892,24619.553,20480000.0,831.85913,3.2494497,0.40618122,-79621.56,370.43726,55.807262,-0.000284695,1.0410761404734876,,,4.079954,,0.0027221239,0.002672622,49512.215,21.371796
xxl,150000,,-0.9703997,177703.66,-0.90653694,2048.0063,2048.0063,177756.55,-0.9068067,-0.9052708,1.0016967,0.99202406,1.0016967,1.2863438,24619.514,20480000.0,831.8605,3.249455,0.40618187,-79621.69,370.43787,53.461292,-0.00027272728,1.040786774303144,,,4.107087,,0.0026265252,0.0025819975,74213.76,18.009739
xxl,160000,,-0.97159344,176522.11,-0.9005094,2048.0068,2048.0068,176573.17,-0.90076995,-0.8992474,1.0016931,0.9920404,1.0016931,1.2857555,24619.678,20480000.0,831.85486,3.249433,0.40617913,-79621.16,370.43536,51.08857,-0.00026062308,1.0402673484176106,,,4.1076956,,0.0025403376,0.0025000079,98912.055,19.579523
xxl,170000,,-0.9728303,175165.19,-0.89358723,2048.0083,2048.0083,175213.78,-0.8938351,-0.892325,1.0016924,0.9920441,1.0016924,1.2856249,24619.623,20480000.0,831.85675,3.2494404,0.40618005,-79621.336,370.4362,48.77495,-0.0002488204,1.0400175783691459,,,4.1036243,,0.0024621193,0.0024253633,123611.96,18.015282
xxl,180000,,-0.9742886,173628.84,-0.88574976,2048.007,2048.007,173675.67,-0.8859886,-0.88449425,1.0016894,0.992057,1.0016894,1.2851559,24619.504,20480000.0,831.8608,3.2494562,0.40618202,-79621.72,370.438,46.675518,-0.00023811035,1.039755062949762,,,4.1076026,,0.0023907027,0.0023570291,148310.14,18.568035
xxl,190000,,-0.9752716,172641.56,-0.88071316,2048.0068,2048.0068,172686.4,-0.880942,-0.87946117,1.0016837,0.9920845,1.0016837,1.2841733,24619.965,20480000.0,831.84515,3.2493951,0.4061744,-79620.23,370.43106,44.581013,-0.00022742545,1.0395301163567674,,,4.121216,,0.0023251679,0.0022941635,173009.28,18.227371
xxl,200000,,-0.97631884,171475.23,-0.8747633,2048.007,2048.007,171518.81,-0.87498564,-0.87350863,1.0016909,0.9920508,1.0016909,1.2853826,24619.621,20480000.0,831.8568,3.2494407,0.40618008,-79621.34,370.43622,43.50418,-0.0002219321,1.0391524845913023,,,4.082382,,0.002264747,0.0022360736,197707.86,19.889317
xxl,210000,,-0.97694856,170770.0,-0.8711656,2048.008,2048.008,170811.72,-0.8713784,-0.8699113,1.0016866,0.9920704,1.0016866,1.2846751,24619.57,20480000.0,831.8585,3.2494473,0.40618092,-79621.51,370.43698,42.280285,-0.00021568852,1.0389452330900717,,,4.0820866,,0.0022087973,0.002182184,222407.9,22.501873
xxl,220000,,-0.9781108,169599.3,-0.8651934,2048.0063,2048.0063,169640.88,-0.8654055,-0.8639482,1.0016868,0.9920699,1.0016868,1.2846953,24619.6,20480000.0,831.85754,3.2494435,0.40618044,-79621.414,370.43655,41.386326,-0.00021112809,1.0387848053570228,,,4.084709,,0.0021567962,0.002132012,247110.6,18.113464
xxl,230000,,-0.9790752,168539.77,-0.85978836,2048.007,2048.007,168579.38,-0.85999036,-0.85854447,1.0016841,0.9920827,1.0016841,1.2842371,24619.797,20480000.0,831.8508,3.2494173,0.40617716,-79620.77,370.43356,39.37212,-0.00020085282,1.0385615728783357,,,4.0996017,,0.0021083185,0.0020851486,271809.4,17.941029
xxl,240000,,-0.97996104,167603.69,-0.855013,2048.0076,2048.0076,167641.72,-0.855207,-0.8537714,1.0016816,0.992095,1.0016816,1.283796,24619.547,20480000.0,831.8593,3.2494504,0.4061813,-79621.586,370.43735,38.35296,-0.00019565367,1.038331589776071,,,4.119388,,0.0020629603,0.0020412458,296507.6,18.361149
xxl,250000,,-0.980692,166816.84,-0.850999,2048.0076,2048.0076,166854.56,-0.8511914,-0.8497601,1.0016843,0.9920817,1.0016843,1.2842742,24619.646,20480000.0,831.85596,3.2494373,0.40617967,-79621.266,370.43585,37.426903,-0.0001909295,1.038263247504389,,,4.098937,,0.00202042,0.002000004,321206.22,18.65976
xxl,260000,,-0.98146784,166030.12,-0.84698564,2048.007,2048.007,166066.77,-0.8471725,-0.84575194,1.0016797,0.9921033,1.0016797,1.283497,24619.607,20480000.0,831.85724,3.2494423,0.4061803,-79621.38,370.43643,36.516167,-0.00018628348,1.0379975194650197,,,4.097321,,0.001980391,0.001961165,345905.06,24.573591
xxl,270000,,-0.98176324,165704.08,-0.8453224,2048.0063,2048.0063,165739.73,-0.8455042,-0.84407145,1.0016974,0.99202025,1.0016974,1.2864814,24620.174,20480000.0,831.8382,3.249368,0.406171,-79619.555,370.42795,35.64979,-0.00018186375,1.0378934149962393,,,4.0901556,,0.0019426537,0.0019245044,370610.4,21.017042
xxl,280000,,-0.9833987,163958.56,-0.8364178,2048.0056,2048.0056,163993.08,-0.83659387,-0.83519274,1.0016776,0.99211276,1.0016776,1.2831545,24619.615,20480000.0,831.85706,3.2494416,0.4061802,-79621.36,370.43634,34.692963,-0.00017698258,1.037667903406845,,,4.0880947,,0.0019070015,0.0018898257,395311.75,17.898497
xxl,290000,,-0.98407763,163141.62,-0.83225024,2048.0085,2048.0085,163175.34,-0.83242226,-0.8310288,1.0016768,0.9921173,1.0016768,1.2829953,24619.586,20480000.0,831.858,3.2494452,0.40618065,-79621.45,370.43677,33.918797,-0.00017303326,1.037539871349585,,,4.0874977,,0.001873255,0.0018569566,420009.94,17.213734
xxl,300000,,-0.983643,163727.6,-0.8352395,2048.0076,2048.0076,163760.88,-0.8354093,-0.834,1.0016899,0.9920555,1.0016899,1.2852125,24619.605,20480000.0,831.8573,3.2494426,0.40618032,-79621.39,370.43646,33.282085,-0.00016978513,1.0373869901404151,,,4.1102467,,0.0018412152,0.001825745,444707.34,18.599382
xxl,1630000,21.37018,-1.001067,145747.67,-0.74351674,2048.0063,2048.0063,145756.77,-0.7435632,-0.74232984,1.0016614,0.9921895,1.0016614,1.2804042,13891.888,20480000.0,1474.2418,2.8793786,0.71984464,-141107.34,328.24915,9.392018,-4.7912417e-05,1.0337389285452396,"b""    from __gin__ import dynamic_registration\n    import __main__ as train_script\n    import data.pile.tasks\n    from models.scalable_t5 import network\n    import seqio\n    from t5x import adafactor\n    from t5x import gin_utils\n    from t5x import models\n    from t5x import partitioning\n    from t5x import trainer\n    from t5x import utils\n    \n#### Macros:\n\n    BATCH_SIZE = 2048\n    DROPOUT_RATE = 0.0\n    EVAL_BATCH_SIZE = 256\n    LABEL_SMOOTHING = 0.0\n    LOSS_NORMALIZING_FACTOR = None\n    MIXTURE_OR_TASK_MODULE = None\n    MIXTURE_OR_TASK_NAME = 'pile_r_denoiser'\n    MODEL = @models.EncoderDecoderModel()\n    MODEL_DIR = 'gs://improved-t5/ckpts/v2_xxl_mlm/'\n    OPTIMIZER = @adafactor.Adafactor()\n    RANDOM_SEED = None\n    SAVING_PERIOD = 10000\n    SHUFFLE_TRAIN_EXAMPLES = True\n    TASK_FEATURE_LENGTHS = {'inputs': 512, 'targets': 114}\n    TRAIN_STEPS = 2000000\n    USE_CACHED_TASKS = False\n    USE_HARDWARE_RNG = False\n    VOCABULARY = @seqio.SentencePieceVocabulary()\n    Z_LOSS = 0.0001\n    \n#### Parameters for adafactor.Adafactor:\n\n    adafactor.Adafactor.decay_rate = 0.8\n    adafactor.Adafactor.logical_factor_rules = \\\n        @adafactor.standard_logical_factor_rules()\n    adafactor.Adafactor.step_offset = 0\n    \n#### Parameters for utils.CheckpointConfig:\n\n    utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()\n    utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()\n    \n#### Parameters for utils.create_learning_rate_scheduler:\n\n    utils.create_learning_rate_scheduler.base_learning_rate = 1.0\n    utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'\n    utils.create_learning_rate_scheduler.warmup_steps = 10000\n    \n#### Parameters for infer_eval/utils.DatasetConfig:\n\n    infer_eval/utils.DatasetConfig.batch_size = %EVAL_BATCH_SIZE\n    infer_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    infer_eval/utils.DatasetConfig.seed = 0\n    infer_eval/utils.DatasetConfig.shuffle = False\n    infer_eval/utils.DatasetConfig.split = 'validation'\n    infer_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    \n#### Parameters for train/utils.DatasetConfig:\n\n    train/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train/utils.DatasetConfig.pack = True\n    train/utils.DatasetConfig.seed = None\n    train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES\n    train/utils.DatasetConfig.split = 'train'\n    train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for train_eval/utils.DatasetConfig:\n\n    train_eval/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train_eval/utils.DatasetConfig.pack = True\n    train_eval/utils.DatasetConfig.seed = 42\n    train_eval/utils.DatasetConfig.shuffle = False\n    train_eval/utils.DatasetConfig.split = 'validation'\n    train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for models.EncoderDecoderModel:\n\n    models.EncoderDecoderModel.input_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.label_smoothing = %LABEL_SMOOTHING\n    models.EncoderDecoderModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR\n    models.EncoderDecoderModel.module = @network.Transformer()\n    models.EncoderDecoderModel.optimizer_def = %OPTIMIZER\n    models.EncoderDecoderModel.output_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.z_loss = %Z_LOSS\n    \n#### Parameters for seqio.Evaluator:\n\n    seqio.Evaluator.logger_cls = \\\n        [@seqio.PyLoggingLogger, @seqio.TensorBoardLogger, @seqio.JSONLogger]\n    seqio.Evaluator.num_examples = 2048\n    \n#### Parameters for partitioning.PjitPartitioner:\n\n    partitioning.PjitPartitioner.logical_axis_rules = \\\n        @partitioning.standard_logical_axis_rules()\n    partitioning.PjitPartitioner.model_parallel_submesh = None\n    partitioning.PjitPartitioner.num_partitions = 4\n    \n#### Parameters for utils.RestoreCheckpointConfig:\n\n    utils.RestoreCheckpointConfig.path = []\n    \n#### Parameters for utils.SaveCheckpointConfig:\n\n    utils.SaveCheckpointConfig.dtype = 'float32'\n    utils.SaveCheckpointConfig.keep = None\n    utils.SaveCheckpointConfig.period = %SAVING_PERIOD\n    utils.SaveCheckpointConfig.save_dataset = False\n    \n#### Parameters for seqio.SentencePieceVocabulary:\n\n    seqio.SentencePieceVocabulary.extra_ids = 100\n    seqio.SentencePieceVocabulary.sentencepiece_model_file = \\\n        'gs://improved-t5/vocabs/tokenizer.model'\n    \n#### Parameters for network.T5Config:\n\n    network.T5Config.dropout_rate = %DROPOUT_RATE\n    network.T5Config.dtype = 'bfloat16'\n    network.T5Config.emb_dim = 4096\n    network.T5Config.head_dim = 64\n    network.T5Config.logits_via_embedding = False\n    network.T5Config.mlp_activations = ('gelu', 'linear')\n    network.T5Config.mlp_dim = 10240\n    network.T5Config.num_decoder_layers = 24\n    network.T5Config.num_encoder_layers = 24\n    network.T5Config.num_heads = 64\n    network.T5Config.remat_policy = 'minimal'\n    network.T5Config.scan_layers = True\n    network.T5Config.vocab_size = 32128\n    \n#### Parameters for train_script.train:\n\n    train_script.train.checkpoint_cfg = @utils.CheckpointConfig()\n    train_script.train.eval_period = %SAVING_PERIOD\n    train_script.train.eval_steps = 100\n    train_script.train.infer_eval_dataset_cfg = @infer_eval/utils.DatasetConfig()\n    train_script.train.inference_evaluator_cls = @seqio.Evaluator\n    train_script.train.model = %MODEL\n    train_script.train.model_dir = %MODEL_DIR\n    train_script.train.partitioner = @partitioning.PjitPartitioner()\n    train_script.train.random_seed = %RANDOM_SEED\n    train_script.train.run_eval_before_training = True\n    train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config\n    train_script.train.total_steps = %TRAIN_STEPS\n    train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()\n    train_script.train.train_eval_dataset_cfg = @train_eval/utils.DatasetConfig()\n    train_script.train.trainer_cls = @trainer.Trainer\n    train_script.train.use_hardware_rng = %USE_HARDWARE_RNG\n    \n#### Parameters for trainer.Trainer:\n\n    trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()\n    trainer.Trainer.num_microbatches = None\n    \n#### Parameters for network.Transformer:\n\n    network.Transformer.config = @network.T5Config()""",131.34859,0.96347976,5.4836273e-06,0.00078447256,0.0007832607,224632.48,46.35793
xxl,1640000,,-0.9987724,148126.7,-0.75565314,2048.007,2048.007,148136.45,-0.7557029,-0.75441754,1.0017039,0.9919901,1.0017039,1.2875615,13953.686,20480000.0,1467.7126,2.8666263,0.71665657,-140482.4,326.7954,9.825603,-5.01243e-05,1.0337351116884204,,,0.96133757,,0.0007820694,0.00078086904,14098.161,53.21908
xxl,1650000,,-0.9984354,148425.45,-0.75717723,2048.0066,2048.0066,148435.39,-0.7572279,-0.75594276,1.0017,0.9920079,1.0017,1.2869215,13952.506,20480000.0,1467.8367,2.8668685,0.7167171,-140494.28,326.823,9.750137,-4.9739323e-05,1.0337368241265363,,,0.97751,,0.0007796874,0.0007784992,28139.537,51.86995
xxl,1660000,,-0.99876523,148147.73,-0.75576043,2048.0068,2048.0068,148157.42,-0.7558099,-0.7545324,1.0016931,0.9920399,1.0016931,1.2857695,13952.571,20480000.0,1467.8298,2.8668551,0.7167138,-140493.62,326.82147,9.750574,-4.9741553e-05,1.0337159824030084,,,0.9632845,,0.00077732676,0.00077615073,42179.645,45.036064
xxl,1670000,,-0.9986639,148293.47,-0.7565039,2048.0068,2048.0068,148303.31,-0.75655407,-0.7552692,1.0017011,0.99200314,1.0017011,1.2870966,13952.504,20480000.0,1467.8369,2.866869,0.71671724,-140494.31,326.82306,9.643335,-4.919448e-05,1.0337197963373257,,,0.969641,,0.00077498663,0.00077382347,56212.883,48.151794
xxl,1680000,,-0.9979081,149072.16,-0.7604763,2048.0085,2048.0085,149081.86,-0.76052576,-0.7592377,1.0016965,0.99202424,1.0016965,1.2863343,13952.488,20480000.0,1467.8386,2.8668723,0.7167181,-140494.47,326.82343,9.4711,-4.8315844e-05,1.033710976623919,,,0.96538997,,0.000772665,0.00077151705,70249.29,373.62476
xxl,1690000,,-0.99819386,148799.44,-0.759085,2048.0063,2048.0063,148808.94,-0.7591335,-0.75784355,1.0017022,0.9919973,1.0017022,1.2872996,13952.575,20480000.0,1467.8293,2.8668542,0.71671355,-140493.58,326.82138,9.444985,-4.818262e-05,1.0337034260589992,,,0.9816272,,0.00077035814,0.000769231,84611.21,45.617558
xxl,1700000,,-0.9982465,148757.27,-0.75886995,2048.0083,2048.0083,148766.73,-0.7589182,-0.75763255,1.001697,0.992022,1.001697,1.2864162,13952.525,20480000.0,1467.8347,2.8668647,0.7167162,-140494.1,326.82257,9.46462,-4.828278e-05,1.0336658792147493,,,0.95955515,,0.00076810434,0.0007669652,98645.016,372.45813
xxl,1710000,,-0.99837357,148615.4,-0.7581462,2048.0076,2048.0076,148624.83,-0.75819427,-0.7569052,1.0017031,0.9919938,1.0017031,1.2874318,13952.498,20480000.0,1467.8375,2.8668702,0.71671754,-140494.36,326.8232,9.433589,-4.8124486e-05,1.0336882367824607,,,0.96954226,,0.00076584175,0.0007647193,113005.63,45.397797
xxl,1720000,,-0.9982085,148755.23,-0.7588596,2048.0076,2048.0076,148764.69,-0.75890774,-0.7576222,1.0016968,0.99202245,1.0016968,1.2863964,13952.546,20480000.0,1467.8325,2.8668604,0.7167151,-140493.88,326.82208,9.447495,-4.8195427e-05,1.0336854670619409,,,0.97393227,,0.0007636021,0.0007624931,127039.266,56.988422
xxl,1730000,,-0.9984847,148510.45,-0.75761086,2048.0059,2048.0059,148519.89,-0.75765896,-0.7563774,1.0016943,0.99203485,1.0016943,1.2859569,13952.508,20480000.0,1467.8365,2.8668683,0.71671706,-140494.27,326.82297,9.457727,-4.8247628e-05,1.0336636987916956,,,0.9957812,,0.00076139066,0.0007602861,141084.58,43.834312
xxl,1740000,,-0.9986233,148345.14,-0.7567675,2048.007,2048.007,148354.2,-0.7568137,-0.7555374,1.0016892,0.99205834,1.0016892,1.2851104,13952.595,20480000.0,1467.8273,2.8668501,0.71671253,-140493.38,326.82092,9.431621,-4.8114445e-05,1.0336670040304607,,,0.97959495,,0.00075918884,0.00075809826,155116.66,47.714127
xxl,1750000,,-0.99865246,148286.2,-0.75646687,2048.0078,2048.0078,148295.5,-0.75651425,-0.7552329,1.0016967,0.9920238,1.0016967,1.2863519,13952.84,20480000.0,1467.8015,2.8667998,0.71669996,-140490.9,326.8152,9.437788,-4.8145906e-05,1.033643398641342,,,0.96314454,,0.00075700623,0.00075592916,169152.92,60.28837
xxl,1760000,,-0.99887085,148076.86,-0.75539887,2048.007,2048.007,148085.97,-0.75544536,-0.7541701,1.001691,0.9920508,1.001691,1.2853829,13952.515,20480000.0,1467.8357,2.8668666,0.71671665,-140494.19,326.8228,9.426877,-4.8090245e-05,1.0336372097332291,,,0.96970105,,0.0007548546,0.0007537786,183201.39,372.22327
xxl,1770000,,-0.99899894,147960.06,-0.75480306,2048.0051,2048.0051,147969.86,-0.754853,-0.75358146,1.0016873,0.9920672,1.0016873,1.2847912,13952.472,20480000.0,1467.8403,2.8668756,0.7167189,-140494.64,326.82382,9.429321,-4.8102713e-05,1.0336174897313049,,,0.98053336,,0.00075270765,0.00075164624,197561.73,47.261665
xxl,1780000,,-0.9999094,147010.12,-0.749957,2048.0078,2048.0078,147019.9,-0.750007,-0.7487432,1.0016879,0.99206513,1.0016879,1.2848688,13952.546,20480000.0,1467.8325,2.8668604,0.7167151,-140493.88,326.82208,9.475541,-4.83385e-05,1.033630919812043,,,0.9679642,,0.00075058796,0.0007495319,211597.36,49.015938
xxl,1790000,,-1.0028397,143962.14,-0.734408,2048.0078,2048.0078,143971.84,-0.7344576,-0.7332458,1.0016526,0.9922307,1.0016526,1.2789248,13952.506,20480000.0,1467.8367,2.8668685,0.7167171,-140494.28,326.823,9.338063,-4.7637168e-05,1.033625483660317,,,0.96457124,,0.00074848067,0.0007474353,225634.5,47.284847
xxl,1800000,,-1.0027324,144067.11,-0.73494357,2048.0068,2048.0068,144076.05,-0.73498917,-0.73377395,1.0016562,0.99221396,1.0016562,1.279525,13952.497,20480000.0,1467.8376,2.8668704,0.7167176,-140494.38,326.82324,9.217174,-4.7020465e-05,1.0336489520152508,,,0.9883325,,0.0007463927,0.00074535626,239669.94,44.357998
xxl,1810000,,-1.0016308,145217.62,-0.7408128,2048.008,2048.008,145227.23,-0.74086183,-0.73962694,1.0016696,0.99215096,1.0016696,1.2817876,13952.572,20480000.0,1467.8296,2.8668547,0.71671367,-140493.61,326.82144,9.468514,-4.8302656e-05,1.0336254654492245,,,0.9734819,,0.0007443258,0.00074329437,253702.53,371.60776
xxl,1820000,,-0.9997771,147062.08,-0.7502221,2048.0088,2048.0088,147071.67,-0.750271,-0.74900657,1.0016882,0.99206364,1.0016882,1.2849224,13952.508,20480000.0,1467.8365,2.8668683,0.71671706,-140494.27,326.82297,9.694335,-4.9454655e-05,1.0336326150213326,,,0.97926974,,0.0007422699,0.00074124953,268062.34,383.4582
xxl,1830000,,-0.9998969,146938.56,-0.74959195,2048.007,2048.007,146948.2,-0.7496412,-0.7483787,1.001687,0.99206907,1.001687,1.2847259,13952.475,20480000.0,1467.84,2.866875,0.71671873,-140494.6,326.82373,9.669273,-4.9326805e-05,1.0336129156102931,,,0.9737084,,0.0007402374,0.00073922146,282433.97,371.94623
xxl,1840000,,-1.0000492,146814.58,-0.7489595,2048.0076,2048.0076,146824.1,-0.74900806,-0.7477416,1.0016937,0.9920372,1.0016937,1.2858703,13952.555,20480000.0,1467.8315,2.8668585,0.7167146,-140493.8,326.82187,9.641242,-4.9183807e-05,1.0336068187876128,,,0.9735768,,0.0007382143,0.00073721,296794.3,372.1525
xxl,1850000,,-0.9996432,147284.66,-0.75135756,2048.0056,2048.0056,147293.81,-0.7514042,-0.7501354,1.0016915,0.9920486,1.0016915,1.2854633,13952.506,20480000.0,1467.8367,2.8668685,0.7167171,-140494.28,326.823,9.494779,-4.843664e-05,1.033570075654607,,,0.9786694,,0.00073621207,0.00073521485,311154.7,48.152588
xxl,1860000,,-0.9993934,147556.14,-0.7527425,2048.008,2048.008,147564.83,-0.75278676,-0.75151783,1.0016885,0.99206215,1.0016885,1.2849767,13953.197,20480000.0,1467.7639,2.8667264,0.7166816,-140487.31,326.80682,9.413606,-4.8022543e-05,1.033570516859278,,,0.98250127,,0.00073421677,0.00073323573,325191.72,46.855427
xxl,1870000,,-0.9995032,147451.11,-0.7522067,2048.0059,2048.0059,147460.38,-0.75225395,-0.75098306,1.0016923,0.99204403,1.0016923,1.2856251,13952.543,20480000.0,1467.8328,2.8668609,0.7167152,-140493.9,326.82214,9.397019,-4.793793e-05,1.0335697165377584,,,0.9823835,,0.00073224993,0.0007312726,339226.8,372.40665
xxl,1880000,,-0.99959344,147371.64,-0.7518013,2048.007,2048.007,147381.25,-0.7518503,-0.75058645,1.0016838,0.99208486,1.0016838,1.2841629,13952.509,20480000.0,1467.8363,2.8668678,0.71671695,-140494.25,326.82294,9.423099,-4.807097e-05,1.0335578062868718,,,0.9791672,,0.0007303058,0.0007293251,353587.4,47.03611
xxl,1890000,,-0.99950093,147428.16,-0.7520896,2048.0068,2048.0068,147437.7,-0.7521383,-0.75087076,1.0016881,0.9920636,1.0016881,1.2849203,13952.482,20480000.0,1467.8392,2.8668735,0.7167184,-140494.52,326.82358,9.403238,-4.7969654e-05,1.0335647861028934,,,0.97431684,,0.0007283683,0.00072739314,367622.56,46.577183
xxl,1900000,,-0.99967134,147246.53,-0.751163,2048.0085,2048.0085,147256.1,-0.7512119,-0.749944,1.0016906,0.9920526,1.0016906,1.2853215,13952.563,20480000.0,1467.8306,2.8668566,0.71671414,-140493.69,326.82166,9.41152,-4.80119e-05,1.033537516157137,,,0.9715843,,0.000726443,0.0007254765,381657.47,46.293217
xxl,1910000,,-0.99976754,147187.58,-0.7508623,2048.0054,2048.0054,147197.05,-0.75091064,-0.749648,1.0016843,0.99208134,1.0016843,1.2842834,13952.785,20480000.0,1467.8073,2.866811,0.71670276,-140491.47,326.81647,9.418195,-4.8045957e-05,1.0335392648533779,,,0.9639516,,0.0007245341,0.00072357483,395692.25,372.16412
xxl,1920000,,-0.99997616,146935.77,-0.7495777,2048.0063,2048.0063,146944.72,-0.7496234,-0.74836266,1.0016847,0.99207985,1.0016847,1.2843394,13952.496,20480000.0,1467.8376,2.8668704,0.7167176,-140494.38,326.82324,9.403668,-4.7971847e-05,1.0335474167906717,,,0.9850838,,0.0007226429,0.00072168803,410052.66,372.77576
xxl,1930000,,-0.9999218,146979.58,-0.7498012,2048.0068,2048.0068,146989.06,-0.7498496,-0.74858946,1.0016834,0.992086,1.0016834,1.2841194,13952.564,20480000.0,1467.8306,2.8668566,0.71671414,-140493.69,326.82166,9.403175,-4.7969334e-05,1.0335307692057931,,,0.9727299,,0.00072075875,0.0007198159,424413.7,46.736885
xxl,1940000,,-1.0001802,146719.38,-0.7484739,2048.0088,2048.0088,146728.8,-0.74852186,-0.7472636,1.0016838,0.99208385,1.0016838,1.284195,13952.488,20480000.0,1467.8386,2.8668723,0.7167181,-140494.47,326.82343,9.408667,-4.7997342e-05,1.0335196442351908,,,0.96781325,,0.0007188838,0.0007179584,438448.6,43.942867
xxl,1950000,,-1.0003188,146628.06,-0.748008,2048.006,2048.006,146637.17,-0.74805444,-0.7467992,1.0016809,0.9920973,1.0016809,1.2837083,13952.485,20480000.0,1467.8389,2.8668728,0.7167182,-140494.5,326.8235,9.4193735,-4.8051967e-05,1.0335131929444055,,,0.98221684,,0.00071702193,0.00071611506,452481.03,374.14963
xxl,1960000,,-0.99913245,147842.28,-0.7542022,2048.0076,2048.0076,147852.36,-0.7542536,-0.7529716,1.0017027,0.99199545,1.0017027,1.2873706,13952.559,20480000.0,1467.8312,2.8668578,0.71671444,-140493.75,326.82178,9.640392,-4.9179474e-05,1.0335213147355302,,,0.9797163,,0.0007151901,0.0007142859,466843.4,43.5968
xxl,1970000,,-0.99915636,147817.2,-0.7540743,2048.007,2048.007,147826.23,-0.75412035,-0.75283915,1.0017018,0.9919991,1.0017018,1.2872382,13952.795,20480000.0,1467.8063,2.8668091,0.7167023,-140491.38,326.81622,9.481358,-4.8368172e-05,1.0335175693204501,,,0.9898071,,0.0007133783,0.0007124707,480875.53,43.288326
xxl,1980000,,-0.9995126,147477.33,-0.75234044,2048.0068,2048.0068,147487.11,-0.7523904,-0.75111324,1.0017003,0.99200714,1.0017003,1.2869529,13952.498,20480000.0,1467.8375,2.8668702,0.71671754,-140494.36,326.8232,9.506066,-4.849422e-05,1.0334932848213836,,,0.9883096,,0.00071157695,0.0007106692,494907.0,45.02446
xxl,1990000,,-0.99969405,147233.8,-0.7510981,2048.0059,2048.0059,147243.48,-0.7511475,-0.74987423,1.0016979,0.9920174,1.0016979,1.2865809,13952.5625,20480000.0,1467.8307,2.8668568,0.7167142,-140493.7,326.8217,9.721772,-4.9594622e-05,1.0334802141435453,,,1.003701,,0.00070977944,0.0007088814,508940.25,44.740685
xxl,2000000,,-0.99983454,147022.95,-0.7500225,2048.0076,2048.0076,147032.66,-0.75007194,-0.74880224,1.0016958,0.9920282,1.0016958,1.2861947,13952.522,20480000.0,1467.835,2.8668652,0.7167163,-140494.12,326.82263,9.713107,-4.955042e-05,1.0334668571989254,,,0.9748175,,0.0007079785,0.00070710696,522973.2,45.290688
xxl,1000000,20.56852,-0.99711293,149720.45,-0.7637835,2048.008,2048.008,149730.7,-0.76383585,-0.7625431,1.0016953,0.9920297,1.0016953,1.2861371,23936.895,20480000.0,855.583,3.3421211,0.41776514,-81892.3,381.00183,10.183707,-5.1951138e-05,1.0341362630553987,"b""    from __gin__ import dynamic_registration\n    import __main__ as train_script\n    import data.pile.tasks\n    from models.scalable_t5 import network\n    import seqio\n    from t5x import adafactor\n    from t5x import gin_utils\n    from t5x import models\n    from t5x import partitioning\n    from t5x import trainer\n    from t5x import utils\n    \n#### Macros:\n\n    BATCH_SIZE = 2048\n    DROPOUT_RATE = 0.0\n    EVAL_BATCH_SIZE = 256\n    LABEL_SMOOTHING = 0.0\n    LOSS_NORMALIZING_FACTOR = None\n    MIXTURE_OR_TASK_MODULE = None\n    MIXTURE_OR_TASK_NAME = 'pile_r_denoiser'\n    MODEL = @models.EncoderDecoderModel()\n    MODEL_DIR = 'gs://improved-t5/ckpts/v2_xxl_mlm/'\n    OPTIMIZER = @adafactor.Adafactor()\n    RANDOM_SEED = None\n    SAVING_PERIOD = 10000\n    SHUFFLE_TRAIN_EXAMPLES = True\n    TASK_FEATURE_LENGTHS = {'inputs': 512, 'targets': 114}\n    TRAIN_STEPS = 2000000\n    USE_CACHED_TASKS = False\n    USE_HARDWARE_RNG = False\n    VOCABULARY = @seqio.SentencePieceVocabulary()\n    Z_LOSS = 0.0001\n    \n#### Parameters for adafactor.Adafactor:\n\n    adafactor.Adafactor.decay_rate = 0.8\n    adafactor.Adafactor.logical_factor_rules = \\\n        @adafactor.standard_logical_factor_rules()\n    adafactor.Adafactor.step_offset = 0\n    \n#### Parameters for utils.CheckpointConfig:\n\n    utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()\n    utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()\n    \n#### Parameters for utils.create_learning_rate_scheduler:\n\n    utils.create_learning_rate_scheduler.base_learning_rate = 1.0\n    utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'\n    utils.create_learning_rate_scheduler.warmup_steps = 10000\n    \n#### Parameters for infer_eval/utils.DatasetConfig:\n\n    infer_eval/utils.DatasetConfig.batch_size = %EVAL_BATCH_SIZE\n    infer_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    infer_eval/utils.DatasetConfig.seed = 0\n    infer_eval/utils.DatasetConfig.shuffle = False\n    infer_eval/utils.DatasetConfig.split = 'validation'\n    infer_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    \n#### Parameters for train/utils.DatasetConfig:\n\n    train/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train/utils.DatasetConfig.pack = True\n    train/utils.DatasetConfig.seed = None\n    train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES\n    train/utils.DatasetConfig.split = 'train'\n    train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for train_eval/utils.DatasetConfig:\n\n    train_eval/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train_eval/utils.DatasetConfig.pack = True\n    train_eval/utils.DatasetConfig.seed = 42\n    train_eval/utils.DatasetConfig.shuffle = False\n    train_eval/utils.DatasetConfig.split = 'validation'\n    train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for models.EncoderDecoderModel:\n\n    models.EncoderDecoderModel.input_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.label_smoothing = %LABEL_SMOOTHING\n    models.EncoderDecoderModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR\n    models.EncoderDecoderModel.module = @network.Transformer()\n    models.EncoderDecoderModel.optimizer_def = %OPTIMIZER\n    models.EncoderDecoderModel.output_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.z_loss = %Z_LOSS\n    \n#### Parameters for seqio.Evaluator:\n\n    seqio.Evaluator.logger_cls = \\\n        [@seqio.PyLoggingLogger, @seqio.TensorBoardLogger, @seqio.JSONLogger]\n    seqio.Evaluator.num_examples = 2048\n    \n#### Parameters for partitioning.PjitPartitioner:\n\n    partitioning.PjitPartitioner.logical_axis_rules = \\\n        @partitioning.standard_logical_axis_rules()\n    partitioning.PjitPartitioner.model_parallel_submesh = None\n    partitioning.PjitPartitioner.num_partitions = 2\n    \n#### Parameters for utils.RestoreCheckpointConfig:\n\n    utils.RestoreCheckpointConfig.path = []\n    \n#### Parameters for utils.SaveCheckpointConfig:\n\n    utils.SaveCheckpointConfig.dtype = 'float32'\n    utils.SaveCheckpointConfig.keep = None\n    utils.SaveCheckpointConfig.period = %SAVING_PERIOD\n    utils.SaveCheckpointConfig.save_dataset = False\n    \n#### Parameters for seqio.SentencePieceVocabulary:\n\n    seqio.SentencePieceVocabulary.extra_ids = 100\n    seqio.SentencePieceVocabulary.sentencepiece_model_file = \\\n        'gs://improved-t5/vocabs/tokenizer.model'\n    \n#### Parameters for network.T5Config:\n\n    network.T5Config.dropout_rate = %DROPOUT_RATE\n    network.T5Config.dtype = 'bfloat16'\n    network.T5Config.emb_dim = 4096\n    network.T5Config.head_dim = 64\n    network.T5Config.logits_via_embedding = False\n    network.T5Config.mlp_activations = ('gelu', 'linear')\n    network.T5Config.mlp_dim = 10240\n    network.T5Config.num_decoder_layers = 24\n    network.T5Config.num_encoder_layers = 24\n    network.T5Config.num_heads = 64\n    network.T5Config.remat_policy = 'minimal'\n    network.T5Config.scan_layers = True\n    network.T5Config.vocab_size = 32128\n    \n#### Parameters for train_script.train:\n\n    train_script.train.checkpoint_cfg = @utils.CheckpointConfig()\n    train_script.train.eval_period = %SAVING_PERIOD\n    train_script.train.eval_steps = 100\n    train_script.train.infer_eval_dataset_cfg = @infer_eval/utils.DatasetConfig()\n    train_script.train.inference_evaluator_cls = @seqio.Evaluator\n    train_script.train.model = %MODEL\n    train_script.train.model_dir = %MODEL_DIR\n    train_script.train.partitioner = @partitioning.PjitPartitioner()\n    train_script.train.random_seed = %RANDOM_SEED\n    train_script.train.run_eval_before_training = True\n    train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config\n    train_script.train.total_steps = %TRAIN_STEPS\n    train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()\n    train_script.train.train_eval_dataset_cfg = @train_eval/utils.DatasetConfig()\n    train_script.train.trainer_cls = @trainer.Trainer\n    train_script.train.use_hardware_rng = %USE_HARDWARE_RNG\n    \n#### Parameters for trainer.Trainer:\n\n    trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()\n    trainer.Trainer.num_microbatches = 2\n    \n#### Parameters for network.Transformer:\n\n    network.Transformer.config = @network.T5Config()""",213.37556,6.205193,1.5497208e-05,0.0010025135,0.0010000005,48093.2,16.246191
xxl,1090000,21.588932,-0.9984003,148419.03,-0.75714445,2048.006,2048.006,148428.9,-0.7571949,-0.7559233,1.0016822,0.9920916,1.0016822,1.2839158,13984.202,20480000.0,1464.5096,2.8603704,0.7150926,-140175.83,326.08224,10.055182,-5.129548e-05,1.0340846416632308,"b""    from __gin__ import dynamic_registration\n    import __main__ as train_script\n    import data.pile.tasks\n    from models.scalable_t5 import network\n    import seqio\n    from t5x import adafactor\n    from t5x import gin_utils\n    from t5x import models\n    from t5x import partitioning\n    from t5x import trainer\n    from t5x import utils\n    \n#### Macros:\n\n    BATCH_SIZE = 2048\n    DROPOUT_RATE = 0.0\n    EVAL_BATCH_SIZE = 256\n    LABEL_SMOOTHING = 0.0\n    LOSS_NORMALIZING_FACTOR = None\n    MIXTURE_OR_TASK_MODULE = None\n    MIXTURE_OR_TASK_NAME = 'pile_r_denoiser'\n    MODEL = @models.EncoderDecoderModel()\n    MODEL_DIR = 'gs://improved-t5/ckpts/v2_xxl_mlm/'\n    OPTIMIZER = @adafactor.Adafactor()\n    RANDOM_SEED = None\n    SAVING_PERIOD = 10000\n    SHUFFLE_TRAIN_EXAMPLES = True\n    TASK_FEATURE_LENGTHS = {'inputs': 512, 'targets': 114}\n    TRAIN_STEPS = 2000000\n    USE_CACHED_TASKS = False\n    USE_HARDWARE_RNG = False\n    VOCABULARY = @seqio.SentencePieceVocabulary()\n    Z_LOSS = 0.0001\n    \n#### Parameters for adafactor.Adafactor:\n\n    adafactor.Adafactor.decay_rate = 0.8\n    adafactor.Adafactor.logical_factor_rules = \\\n        @adafactor.standard_logical_factor_rules()\n    adafactor.Adafactor.step_offset = 0\n    \n#### Parameters for utils.CheckpointConfig:\n\n    utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()\n    utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()\n    \n#### Parameters for utils.create_learning_rate_scheduler:\n\n    utils.create_learning_rate_scheduler.base_learning_rate = 1.0\n    utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'\n    utils.create_learning_rate_scheduler.warmup_steps = 10000\n    \n#### Parameters for infer_eval/utils.DatasetConfig:\n\n    infer_eval/utils.DatasetConfig.batch_size = %EVAL_BATCH_SIZE\n    infer_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    infer_eval/utils.DatasetConfig.seed = 0\n    infer_eval/utils.DatasetConfig.shuffle = False\n    infer_eval/utils.DatasetConfig.split = 'validation'\n    infer_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    \n#### Parameters for train/utils.DatasetConfig:\n\n    train/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train/utils.DatasetConfig.pack = True\n    train/utils.DatasetConfig.seed = None\n    train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES\n    train/utils.DatasetConfig.split = 'train'\n    train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for train_eval/utils.DatasetConfig:\n\n    train_eval/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train_eval/utils.DatasetConfig.pack = True\n    train_eval/utils.DatasetConfig.seed = 42\n    train_eval/utils.DatasetConfig.shuffle = False\n    train_eval/utils.DatasetConfig.split = 'validation'\n    train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for models.EncoderDecoderModel:\n\n    models.EncoderDecoderModel.input_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.label_smoothing = %LABEL_SMOOTHING\n    models.EncoderDecoderModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR\n    models.EncoderDecoderModel.module = @network.Transformer()\n    models.EncoderDecoderModel.optimizer_def = %OPTIMIZER\n    models.EncoderDecoderModel.output_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.z_loss = %Z_LOSS\n    \n#### Parameters for seqio.Evaluator:\n\n    seqio.Evaluator.logger_cls = \\\n        [@seqio.PyLoggingLogger, @seqio.TensorBoardLogger, @seqio.JSONLogger]\n    seqio.Evaluator.num_examples = 2048\n    \n#### Parameters for partitioning.PjitPartitioner:\n\n    partitioning.PjitPartitioner.logical_axis_rules = \\\n        @partitioning.standard_logical_axis_rules()\n    partitioning.PjitPartitioner.model_parallel_submesh = None\n    partitioning.PjitPartitioner.num_partitions = 4\n    \n#### Parameters for utils.RestoreCheckpointConfig:\n\n    utils.RestoreCheckpointConfig.path = []\n    \n#### Parameters for utils.SaveCheckpointConfig:\n\n    utils.SaveCheckpointConfig.dtype = 'float32'\n    utils.SaveCheckpointConfig.keep = None\n    utils.SaveCheckpointConfig.period = %SAVING_PERIOD\n    utils.SaveCheckpointConfig.save_dataset = False\n    \n#### Parameters for seqio.SentencePieceVocabulary:\n\n    seqio.SentencePieceVocabulary.extra_ids = 100\n    seqio.SentencePieceVocabulary.sentencepiece_model_file = \\\n        'gs://improved-t5/vocabs/tokenizer.model'\n    \n#### Parameters for network.T5Config:\n\n    network.T5Config.dropout_rate = %DROPOUT_RATE\n    network.T5Config.dtype = 'bfloat16'\n    network.T5Config.emb_dim = 4096\n    network.T5Config.head_dim = 64\n    network.T5Config.logits_via_embedding = False\n    network.T5Config.mlp_activations = ('gelu', 'linear')\n    network.T5Config.mlp_dim = 10240\n    network.T5Config.num_decoder_layers = 24\n    network.T5Config.num_encoder_layers = 24\n    network.T5Config.num_heads = 64\n    network.T5Config.remat_policy = 'minimal'\n    network.T5Config.scan_layers = True\n    network.T5Config.vocab_size = 32128\n    \n#### Parameters for train_script.train:\n\n    train_script.train.checkpoint_cfg = @utils.CheckpointConfig()\n    train_script.train.eval_period = %SAVING_PERIOD\n    train_script.train.eval_steps = 100\n    train_script.train.infer_eval_dataset_cfg = @infer_eval/utils.DatasetConfig()\n    train_script.train.inference_evaluator_cls = @seqio.Evaluator\n    train_script.train.model = %MODEL\n    train_script.train.model_dir = %MODEL_DIR\n    train_script.train.partitioner = @partitioning.PjitPartitioner()\n    train_script.train.random_seed = %RANDOM_SEED\n    train_script.train.run_eval_before_training = True\n    train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config\n    train_script.train.total_steps = %TRAIN_STEPS\n    train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()\n    train_script.train.train_eval_dataset_cfg = @train_eval/utils.DatasetConfig()\n    train_script.train.trainer_cls = @trainer.Trainer\n    train_script.train.use_hardware_rng = %USE_HARDWARE_RNG\n    \n#### Parameters for trainer.Trainer:\n\n    trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()\n    trainer.Trainer.num_microbatches = None\n    \n#### Parameters for network.Transformer:\n\n    network.Transformer.config = @network.T5Config()""",116.51196,1.001822,1.7642975e-05,0.00096004637,0.0009578268,28535.6,43.871685
xxl,1100000,21.86042,-0.9984842,148409.31,-0.75709486,2048.007,2048.007,148419.55,-0.7571471,-0.7558707,1.0016886,0.9920615,1.0016886,1.2849963,13983.723,20480000.0,1464.5599,2.8604686,0.71511716,-140180.64,326.0934,10.0878525,-5.1462142e-05,1.034085625309994,"b""    from __gin__ import dynamic_registration\n    import __main__ as train_script\n    import data.pile.tasks\n    from models.scalable_t5 import network\n    import seqio\n    from t5x import adafactor\n    from t5x import gin_utils\n    from t5x import models\n    from t5x import partitioning\n    from t5x import trainer\n    from t5x import utils\n    \n#### Macros:\n\n    BATCH_SIZE = 2048\n    DROPOUT_RATE = 0.0\n    EVAL_BATCH_SIZE = 256\n    LABEL_SMOOTHING = 0.0\n    LOSS_NORMALIZING_FACTOR = None\n    MIXTURE_OR_TASK_MODULE = None\n    MIXTURE_OR_TASK_NAME = 'pile_r_denoiser'\n    MODEL = @models.EncoderDecoderModel()\n    MODEL_DIR = 'gs://improved-t5/ckpts/v2_xxl_mlm/'\n    OPTIMIZER = @adafactor.Adafactor()\n    RANDOM_SEED = None\n    SAVING_PERIOD = 10000\n    SHUFFLE_TRAIN_EXAMPLES = True\n    TASK_FEATURE_LENGTHS = {'inputs': 512, 'targets': 114}\n    TRAIN_STEPS = 2000000\n    USE_CACHED_TASKS = False\n    USE_HARDWARE_RNG = False\n    VOCABULARY = @seqio.SentencePieceVocabulary()\n    Z_LOSS = 0.0001\n    \n#### Parameters for adafactor.Adafactor:\n\n    adafactor.Adafactor.decay_rate = 0.8\n    adafactor.Adafactor.logical_factor_rules = \\\n        @adafactor.standard_logical_factor_rules()\n    adafactor.Adafactor.step_offset = 0\n    \n#### Parameters for utils.CheckpointConfig:\n\n    utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()\n    utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()\n    \n#### Parameters for utils.create_learning_rate_scheduler:\n\n    utils.create_learning_rate_scheduler.base_learning_rate = 1.0\n    utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'\n    utils.create_learning_rate_scheduler.warmup_steps = 10000\n    \n#### Parameters for infer_eval/utils.DatasetConfig:\n\n    infer_eval/utils.DatasetConfig.batch_size = %EVAL_BATCH_SIZE\n    infer_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    infer_eval/utils.DatasetConfig.seed = 0\n    infer_eval/utils.DatasetConfig.shuffle = False\n    infer_eval/utils.DatasetConfig.split = 'validation'\n    infer_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    \n#### Parameters for train/utils.DatasetConfig:\n\n    train/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train/utils.DatasetConfig.pack = True\n    train/utils.DatasetConfig.seed = None\n    train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES\n    train/utils.DatasetConfig.split = 'train'\n    train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for train_eval/utils.DatasetConfig:\n\n    train_eval/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train_eval/utils.DatasetConfig.pack = True\n    train_eval/utils.DatasetConfig.seed = 42\n    train_eval/utils.DatasetConfig.shuffle = False\n    train_eval/utils.DatasetConfig.split = 'validation'\n    train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for models.EncoderDecoderModel:\n\n    models.EncoderDecoderModel.input_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.label_smoothing = %LABEL_SMOOTHING\n    models.EncoderDecoderModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR\n    models.EncoderDecoderModel.module = @network.Transformer()\n    models.EncoderDecoderModel.optimizer_def = %OPTIMIZER\n    models.EncoderDecoderModel.output_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.z_loss = %Z_LOSS\n    \n#### Parameters for seqio.Evaluator:\n\n    seqio.Evaluator.logger_cls = \\\n        [@seqio.PyLoggingLogger, @seqio.TensorBoardLogger, @seqio.JSONLogger]\n    seqio.Evaluator.num_examples = 2048\n    \n#### Parameters for partitioning.PjitPartitioner:\n\n    partitioning.PjitPartitioner.logical_axis_rules = \\\n        @partitioning.standard_logical_axis_rules()\n    partitioning.PjitPartitioner.model_parallel_submesh = None\n    partitioning.PjitPartitioner.num_partitions = 4\n    \n#### Parameters for utils.RestoreCheckpointConfig:\n\n    utils.RestoreCheckpointConfig.path = []\n    \n#### Parameters for utils.SaveCheckpointConfig:\n\n    utils.SaveCheckpointConfig.dtype = 'float32'\n    utils.SaveCheckpointConfig.keep = None\n    utils.SaveCheckpointConfig.period = %SAVING_PERIOD\n    utils.SaveCheckpointConfig.save_dataset = False\n    \n#### Parameters for seqio.SentencePieceVocabulary:\n\n    seqio.SentencePieceVocabulary.extra_ids = 100\n    seqio.SentencePieceVocabulary.sentencepiece_model_file = \\\n        'gs://improved-t5/vocabs/tokenizer.model'\n    \n#### Parameters for network.T5Config:\n\n    network.T5Config.dropout_rate = %DROPOUT_RATE\n    network.T5Config.dtype = 'bfloat16'\n    network.T5Config.emb_dim = 4096\n    network.T5Config.head_dim = 64\n    network.T5Config.logits_via_embedding = False\n    network.T5Config.mlp_activations = ('gelu', 'linear')\n    network.T5Config.mlp_dim = 10240\n    network.T5Config.num_decoder_layers = 24\n    network.T5Config.num_encoder_layers = 24\n    network.T5Config.num_heads = 64\n    network.T5Config.remat_policy = 'minimal'\n    network.T5Config.scan_layers = True\n    network.T5Config.vocab_size = 32128\n    \n#### Parameters for train_script.train:\n\n    train_script.train.checkpoint_cfg = @utils.CheckpointConfig()\n    train_script.train.eval_period = %SAVING_PERIOD\n    train_script.train.eval_steps = 100\n    train_script.train.infer_eval_dataset_cfg = @infer_eval/utils.DatasetConfig()\n    train_script.train.inference_evaluator_cls = @seqio.Evaluator\n    train_script.train.model = %MODEL\n    train_script.train.model_dir = %MODEL_DIR\n    train_script.train.partitioner = @partitioning.PjitPartitioner()\n    train_script.train.random_seed = %RANDOM_SEED\n    train_script.train.run_eval_before_training = True\n    train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config\n    train_script.train.total_steps = %TRAIN_STEPS\n    train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()\n    train_script.train.train_eval_dataset_cfg = @train_eval/utils.DatasetConfig()\n    train_script.train.trainer_cls = @trainer.Trainer\n    train_script.train.use_hardware_rng = %USE_HARDWARE_RNG\n    \n#### Parameters for trainer.Trainer:\n\n    trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()\n    trainer.Trainer.num_microbatches = None\n    \n#### Parameters for network.Transformer:\n\n    network.Transformer.config = @network.T5Config()""",127.83887,5.7735987,1.2397766e-05,0.0009556196,0.000953463,14138.529,371.63663
xxl,990000,,-0.9975239,149385.5,-0.7620748,2048.008,2048.008,149396.0,-0.76212835,-0.7608416,1.0016912,0.9920488,1.0016912,1.2854512,23940.11,20480000.0,855.4681,3.3416722,0.41770902,-81881.3,380.95062,10.216296,-5.211739e-05,1.034143890722851,,,4.0667086,,0.0010075895,0.0010050383,24078.744,18.311005
xxl,790000,59.41421,-0.9969827,149836.97,-0.76437795,2048.0078,2048.0078,149847.52,-0.76443166,-0.7631399,1.0016927,0.99204224,1.0016927,1.2856882,23702.963,20480000.0,864.027,3.3751054,0.42188817,-82700.516,384.76202,10.791918,-5.505386e-05,1.0343172199341744,"b""    from __gin__ import dynamic_registration\n    import __main__ as train_script\n    import data.pile.tasks\n    from models.scalable_t5 import network\n    import seqio\n    from t5x import adafactor\n    from t5x import gin_utils\n    from t5x import models\n    from t5x import partitioning\n    from t5x import trainer\n    from t5x import utils\n    \n#### Macros:\n\n    BATCH_SIZE = 2048\n    DROPOUT_RATE = 0.0\n    EVAL_BATCH_SIZE = 256\n    LABEL_SMOOTHING = 0.0\n    LOSS_NORMALIZING_FACTOR = None\n    MIXTURE_OR_TASK_MODULE = None\n    MIXTURE_OR_TASK_NAME = 'pile_r_denoiser'\n    MODEL = @models.EncoderDecoderModel()\n    MODEL_DIR = 'gs://improved-t5/ckpts/v2_xxl_mlm/'\n    OPTIMIZER = @adafactor.Adafactor()\n    RANDOM_SEED = None\n    SAVING_PERIOD = 10000\n    SHUFFLE_TRAIN_EXAMPLES = True\n    TASK_FEATURE_LENGTHS = {'inputs': 512, 'targets': 114}\n    TRAIN_STEPS = 1000000\n    USE_CACHED_TASKS = False\n    USE_HARDWARE_RNG = False\n    VOCABULARY = @seqio.SentencePieceVocabulary()\n    Z_LOSS = 0.0001\n    \n#### Parameters for adafactor.Adafactor:\n\n    adafactor.Adafactor.decay_rate = 0.8\n    adafactor.Adafactor.logical_factor_rules = \\\n        @adafactor.standard_logical_factor_rules()\n    adafactor.Adafactor.step_offset = 0\n    \n#### Parameters for utils.CheckpointConfig:\n\n    utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()\n    utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()\n    \n#### Parameters for utils.create_learning_rate_scheduler:\n\n    utils.create_learning_rate_scheduler.base_learning_rate = 1.0\n    utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'\n    utils.create_learning_rate_scheduler.warmup_steps = 10000\n    \n#### Parameters for infer_eval/utils.DatasetConfig:\n\n    infer_eval/utils.DatasetConfig.batch_size = %EVAL_BATCH_SIZE\n    infer_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    infer_eval/utils.DatasetConfig.seed = 0\n    infer_eval/utils.DatasetConfig.shuffle = False\n    infer_eval/utils.DatasetConfig.split = 'validation'\n    infer_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    \n#### Parameters for train/utils.DatasetConfig:\n\n    train/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train/utils.DatasetConfig.pack = True\n    train/utils.DatasetConfig.seed = None\n    train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES\n    train/utils.DatasetConfig.split = 'train'\n    train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for train_eval/utils.DatasetConfig:\n\n    train_eval/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train_eval/utils.DatasetConfig.pack = True\n    train_eval/utils.DatasetConfig.seed = 42\n    train_eval/utils.DatasetConfig.shuffle = False\n    train_eval/utils.DatasetConfig.split = 'validation'\n    train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for models.EncoderDecoderModel:\n\n    models.EncoderDecoderModel.input_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.label_smoothing = %LABEL_SMOOTHING\n    models.EncoderDecoderModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR\n    models.EncoderDecoderModel.module = @network.Transformer()\n    models.EncoderDecoderModel.optimizer_def = %OPTIMIZER\n    models.EncoderDecoderModel.output_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.z_loss = %Z_LOSS\n    \n#### Parameters for seqio.Evaluator:\n\n    seqio.Evaluator.logger_cls = \\\n        [@seqio.PyLoggingLogger, @seqio.TensorBoardLogger, @seqio.JSONLogger]\n    seqio.Evaluator.num_examples = 2048\n    \n#### Parameters for partitioning.PjitPartitioner:\n\n    partitioning.PjitPartitioner.logical_axis_rules = \\\n        @partitioning.standard_logical_axis_rules()\n    partitioning.PjitPartitioner.model_parallel_submesh = None\n    partitioning.PjitPartitioner.num_partitions = 1\n    \n#### Parameters for utils.RestoreCheckpointConfig:\n\n    utils.RestoreCheckpointConfig.path = []\n    \n#### Parameters for utils.SaveCheckpointConfig:\n\n    utils.SaveCheckpointConfig.dtype = 'float32'\n    utils.SaveCheckpointConfig.keep = None\n    utils.SaveCheckpointConfig.period = %SAVING_PERIOD\n    utils.SaveCheckpointConfig.save_dataset = False\n    \n#### Parameters for seqio.SentencePieceVocabulary:\n\n    seqio.SentencePieceVocabulary.extra_ids = 100\n    seqio.SentencePieceVocabulary.sentencepiece_model_file = \\\n        'gs://improved-t5/vocabs/tokenizer.model'\n    \n#### Parameters for partitioning.standard_logical_axis_rules:\n\n    partitioning.standard_logical_axis_rules.activation_partitioning_dims = 1\n    partitioning.standard_logical_axis_rules.parameter_partitioning_dims = 2\n    \n#### Parameters for network.T5Config:\n\n    network.T5Config.dropout_rate = %DROPOUT_RATE\n    network.T5Config.dtype = 'bfloat16'\n    network.T5Config.emb_dim = 4096\n    network.T5Config.head_dim = 64\n    network.T5Config.logits_via_embedding = False\n    network.T5Config.mlp_activations = ('gelu', 'linear')\n    network.T5Config.mlp_dim = 10240\n    network.T5Config.num_decoder_layers = 24\n    network.T5Config.num_encoder_layers = 24\n    network.T5Config.num_heads = 64\n    network.T5Config.remat_policy = 'minimal'\n    network.T5Config.scan_layers = True\n    network.T5Config.vocab_size = 32128\n    \n#### Parameters for train_script.train:\n\n    train_script.train.checkpoint_cfg = @utils.CheckpointConfig()\n    train_script.train.eval_period = %SAVING_PERIOD\n    train_script.train.eval_steps = 100\n    train_script.train.infer_eval_dataset_cfg = @infer_eval/utils.DatasetConfig()\n    train_script.train.inference_evaluator_cls = @seqio.Evaluator\n    train_script.train.model = %MODEL\n    train_script.train.model_dir = %MODEL_DIR\n    train_script.train.partitioner = @partitioning.PjitPartitioner()\n    train_script.train.random_seed = %RANDOM_SEED\n    train_script.train.run_eval_before_training = True\n    train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config\n    train_script.train.total_steps = %TRAIN_STEPS\n    train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()\n    train_script.train.train_eval_dataset_cfg = @train_eval/utils.DatasetConfig()\n    train_script.train.trainer_cls = @trainer.Trainer\n    train_script.train.use_hardware_rng = %USE_HARDWARE_RNG\n    \n#### Parameters for trainer.Trainer:\n\n    trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()\n    trainer.Trainer.num_microbatches = None\n    \n#### Parameters for network.Transformer:\n\n    network.Transformer.config = @network.T5Config()""",12.768717,4.0927453,2.5510788e-05,0.0011286756,0.0011250885,47711.625,35.02506
xxl,800000,,-0.9972018,149715.06,-0.76375604,2048.007,2048.007,149726.1,-0.76381224,-0.76252013,1.0016946,0.9920335,1.0016946,1.2860042,23987.188,20480000.0,853.78906,3.3351135,0.4168892,-81720.59,380.20294,10.8053875,-5.5122575e-05,1.0342933236650205,,,4.1139793,,0.001121544,0.0011180347,24179.477,20.364788
xxl,810000,,-0.9968413,149996.44,-0.76519144,2048.006,2048.006,150006.95,-0.765245,-0.7639521,1.0016924,0.99204373,1.0016924,1.285639,23981.764,20480000.0,853.9823,3.3358684,0.41698354,-81739.086,380.28897,10.738472,-5.478121e-05,1.0343019447438833,,,4.0804567,,0.0011145591,0.0011111118,48241.035,16.570948
xxl,820000,61.73056,-0.9973335,149551.45,-0.7629214,2048.006,2048.006,149562.27,-0.7629765,-0.7616923,1.0016861,0.9920726,1.0016861,1.2845935,23981.596,20480000.0,853.98816,3.3358912,0.4169864,-81739.65,380.2916,10.717048,-5.467192e-05,1.0342883895481958,"b""    from __gin__ import dynamic_registration\n    import __main__ as train_script\n    import data.pile.tasks\n    from models.scalable_t5 import network\n    import seqio\n    from t5x import adafactor\n    from t5x import gin_utils\n    from t5x import models\n    from t5x import partitioning\n    from t5x import trainer\n    from t5x import utils\n    \n#### Macros:\n\n    BATCH_SIZE = 2048\n    DROPOUT_RATE = 0.0\n    EVAL_BATCH_SIZE = 256\n    LABEL_SMOOTHING = 0.0\n    LOSS_NORMALIZING_FACTOR = None\n    MIXTURE_OR_TASK_MODULE = None\n    MIXTURE_OR_TASK_NAME = 'pile_r_denoiser'\n    MODEL = @models.EncoderDecoderModel()\n    MODEL_DIR = 'gs://improved-t5/ckpts/v2_xxl_mlm/'\n    OPTIMIZER = @adafactor.Adafactor()\n    RANDOM_SEED = None\n    SAVING_PERIOD = 10000\n    SHUFFLE_TRAIN_EXAMPLES = True\n    TASK_FEATURE_LENGTHS = {'inputs': 512, 'targets': 114}\n    TRAIN_STEPS = 1000000\n    USE_CACHED_TASKS = False\n    USE_HARDWARE_RNG = False\n    VOCABULARY = @seqio.SentencePieceVocabulary()\n    Z_LOSS = 0.0001\n    \n#### Parameters for adafactor.Adafactor:\n\n    adafactor.Adafactor.decay_rate = 0.8\n    adafactor.Adafactor.logical_factor_rules = \\\n        @adafactor.standard_logical_factor_rules()\n    adafactor.Adafactor.step_offset = 0\n    \n#### Parameters for utils.CheckpointConfig:\n\n    utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()\n    utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()\n    \n#### Parameters for utils.create_learning_rate_scheduler:\n\n    utils.create_learning_rate_scheduler.base_learning_rate = 1.0\n    utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'\n    utils.create_learning_rate_scheduler.warmup_steps = 10000\n    \n#### Parameters for infer_eval/utils.DatasetConfig:\n\n    infer_eval/utils.DatasetConfig.batch_size = %EVAL_BATCH_SIZE\n    infer_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    infer_eval/utils.DatasetConfig.seed = 0\n    infer_eval/utils.DatasetConfig.shuffle = False\n    infer_eval/utils.DatasetConfig.split = 'validation'\n    infer_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    \n#### Parameters for train/utils.DatasetConfig:\n\n    train/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train/utils.DatasetConfig.pack = True\n    train/utils.DatasetConfig.seed = None\n    train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES\n    train/utils.DatasetConfig.split = 'train'\n    train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for train_eval/utils.DatasetConfig:\n\n    train_eval/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train_eval/utils.DatasetConfig.pack = True\n    train_eval/utils.DatasetConfig.seed = 42\n    train_eval/utils.DatasetConfig.shuffle = False\n    train_eval/utils.DatasetConfig.split = 'validation'\n    train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for models.EncoderDecoderModel:\n\n    models.EncoderDecoderModel.input_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.label_smoothing = %LABEL_SMOOTHING\n    models.EncoderDecoderModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR\n    models.EncoderDecoderModel.module = @network.Transformer()\n    models.EncoderDecoderModel.optimizer_def = %OPTIMIZER\n    models.EncoderDecoderModel.output_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.z_loss = %Z_LOSS\n    \n#### Parameters for seqio.Evaluator:\n\n    seqio.Evaluator.logger_cls = \\\n        [@seqio.PyLoggingLogger, @seqio.TensorBoardLogger, @seqio.JSONLogger]\n    seqio.Evaluator.num_examples = 2048\n    \n#### Parameters for partitioning.PjitPartitioner:\n\n    partitioning.PjitPartitioner.logical_axis_rules = \\\n        @partitioning.standard_logical_axis_rules()\n    partitioning.PjitPartitioner.model_parallel_submesh = None\n    partitioning.PjitPartitioner.num_partitions = 1\n    \n#### Parameters for utils.RestoreCheckpointConfig:\n\n    utils.RestoreCheckpointConfig.path = []\n    \n#### Parameters for utils.SaveCheckpointConfig:\n\n    utils.SaveCheckpointConfig.dtype = 'float32'\n    utils.SaveCheckpointConfig.keep = None\n    utils.SaveCheckpointConfig.period = %SAVING_PERIOD\n    utils.SaveCheckpointConfig.save_dataset = False\n    \n#### Parameters for seqio.SentencePieceVocabulary:\n\n    seqio.SentencePieceVocabulary.extra_ids = 100\n    seqio.SentencePieceVocabulary.sentencepiece_model_file = \\\n        'gs://improved-t5/vocabs/tokenizer.model'\n    \n#### Parameters for partitioning.standard_logical_axis_rules:\n\n    partitioning.standard_logical_axis_rules.activation_partitioning_dims = 1\n    partitioning.standard_logical_axis_rules.parameter_partitioning_dims = 2\n    \n#### Parameters for network.T5Config:\n\n    network.T5Config.dropout_rate = %DROPOUT_RATE\n    network.T5Config.dtype = 'bfloat16'\n    network.T5Config.emb_dim = 4096\n    network.T5Config.head_dim = 64\n    network.T5Config.logits_via_embedding = False\n    network.T5Config.mlp_activations = ('gelu', 'linear')\n    network.T5Config.mlp_dim = 10240\n    network.T5Config.num_decoder_layers = 24\n    network.T5Config.num_encoder_layers = 24\n    network.T5Config.num_heads = 64\n    network.T5Config.remat_policy = 'minimal'\n    network.T5Config.scan_layers = True\n    network.T5Config.vocab_size = 32128\n    \n#### Parameters for train_script.train:\n\n    train_script.train.checkpoint_cfg = @utils.CheckpointConfig()\n    train_script.train.eval_period = %SAVING_PERIOD\n    train_script.train.eval_steps = 100\n    train_script.train.infer_eval_dataset_cfg = @infer_eval/utils.DatasetConfig()\n    train_script.train.inference_evaluator_cls = @seqio.Evaluator\n    train_script.train.model = %MODEL\n    train_script.train.model_dir = %MODEL_DIR\n    train_script.train.partitioner = @partitioning.PjitPartitioner()\n    train_script.train.random_seed = %RANDOM_SEED\n    train_script.train.run_eval_before_training = True\n    train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config\n    train_script.train.total_steps = %TRAIN_STEPS\n    train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()\n    train_script.train.train_eval_dataset_cfg = @train_eval/utils.DatasetConfig()\n    train_script.train.trainer_cls = @trainer.Trainer\n    train_script.train.use_hardware_rng = %USE_HARDWARE_RNG\n    \n#### Parameters for trainer.Trainer:\n\n    trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()\n    trainer.Trainer.num_microbatches = None\n    \n#### Parameters for network.Transformer:\n\n    network.Transformer.config = @network.T5Config()""",12.471348,25.961807,3.3140182e-05,0.001107703,0.001104316,72298.66,15.973887
xxl,1210000,,-0.99936956,147503.23,-0.7524726,2048.0059,2048.0059,147512.98,-0.75252235,-0.7512651,1.0016735,0.99213207,1.0016735,1.2824614,13964.541,20480000.0,1466.5717,2.8643978,0.71609944,-140373.2,326.54135,9.9512205,-5.076513e-05,1.0340127753421184,,,0.9705589,,0.0009109817,0.00090909126,14112.192,44.47025
xxl,1220000,,-0.9990051,147782.58,-0.7538976,2048.007,2048.007,147792.73,-0.75394946,-0.7526794,1.0016874,0.99206746,1.0016874,1.2847843,13964.391,20480000.0,1466.5875,2.8644288,0.7161072,-140374.72,326.54486,9.91148,-5.0562398e-05,1.0340138943460793,,,0.9875183,,0.00090723217,0.0009053578,28156.742,373.61325
xxl,1230000,,-0.9993154,147509.17,-0.75250286,2048.007,2048.007,147519.17,-0.7525539,-0.75128925,1.0016832,0.9920868,1.0016832,1.2840903,13963.535,20480000.0,1466.6774,2.8646042,0.71615106,-140383.31,326.56488,9.883976,-5.042209e-05,1.0340080270581036,,,0.9729719,,0.0009035372,0.0009016701,42529.633,46.015278
xxl,1240000,,-0.9991753,147718.06,-0.75356853,2048.0068,2048.0068,147728.1,-0.75361973,-0.75235575,1.00168,0.9921021,1.00168,1.283542,13963.498,20480000.0,1466.6812,2.8646116,0.7161529,-140383.69,326.56573,9.752688,-4.975234e-05,1.0340000267444491,,,0.96531796,,0.00089985144,0.0008980269,56574.863,372.06293
xxl,670000,55.31812,0.0,nan,nan,2048.0068,2048.0068,nan,nan,nan,1.0016956,0.99202836,1.0016956,1.286188,24517.525,20480000.0,835.3208,3.2629719,0.40787148,-79952.9,371.9788,nan,nan,1.0344282450851123,"b""    from __gin__ import dynamic_registration\n    import __main__ as train_script\n    import data.pile.tasks\n    from models.scalable_t5 import network\n    import seqio\n    from t5x import adafactor\n    from t5x import gin_utils\n    from t5x import models\n    from t5x import partitioning\n    from t5x import trainer\n    from t5x import utils\n    \n#### Macros:\n\n    BATCH_SIZE = 2048\n    DROPOUT_RATE = 0.0\n    EVAL_BATCH_SIZE = 256\n    LABEL_SMOOTHING = 0.0\n    LOSS_NORMALIZING_FACTOR = None\n    MIXTURE_OR_TASK_MODULE = None\n    MIXTURE_OR_TASK_NAME = 'pile_r_denoiser'\n    MODEL = @models.EncoderDecoderModel()\n    MODEL_DIR = 'gs://improved-t5/ckpts/v2_xxl_mlm/'\n    OPTIMIZER = @adafactor.Adafactor()\n    RANDOM_SEED = None\n    SAVING_PERIOD = 10000\n    SHUFFLE_TRAIN_EXAMPLES = True\n    TASK_FEATURE_LENGTHS = {'inputs': 512, 'targets': 114}\n    TRAIN_STEPS = 1000000\n    USE_CACHED_TASKS = False\n    USE_HARDWARE_RNG = False\n    VOCABULARY = @seqio.SentencePieceVocabulary()\n    Z_LOSS = 0.0001\n    \n#### Parameters for adafactor.Adafactor:\n\n    adafactor.Adafactor.decay_rate = 0.8\n    adafactor.Adafactor.logical_factor_rules = \\\n        @adafactor.standard_logical_factor_rules()\n    adafactor.Adafactor.step_offset = 0\n    \n#### Parameters for utils.CheckpointConfig:\n\n    utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()\n    utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()\n    \n#### Parameters for utils.create_learning_rate_scheduler:\n\n    utils.create_learning_rate_scheduler.base_learning_rate = 1.0\n    utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'\n    utils.create_learning_rate_scheduler.warmup_steps = 10000\n    \n#### Parameters for infer_eval/utils.DatasetConfig:\n\n    infer_eval/utils.DatasetConfig.batch_size = %EVAL_BATCH_SIZE\n    infer_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    infer_eval/utils.DatasetConfig.seed = 0\n    infer_eval/utils.DatasetConfig.shuffle = False\n    infer_eval/utils.DatasetConfig.split = 'validation'\n    infer_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    \n#### Parameters for train/utils.DatasetConfig:\n\n    train/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train/utils.DatasetConfig.pack = True\n    train/utils.DatasetConfig.seed = None\n    train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES\n    train/utils.DatasetConfig.split = 'train'\n    train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for train_eval/utils.DatasetConfig:\n\n    train_eval/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train_eval/utils.DatasetConfig.pack = True\n    train_eval/utils.DatasetConfig.seed = 42\n    train_eval/utils.DatasetConfig.shuffle = False\n    train_eval/utils.DatasetConfig.split = 'validation'\n    train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for models.EncoderDecoderModel:\n\n    models.EncoderDecoderModel.input_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.label_smoothing = %LABEL_SMOOTHING\n    models.EncoderDecoderModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR\n    models.EncoderDecoderModel.module = @network.Transformer()\n    models.EncoderDecoderModel.optimizer_def = %OPTIMIZER\n    models.EncoderDecoderModel.output_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.z_loss = %Z_LOSS\n    \n#### Parameters for seqio.Evaluator:\n\n    seqio.Evaluator.logger_cls = \\\n        [@seqio.PyLoggingLogger, @seqio.TensorBoardLogger, @seqio.JSONLogger]\n    seqio.Evaluator.num_examples = 2048\n    \n#### Parameters for partitioning.PjitPartitioner:\n\n    partitioning.PjitPartitioner.logical_axis_rules = \\\n        @partitioning.standard_logical_axis_rules()\n    partitioning.PjitPartitioner.model_parallel_submesh = None\n    partitioning.PjitPartitioner.num_partitions = 1\n    \n#### Parameters for utils.RestoreCheckpointConfig:\n\n    utils.RestoreCheckpointConfig.path = []\n    \n#### Parameters for utils.SaveCheckpointConfig:\n\n    utils.SaveCheckpointConfig.dtype = 'float32'\n    utils.SaveCheckpointConfig.keep = None\n    utils.SaveCheckpointConfig.period = %SAVING_PERIOD\n    utils.SaveCheckpointConfig.save_dataset = False\n    \n#### Parameters for seqio.SentencePieceVocabulary:\n\n    seqio.SentencePieceVocabulary.extra_ids = 100\n    seqio.SentencePieceVocabulary.sentencepiece_model_file = \\\n        'gs://improved-t5/vocabs/tokenizer.model'\n    \n#### Parameters for partitioning.standard_logical_axis_rules:\n\n    partitioning.standard_logical_axis_rules.activation_partitioning_dims = 1\n    partitioning.standard_logical_axis_rules.parameter_partitioning_dims = 2\n    \n#### Parameters for network.T5Config:\n\n    network.T5Config.dropout_rate = %DROPOUT_RATE\n    network.T5Config.dtype = 'bfloat16'\n    network.T5Config.emb_dim = 4096\n    network.T5Config.head_dim = 64\n    network.T5Config.logits_via_embedding = False\n    network.T5Config.mlp_activations = ('gelu', 'linear')\n    network.T5Config.mlp_dim = 10240\n    network.T5Config.num_decoder_layers = 24\n    network.T5Config.num_encoder_layers = 24\n    network.T5Config.num_heads = 64\n    network.T5Config.remat_policy = 'minimal'\n    network.T5Config.scan_layers = True\n    network.T5Config.vocab_size = 32128\n    \n#### Parameters for train_script.train:\n\n    train_script.train.checkpoint_cfg = @utils.CheckpointConfig()\n    train_script.train.eval_period = %SAVING_PERIOD\n    train_script.train.eval_steps = 100\n    train_script.train.infer_eval_dataset_cfg = @infer_eval/utils.DatasetConfig()\n    train_script.train.inference_evaluator_cls = @seqio.Evaluator\n    train_script.train.model = %MODEL\n    train_script.train.model_dir = %MODEL_DIR\n    train_script.train.partitioner = @partitioning.PjitPartitioner()\n    train_script.train.random_seed = %RANDOM_SEED\n    train_script.train.run_eval_before_training = True\n    train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config\n    train_script.train.total_steps = %TRAIN_STEPS\n    train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()\n    train_script.train.train_eval_dataset_cfg = @train_eval/utils.DatasetConfig()\n    train_script.train.trainer_cls = @trainer.Trainer\n    train_script.train.use_hardware_rng = %USE_HARDWARE_RNG\n    \n#### Parameters for trainer.Trainer:\n\n    trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()\n    trainer.Trainer.num_microbatches = None\n    \n#### Parameters for network.Transformer:\n\n    network.Transformer.config = @network.T5Config()""",10.819375,4.052695,1.5497208e-05,0.0012262804,0.0012216953,73869.57,15.255091
xxl,680000,50.809265,0.0,nan,nan,2048.005,2048.005,nan,nan,nan,1.0016911,0.99204946,1.0016911,1.2854297,24517.549,20480000.0,835.32,3.2629688,0.4078711,-79952.82,371.97842,nan,nan,1.03442226220406,"b""    from __gin__ import dynamic_registration\n    import __main__ as train_script\n    import data.pile.tasks\n    from models.scalable_t5 import network\n    import seqio\n    from t5x import adafactor\n    from t5x import gin_utils\n    from t5x import models\n    from t5x import partitioning\n    from t5x import trainer\n    from t5x import utils\n    \n#### Macros:\n\n    BATCH_SIZE = 2048\n    DROPOUT_RATE = 0.0\n    EVAL_BATCH_SIZE = 256\n    LABEL_SMOOTHING = 0.0\n    LOSS_NORMALIZING_FACTOR = None\n    MIXTURE_OR_TASK_MODULE = None\n    MIXTURE_OR_TASK_NAME = 'pile_r_denoiser'\n    MODEL = @models.EncoderDecoderModel()\n    MODEL_DIR = 'gs://improved-t5/ckpts/v2_xxl_mlm/'\n    OPTIMIZER = @adafactor.Adafactor()\n    RANDOM_SEED = None\n    SAVING_PERIOD = 10000\n    SHUFFLE_TRAIN_EXAMPLES = True\n    TASK_FEATURE_LENGTHS = {'inputs': 512, 'targets': 114}\n    TRAIN_STEPS = 1000000\n    USE_CACHED_TASKS = False\n    USE_HARDWARE_RNG = False\n    VOCABULARY = @seqio.SentencePieceVocabulary()\n    Z_LOSS = 0.0001\n    \n#### Parameters for adafactor.Adafactor:\n\n    adafactor.Adafactor.decay_rate = 0.8\n    adafactor.Adafactor.logical_factor_rules = \\\n        @adafactor.standard_logical_factor_rules()\n    adafactor.Adafactor.step_offset = 0\n    \n#### Parameters for utils.CheckpointConfig:\n\n    utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()\n    utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()\n    \n#### Parameters for utils.create_learning_rate_scheduler:\n\n    utils.create_learning_rate_scheduler.base_learning_rate = 1.0\n    utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'\n    utils.create_learning_rate_scheduler.warmup_steps = 10000\n    \n#### Parameters for infer_eval/utils.DatasetConfig:\n\n    infer_eval/utils.DatasetConfig.batch_size = %EVAL_BATCH_SIZE\n    infer_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    infer_eval/utils.DatasetConfig.seed = 0\n    infer_eval/utils.DatasetConfig.shuffle = False\n    infer_eval/utils.DatasetConfig.split = 'validation'\n    infer_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    \n#### Parameters for train/utils.DatasetConfig:\n\n    train/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train/utils.DatasetConfig.pack = True\n    train/utils.DatasetConfig.seed = None\n    train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES\n    train/utils.DatasetConfig.split = 'train'\n    train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for train_eval/utils.DatasetConfig:\n\n    train_eval/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train_eval/utils.DatasetConfig.pack = True\n    train_eval/utils.DatasetConfig.seed = 42\n    train_eval/utils.DatasetConfig.shuffle = False\n    train_eval/utils.DatasetConfig.split = 'validation'\n    train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for models.EncoderDecoderModel:\n\n    models.EncoderDecoderModel.input_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.label_smoothing = %LABEL_SMOOTHING\n    models.EncoderDecoderModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR\n    models.EncoderDecoderModel.module = @network.Transformer()\n    models.EncoderDecoderModel.optimizer_def = %OPTIMIZER\n    models.EncoderDecoderModel.output_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.z_loss = %Z_LOSS\n    \n#### Parameters for seqio.Evaluator:\n\n    seqio.Evaluator.logger_cls = \\\n        [@seqio.PyLoggingLogger, @seqio.TensorBoardLogger, @seqio.JSONLogger]\n    seqio.Evaluator.num_examples = 2048\n    \n#### Parameters for partitioning.PjitPartitioner:\n\n    partitioning.PjitPartitioner.logical_axis_rules = \\\n        @partitioning.standard_logical_axis_rules()\n    partitioning.PjitPartitioner.model_parallel_submesh = None\n    partitioning.PjitPartitioner.num_partitions = 1\n    \n#### Parameters for utils.RestoreCheckpointConfig:\n\n    utils.RestoreCheckpointConfig.path = []\n    \n#### Parameters for utils.SaveCheckpointConfig:\n\n    utils.SaveCheckpointConfig.dtype = 'float32'\n    utils.SaveCheckpointConfig.keep = None\n    utils.SaveCheckpointConfig.period = %SAVING_PERIOD\n    utils.SaveCheckpointConfig.save_dataset = False\n    \n#### Parameters for seqio.SentencePieceVocabulary:\n\n    seqio.SentencePieceVocabulary.extra_ids = 100\n    seqio.SentencePieceVocabulary.sentencepiece_model_file = \\\n        'gs://improved-t5/vocabs/tokenizer.model'\n    \n#### Parameters for partitioning.standard_logical_axis_rules:\n\n    partitioning.standard_logical_axis_rules.activation_partitioning_dims = 1\n    partitioning.standard_logical_axis_rules.parameter_partitioning_dims = 2\n    \n#### Parameters for network.T5Config:\n\n    network.T5Config.dropout_rate = %DROPOUT_RATE\n    network.T5Config.dtype = 'bfloat16'\n    network.T5Config.emb_dim = 4096\n    network.T5Config.head_dim = 64\n    network.T5Config.logits_via_embedding = False\n    network.T5Config.mlp_activations = ('gelu', 'linear')\n    network.T5Config.mlp_dim = 10240\n    network.T5Config.num_decoder_layers = 24\n    network.T5Config.num_encoder_layers = 24\n    network.T5Config.num_heads = 64\n    network.T5Config.remat_policy = 'minimal'\n    network.T5Config.scan_layers = True\n    network.T5Config.vocab_size = 32128\n    \n#### Parameters for train_script.train:\n\n    train_script.train.checkpoint_cfg = @utils.CheckpointConfig()\n    train_script.train.eval_period = %SAVING_PERIOD\n    train_script.train.eval_steps = 100\n    train_script.train.infer_eval_dataset_cfg = @infer_eval/utils.DatasetConfig()\n    train_script.train.inference_evaluator_cls = @seqio.Evaluator\n    train_script.train.model = %MODEL\n    train_script.train.model_dir = %MODEL_DIR\n    train_script.train.partitioner = @partitioning.PjitPartitioner()\n    train_script.train.random_seed = %RANDOM_SEED\n    train_script.train.run_eval_before_training = True\n    train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config\n    train_script.train.total_steps = %TRAIN_STEPS\n    train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()\n    train_script.train.train_eval_dataset_cfg = @train_eval/utils.DatasetConfig()\n    train_script.train.trainer_cls = @trainer.Trainer\n    train_script.train.use_hardware_rng = %USE_HARDWARE_RNG\n    \n#### Parameters for trainer.Trainer:\n\n    trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()\n    trainer.Trainer.num_microbatches = None\n    \n#### Parameters for network.Transformer:\n\n    network.Transformer.config = @network.T5Config()""",10.674364,23.573328,2.4795532e-05,0.0012171788,0.001212679,98462.81,14.897102
xxl,690000,,-0.99584204,151027.73,-0.7704525,2048.007,2048.007,151039.39,-0.770512,-0.76920694,1.0016966,0.99202394,1.0016966,1.2863456,24119.404,20480000.0,849.1088,3.3168314,0.41460392,-81272.625,378.11877,11.6292925,-5.932564e-05,1.0344146778079253,,,4.0812297,,0.0012082481,0.0012038593,48516.273,17.540516
xxl,700000,,-0.9963975,150497.83,-0.7677492,2048.0076,2048.0076,150509.58,-0.76780915,-0.7665116,1.0016928,0.99204177,1.0016928,1.2857049,24119.574,20480000.0,849.1029,3.3168082,0.41460103,-81272.055,378.11612,11.548548,-5.8913727e-05,1.0344076585981956,,,4.0764523,,0.0011995289,0.0011952295,72712.805,374.58606
xxl,710000,63.77153,-0.9964699,150502.81,-0.7677747,2048.0093,2048.0093,150514.2,-0.7678328,-0.7665333,1.0016953,0.99203044,1.0016953,1.2861141,24119.494,20480000.0,849.1057,3.3168192,0.4146024,-81272.32,378.1174,11.378366,-5.8045567e-05,1.034382911791325,"b""    from __gin__ import dynamic_registration\n    import __main__ as train_script\n    import data.pile.tasks\n    from models.scalable_t5 import network\n    import seqio\n    from t5x import adafactor\n    from t5x import gin_utils\n    from t5x import models\n    from t5x import partitioning\n    from t5x import trainer\n    from t5x import utils\n    \n#### Macros:\n\n    BATCH_SIZE = 2048\n    DROPOUT_RATE = 0.0\n    EVAL_BATCH_SIZE = 256\n    LABEL_SMOOTHING = 0.0\n    LOSS_NORMALIZING_FACTOR = None\n    MIXTURE_OR_TASK_MODULE = None\n    MIXTURE_OR_TASK_NAME = 'pile_r_denoiser'\n    MODEL = @models.EncoderDecoderModel()\n    MODEL_DIR = 'gs://improved-t5/ckpts/v2_xxl_mlm/'\n    OPTIMIZER = @adafactor.Adafactor()\n    RANDOM_SEED = None\n    SAVING_PERIOD = 10000\n    SHUFFLE_TRAIN_EXAMPLES = True\n    TASK_FEATURE_LENGTHS = {'inputs': 512, 'targets': 114}\n    TRAIN_STEPS = 1000000\n    USE_CACHED_TASKS = False\n    USE_HARDWARE_RNG = False\n    VOCABULARY = @seqio.SentencePieceVocabulary()\n    Z_LOSS = 0.0001\n    \n#### Parameters for adafactor.Adafactor:\n\n    adafactor.Adafactor.decay_rate = 0.8\n    adafactor.Adafactor.logical_factor_rules = \\\n        @adafactor.standard_logical_factor_rules()\n    adafactor.Adafactor.step_offset = 0\n    \n#### Parameters for utils.CheckpointConfig:\n\n    utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()\n    utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()\n    \n#### Parameters for utils.create_learning_rate_scheduler:\n\n    utils.create_learning_rate_scheduler.base_learning_rate = 1.0\n    utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'\n    utils.create_learning_rate_scheduler.warmup_steps = 10000\n    \n#### Parameters for infer_eval/utils.DatasetConfig:\n\n    infer_eval/utils.DatasetConfig.batch_size = %EVAL_BATCH_SIZE\n    infer_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    infer_eval/utils.DatasetConfig.seed = 0\n    infer_eval/utils.DatasetConfig.shuffle = False\n    infer_eval/utils.DatasetConfig.split = 'validation'\n    infer_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    \n#### Parameters for train/utils.DatasetConfig:\n\n    train/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train/utils.DatasetConfig.pack = True\n    train/utils.DatasetConfig.seed = None\n    train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES\n    train/utils.DatasetConfig.split = 'train'\n    train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for train_eval/utils.DatasetConfig:\n\n    train_eval/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train_eval/utils.DatasetConfig.pack = True\n    train_eval/utils.DatasetConfig.seed = 42\n    train_eval/utils.DatasetConfig.shuffle = False\n    train_eval/utils.DatasetConfig.split = 'validation'\n    train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for models.EncoderDecoderModel:\n\n    models.EncoderDecoderModel.input_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.label_smoothing = %LABEL_SMOOTHING\n    models.EncoderDecoderModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR\n    models.EncoderDecoderModel.module = @network.Transformer()\n    models.EncoderDecoderModel.optimizer_def = %OPTIMIZER\n    models.EncoderDecoderModel.output_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.z_loss = %Z_LOSS\n    \n#### Parameters for seqio.Evaluator:\n\n    seqio.Evaluator.logger_cls = \\\n        [@seqio.PyLoggingLogger, @seqio.TensorBoardLogger, @seqio.JSONLogger]\n    seqio.Evaluator.num_examples = 2048\n    \n#### Parameters for partitioning.PjitPartitioner:\n\n    partitioning.PjitPartitioner.logical_axis_rules = \\\n        @partitioning.standard_logical_axis_rules()\n    partitioning.PjitPartitioner.model_parallel_submesh = None\n    partitioning.PjitPartitioner.num_partitions = 1\n    \n#### Parameters for utils.RestoreCheckpointConfig:\n\n    utils.RestoreCheckpointConfig.path = []\n    \n#### Parameters for utils.SaveCheckpointConfig:\n\n    utils.SaveCheckpointConfig.dtype = 'float32'\n    utils.SaveCheckpointConfig.keep = None\n    utils.SaveCheckpointConfig.period = %SAVING_PERIOD\n    utils.SaveCheckpointConfig.save_dataset = False\n    \n#### Parameters for seqio.SentencePieceVocabulary:\n\n    seqio.SentencePieceVocabulary.extra_ids = 100\n    seqio.SentencePieceVocabulary.sentencepiece_model_file = \\\n        'gs://improved-t5/vocabs/tokenizer.model'\n    \n#### Parameters for partitioning.standard_logical_axis_rules:\n\n    partitioning.standard_logical_axis_rules.activation_partitioning_dims = 1\n    partitioning.standard_logical_axis_rules.parameter_partitioning_dims = 2\n    \n#### Parameters for network.T5Config:\n\n    network.T5Config.dropout_rate = %DROPOUT_RATE\n    network.T5Config.dtype = 'bfloat16'\n    network.T5Config.emb_dim = 4096\n    network.T5Config.head_dim = 64\n    network.T5Config.logits_via_embedding = False\n    network.T5Config.mlp_activations = ('gelu', 'linear')\n    network.T5Config.mlp_dim = 10240\n    network.T5Config.num_decoder_layers = 24\n    network.T5Config.num_encoder_layers = 24\n    network.T5Config.num_heads = 64\n    network.T5Config.remat_policy = 'minimal'\n    network.T5Config.scan_layers = True\n    network.T5Config.vocab_size = 32128\n    \n#### Parameters for train_script.train:\n\n    train_script.train.checkpoint_cfg = @utils.CheckpointConfig()\n    train_script.train.eval_period = %SAVING_PERIOD\n    train_script.train.eval_steps = 100\n    train_script.train.infer_eval_dataset_cfg = @infer_eval/utils.DatasetConfig()\n    train_script.train.inference_evaluator_cls = @seqio.Evaluator\n    train_script.train.model = %MODEL\n    train_script.train.model_dir = %MODEL_DIR\n    train_script.train.partitioner = @partitioning.PjitPartitioner()\n    train_script.train.random_seed = %RANDOM_SEED\n    train_script.train.run_eval_before_training = True\n    train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config\n    train_script.train.total_steps = %TRAIN_STEPS\n    train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()\n    train_script.train.train_eval_dataset_cfg = @train_eval/utils.DatasetConfig()\n    train_script.train.trainer_cls = @trainer.Trainer\n    train_script.train.use_hardware_rng = %USE_HARDWARE_RNG\n    \n#### Parameters for trainer.Trainer:\n\n    trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()\n    trainer.Trainer.num_microbatches = None\n    \n#### Parameters for network.Transformer:\n\n    network.Transformer.config = @network.T5Config()""",21.287838,26.372004,1.5735626e-05,0.0011909944,0.0011867824,97266.26,31.933594
xxl,600000,51.454594,-0.9933334,1620209800.0,-8265.334,2048.007,2048.0312,1620686200.0,-8267.765,-29837.393,1.0016924,0.9920443,0.27709407,1.2856183,24522.693,20480000.0,835.1447,3.262284,0.4077855,-79936.05,371.9004,476503.53,-2.4308338,1.0347703757604185,"b""    from __gin__ import dynamic_registration\n    import __main__ as train_script\n    import data.pile.tasks\n    from models.scalable_t5 import network\n    import seqio\n    from t5x import adafactor\n    from t5x import gin_utils\n    from t5x import models\n    from t5x import partitioning\n    from t5x import trainer\n    from t5x import utils\n    \n#### Macros:\n\n    BATCH_SIZE = 2048\n    DROPOUT_RATE = 0.0\n    EVAL_BATCH_SIZE = 256\n    LABEL_SMOOTHING = 0.0\n    LOSS_NORMALIZING_FACTOR = None\n    MIXTURE_OR_TASK_MODULE = None\n    MIXTURE_OR_TASK_NAME = 'pile_r_denoiser'\n    MODEL = @models.EncoderDecoderModel()\n    MODEL_DIR = 'gs://improved-t5/ckpts/v2_xxl_mlm/'\n    OPTIMIZER = @adafactor.Adafactor()\n    RANDOM_SEED = None\n    SAVING_PERIOD = 10000\n    SHUFFLE_TRAIN_EXAMPLES = True\n    TASK_FEATURE_LENGTHS = {'inputs': 512, 'targets': 114}\n    TRAIN_STEPS = 1000000\n    USE_CACHED_TASKS = False\n    USE_HARDWARE_RNG = False\n    VOCABULARY = @seqio.SentencePieceVocabulary()\n    Z_LOSS = 0.0001\n    \n#### Parameters for adafactor.Adafactor:\n\n    adafactor.Adafactor.decay_rate = 0.8\n    adafactor.Adafactor.logical_factor_rules = \\\n        @adafactor.standard_logical_factor_rules()\n    adafactor.Adafactor.step_offset = 0\n    \n#### Parameters for utils.CheckpointConfig:\n\n    utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()\n    utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()\n    \n#### Parameters for utils.create_learning_rate_scheduler:\n\n    utils.create_learning_rate_scheduler.base_learning_rate = 1.0\n    utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'\n    utils.create_learning_rate_scheduler.warmup_steps = 10000\n    \n#### Parameters for infer_eval/utils.DatasetConfig:\n\n    infer_eval/utils.DatasetConfig.batch_size = %EVAL_BATCH_SIZE\n    infer_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    infer_eval/utils.DatasetConfig.seed = 0\n    infer_eval/utils.DatasetConfig.shuffle = False\n    infer_eval/utils.DatasetConfig.split = 'validation'\n    infer_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    \n#### Parameters for train/utils.DatasetConfig:\n\n    train/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train/utils.DatasetConfig.pack = True\n    train/utils.DatasetConfig.seed = None\n    train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES\n    train/utils.DatasetConfig.split = 'train'\n    train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for train_eval/utils.DatasetConfig:\n\n    train_eval/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train_eval/utils.DatasetConfig.pack = True\n    train_eval/utils.DatasetConfig.seed = 42\n    train_eval/utils.DatasetConfig.shuffle = False\n    train_eval/utils.DatasetConfig.split = 'validation'\n    train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for models.EncoderDecoderModel:\n\n    models.EncoderDecoderModel.input_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.label_smoothing = %LABEL_SMOOTHING\n    models.EncoderDecoderModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR\n    models.EncoderDecoderModel.module = @network.Transformer()\n    models.EncoderDecoderModel.optimizer_def = %OPTIMIZER\n    models.EncoderDecoderModel.output_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.z_loss = %Z_LOSS\n    \n#### Parameters for seqio.Evaluator:\n\n    seqio.Evaluator.logger_cls = \\\n        [@seqio.PyLoggingLogger, @seqio.TensorBoardLogger, @seqio.JSONLogger]\n    seqio.Evaluator.num_examples = 2048\n    \n#### Parameters for partitioning.PjitPartitioner:\n\n    partitioning.PjitPartitioner.logical_axis_rules = \\\n        @partitioning.standard_logical_axis_rules()\n    partitioning.PjitPartitioner.model_parallel_submesh = None\n    partitioning.PjitPartitioner.num_partitions = 1\n    \n#### Parameters for utils.RestoreCheckpointConfig:\n\n    utils.RestoreCheckpointConfig.path = []\n    \n#### Parameters for utils.SaveCheckpointConfig:\n\n    utils.SaveCheckpointConfig.dtype = 'float32'\n    utils.SaveCheckpointConfig.keep = None\n    utils.SaveCheckpointConfig.period = %SAVING_PERIOD\n    utils.SaveCheckpointConfig.save_dataset = False\n    \n#### Parameters for seqio.SentencePieceVocabulary:\n\n    seqio.SentencePieceVocabulary.extra_ids = 100\n    seqio.SentencePieceVocabulary.sentencepiece_model_file = \\\n        'gs://improved-t5/vocabs/tokenizer.model'\n    \n#### Parameters for partitioning.standard_logical_axis_rules:\n\n    partitioning.standard_logical_axis_rules.activation_partitioning_dims = 1\n    partitioning.standard_logical_axis_rules.parameter_partitioning_dims = 2\n    \n#### Parameters for network.T5Config:\n\n    network.T5Config.dropout_rate = %DROPOUT_RATE\n    network.T5Config.dtype = 'bfloat16'\n    network.T5Config.emb_dim = 4096\n    network.T5Config.head_dim = 64\n    network.T5Config.logits_via_embedding = False\n    network.T5Config.mlp_activations = ('gelu', 'linear')\n    network.T5Config.mlp_dim = 10240\n    network.T5Config.num_decoder_layers = 24\n    network.T5Config.num_encoder_layers = 24\n    network.T5Config.num_heads = 64\n    network.T5Config.remat_policy = 'minimal'\n    network.T5Config.scan_layers = True\n    network.T5Config.vocab_size = 32128\n    \n#### Parameters for train_script.train:\n\n    train_script.train.checkpoint_cfg = @utils.CheckpointConfig()\n    train_script.train.eval_period = %SAVING_PERIOD\n    train_script.train.eval_steps = 100\n    train_script.train.infer_eval_dataset_cfg = @infer_eval/utils.DatasetConfig()\n    train_script.train.inference_evaluator_cls = @seqio.Evaluator\n    train_script.train.model = %MODEL\n    train_script.train.model_dir = %MODEL_DIR\n    train_script.train.partitioner = @partitioning.PjitPartitioner()\n    train_script.train.random_seed = %RANDOM_SEED\n    train_script.train.run_eval_before_training = True\n    train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config\n    train_script.train.total_steps = %TRAIN_STEPS\n    train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()\n    train_script.train.train_eval_dataset_cfg = @train_eval/utils.DatasetConfig()\n    train_script.train.trainer_cls = @trainer.Trainer\n    train_script.train.use_hardware_rng = %USE_HARDWARE_RNG\n    \n#### Parameters for trainer.Trainer:\n\n    trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()\n    trainer.Trainer.num_microbatches = None\n    \n#### Parameters for network.Transformer:\n\n    network.Transformer.config = @network.T5Config()""",9.945432,4.078705,1.4305115e-05,0.0012964226,0.0012909955,98505.06,20.311268
xxl,610000,,-0.99395025,153065.67,-0.7808488,2048.0518,2048.0579,153082.34,-0.78093386,-0.77960956,1.0016984,0.9920155,1.0016986,1.2866498,24522.729,20480000.0,835.1436,3.2622797,0.40778497,-79935.94,371.8999,16.47142,-8.4027255e-05,1.0346348493251152,,,4.075556,,0.0012856664,0.0012803698,123108.6,17.979033
xxl,620000,,-0.41214502,nan,nan,2048.098,2048.009,nan,nan,nan,1.0016916,0.9920474,0.24633996,1.2855026,24522.785,20480000.0,835.14166,3.2622721,0.40778401,-79935.75,371.89905,nan,nan,1.0345716263793139,,,4.090306,,0.0012751668,0.0012700022,147709.86,14.571421
xxl,630000,,-6.213148e-08,nan,nan,2048.0068,2048.0068,nan,nan,nan,1.0016991,0.9920124,1.0016991,1.2867631,24523.305,20480000.0,835.12396,3.262203,0.40777537,-79934.055,371.89114,nan,nan,1.0345336785200279,,,4.0969944,,0.0012649208,0.0012598827,172308.2,15.749658
xxl,640000,50.916824,-2.3223073e-07,nan,nan,2048.0403,2048.207,nan,nan,nan,1.001692,0.99204516,0.9971796,1.2855841,24522.959,20480000.0,835.1358,3.2622492,0.40778115,-79935.19,371.89642,nan,nan,1.0344945870913749,"b""    from __gin__ import dynamic_registration\n    import __main__ as train_script\n    import data.pile.tasks\n    from models.scalable_t5 import network\n    import seqio\n    from t5x import adafactor\n    from t5x import gin_utils\n    from t5x import models\n    from t5x import partitioning\n    from t5x import trainer\n    from t5x import utils\n    \n#### Macros:\n\n    BATCH_SIZE = 2048\n    DROPOUT_RATE = 0.0\n    EVAL_BATCH_SIZE = 256\n    LABEL_SMOOTHING = 0.0\n    LOSS_NORMALIZING_FACTOR = None\n    MIXTURE_OR_TASK_MODULE = None\n    MIXTURE_OR_TASK_NAME = 'pile_r_denoiser'\n    MODEL = @models.EncoderDecoderModel()\n    MODEL_DIR = 'gs://improved-t5/ckpts/v2_xxl_mlm/'\n    OPTIMIZER = @adafactor.Adafactor()\n    RANDOM_SEED = None\n    SAVING_PERIOD = 10000\n    SHUFFLE_TRAIN_EXAMPLES = True\n    TASK_FEATURE_LENGTHS = {'inputs': 512, 'targets': 114}\n    TRAIN_STEPS = 1000000\n    USE_CACHED_TASKS = False\n    USE_HARDWARE_RNG = False\n    VOCABULARY = @seqio.SentencePieceVocabulary()\n    Z_LOSS = 0.0001\n    \n#### Parameters for adafactor.Adafactor:\n\n    adafactor.Adafactor.decay_rate = 0.8\n    adafactor.Adafactor.logical_factor_rules = \\\n        @adafactor.standard_logical_factor_rules()\n    adafactor.Adafactor.step_offset = 0\n    \n#### Parameters for utils.CheckpointConfig:\n\n    utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()\n    utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()\n    \n#### Parameters for utils.create_learning_rate_scheduler:\n\n    utils.create_learning_rate_scheduler.base_learning_rate = 1.0\n    utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'\n    utils.create_learning_rate_scheduler.warmup_steps = 10000\n    \n#### Parameters for infer_eval/utils.DatasetConfig:\n\n    infer_eval/utils.DatasetConfig.batch_size = %EVAL_BATCH_SIZE\n    infer_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    infer_eval/utils.DatasetConfig.seed = 0\n    infer_eval/utils.DatasetConfig.shuffle = False\n    infer_eval/utils.DatasetConfig.split = 'validation'\n    infer_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    \n#### Parameters for train/utils.DatasetConfig:\n\n    train/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train/utils.DatasetConfig.pack = True\n    train/utils.DatasetConfig.seed = None\n    train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES\n    train/utils.DatasetConfig.split = 'train'\n    train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for train_eval/utils.DatasetConfig:\n\n    train_eval/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train_eval/utils.DatasetConfig.pack = True\n    train_eval/utils.DatasetConfig.seed = 42\n    train_eval/utils.DatasetConfig.shuffle = False\n    train_eval/utils.DatasetConfig.split = 'validation'\n    train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for models.EncoderDecoderModel:\n\n    models.EncoderDecoderModel.input_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.label_smoothing = %LABEL_SMOOTHING\n    models.EncoderDecoderModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR\n    models.EncoderDecoderModel.module = @network.Transformer()\n    models.EncoderDecoderModel.optimizer_def = %OPTIMIZER\n    models.EncoderDecoderModel.output_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.z_loss = %Z_LOSS\n    \n#### Parameters for seqio.Evaluator:\n\n    seqio.Evaluator.logger_cls = \\\n        [@seqio.PyLoggingLogger, @seqio.TensorBoardLogger, @seqio.JSONLogger]\n    seqio.Evaluator.num_examples = 2048\n    \n#### Parameters for partitioning.PjitPartitioner:\n\n    partitioning.PjitPartitioner.logical_axis_rules = \\\n        @partitioning.standard_logical_axis_rules()\n    partitioning.PjitPartitioner.model_parallel_submesh = None\n    partitioning.PjitPartitioner.num_partitions = 1\n    \n#### Parameters for utils.RestoreCheckpointConfig:\n\n    utils.RestoreCheckpointConfig.path = []\n    \n#### Parameters for utils.SaveCheckpointConfig:\n\n    utils.SaveCheckpointConfig.dtype = 'float32'\n    utils.SaveCheckpointConfig.keep = None\n    utils.SaveCheckpointConfig.period = %SAVING_PERIOD\n    utils.SaveCheckpointConfig.save_dataset = False\n    \n#### Parameters for seqio.SentencePieceVocabulary:\n\n    seqio.SentencePieceVocabulary.extra_ids = 100\n    seqio.SentencePieceVocabulary.sentencepiece_model_file = \\\n        'gs://improved-t5/vocabs/tokenizer.model'\n    \n#### Parameters for partitioning.standard_logical_axis_rules:\n\n    partitioning.standard_logical_axis_rules.activation_partitioning_dims = 1\n    partitioning.standard_logical_axis_rules.parameter_partitioning_dims = 2\n    \n#### Parameters for network.T5Config:\n\n    network.T5Config.dropout_rate = %DROPOUT_RATE\n    network.T5Config.dtype = 'bfloat16'\n    network.T5Config.emb_dim = 4096\n    network.T5Config.head_dim = 64\n    network.T5Config.logits_via_embedding = False\n    network.T5Config.mlp_activations = ('gelu', 'linear')\n    network.T5Config.mlp_dim = 10240\n    network.T5Config.num_decoder_layers = 24\n    network.T5Config.num_encoder_layers = 24\n    network.T5Config.num_heads = 64\n    network.T5Config.remat_policy = 'minimal'\n    network.T5Config.scan_layers = True\n    network.T5Config.vocab_size = 32128\n    \n#### Parameters for train_script.train:\n\n    train_script.train.checkpoint_cfg = @utils.CheckpointConfig()\n    train_script.train.eval_period = %SAVING_PERIOD\n    train_script.train.eval_steps = 100\n    train_script.train.infer_eval_dataset_cfg = @infer_eval/utils.DatasetConfig()\n    train_script.train.inference_evaluator_cls = @seqio.Evaluator\n    train_script.train.model = %MODEL\n    train_script.train.model_dir = %MODEL_DIR\n    train_script.train.partitioner = @partitioning.PjitPartitioner()\n    train_script.train.random_seed = %RANDOM_SEED\n    train_script.train.run_eval_before_training = True\n    train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config\n    train_script.train.total_steps = %TRAIN_STEPS\n    train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()\n    train_script.train.train_eval_dataset_cfg = @train_eval/utils.DatasetConfig()\n    train_script.train.trainer_cls = @trainer.Trainer\n    train_script.train.use_hardware_rng = %USE_HARDWARE_RNG\n    \n#### Parameters for trainer.Trainer:\n\n    trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()\n    trainer.Trainer.num_microbatches = None\n    \n#### Parameters for network.Transformer:\n\n    network.Transformer.config = @network.T5Config()""",9.570622,23.3171,2.527237e-05,0.0012549129,0.0012500009,196907.36,20.514677
xxl,650000,,0.0,nan,nan,2048.0068,2048.0068,nan,nan,nan,1.001696,0.9920266,1.001696,1.2862475,24520.193,20480000.0,835.23,3.262617,0.40782714,-79944.2,371.93835,nan,nan,1.034485600249905,,,4.095031,,0.0012451629,0.0012403483,24682.732,15.547555
xxl,660000,,0.0,nan,nan,2048.0066,2048.0066,nan,nan,nan,1.0016903,0.99205345,1.0016903,1.2852829,24517.504,20480000.0,835.3216,3.262975,0.40787187,-79952.98,371.97916,nan,nan,1.0344594060929875,,,4.0821643,,0.0012356176,0.0012309158,49276.293,15.22248
xxl,530000,50.1029,-0.99059004,156544.97,-0.79859805,2048.008,2048.008,156567.23,-0.7987117,-0.7973485,1.0017097,0.9919619,1.0017097,1.2885692,24471.895,20480000.0,836.8784,3.2690563,0.40863204,-80101.984,372.67242,22.211603,-0.00011331021,1.0353821740009077,"b""    from __gin__ import dynamic_registration\n    import __main__ as train_script\n    import data.pile.tasks\n    from models.scalable_t5 import network\n    import seqio\n    from t5x import adafactor\n    from t5x import gin_utils\n    from t5x import models\n    from t5x import partitioning\n    from t5x import trainer\n    from t5x import utils\n    \n#### Macros:\n\n    BATCH_SIZE = 2048\n    DROPOUT_RATE = 0.0\n    EVAL_BATCH_SIZE = 256\n    LABEL_SMOOTHING = 0.0\n    LOSS_NORMALIZING_FACTOR = None\n    MIXTURE_OR_TASK_MODULE = None\n    MIXTURE_OR_TASK_NAME = 'pile_r_denoiser'\n    MODEL = @models.EncoderDecoderModel()\n    MODEL_DIR = 'gs://improved-t5/ckpts/v2_xxl_mlm/'\n    OPTIMIZER = @adafactor.Adafactor()\n    RANDOM_SEED = None\n    SAVING_PERIOD = 10000\n    SHUFFLE_TRAIN_EXAMPLES = True\n    TASK_FEATURE_LENGTHS = {'inputs': 512, 'targets': 114}\n    TRAIN_STEPS = 1000000\n    USE_CACHED_TASKS = False\n    USE_HARDWARE_RNG = False\n    VOCABULARY = @seqio.SentencePieceVocabulary()\n    Z_LOSS = 0.0001\n    \n#### Parameters for adafactor.Adafactor:\n\n    adafactor.Adafactor.decay_rate = 0.8\n    adafactor.Adafactor.logical_factor_rules = \\\n        @adafactor.standard_logical_factor_rules()\n    adafactor.Adafactor.step_offset = 0\n    \n#### Parameters for utils.CheckpointConfig:\n\n    utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()\n    utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()\n    \n#### Parameters for utils.create_learning_rate_scheduler:\n\n    utils.create_learning_rate_scheduler.base_learning_rate = 1.0\n    utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'\n    utils.create_learning_rate_scheduler.warmup_steps = 10000\n    \n#### Parameters for infer_eval/utils.DatasetConfig:\n\n    infer_eval/utils.DatasetConfig.batch_size = %EVAL_BATCH_SIZE\n    infer_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    infer_eval/utils.DatasetConfig.seed = 0\n    infer_eval/utils.DatasetConfig.shuffle = False\n    infer_eval/utils.DatasetConfig.split = 'validation'\n    infer_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    \n#### Parameters for train/utils.DatasetConfig:\n\n    train/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train/utils.DatasetConfig.pack = True\n    train/utils.DatasetConfig.seed = None\n    train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES\n    train/utils.DatasetConfig.split = 'train'\n    train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for train_eval/utils.DatasetConfig:\n\n    train_eval/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train_eval/utils.DatasetConfig.pack = True\n    train_eval/utils.DatasetConfig.seed = 42\n    train_eval/utils.DatasetConfig.shuffle = False\n    train_eval/utils.DatasetConfig.split = 'validation'\n    train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for models.EncoderDecoderModel:\n\n    models.EncoderDecoderModel.input_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.label_smoothing = %LABEL_SMOOTHING\n    models.EncoderDecoderModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR\n    models.EncoderDecoderModel.module = @network.Transformer()\n    models.EncoderDecoderModel.optimizer_def = %OPTIMIZER\n    models.EncoderDecoderModel.output_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.z_loss = %Z_LOSS\n    \n#### Parameters for seqio.Evaluator:\n\n    seqio.Evaluator.logger_cls = \\\n        [@seqio.PyLoggingLogger, @seqio.TensorBoardLogger, @seqio.JSONLogger]\n    seqio.Evaluator.num_examples = 2048\n    \n#### Parameters for partitioning.PjitPartitioner:\n\n    partitioning.PjitPartitioner.logical_axis_rules = \\\n        @partitioning.standard_logical_axis_rules()\n    partitioning.PjitPartitioner.model_parallel_submesh = None\n    partitioning.PjitPartitioner.num_partitions = 1\n    \n#### Parameters for utils.RestoreCheckpointConfig:\n\n    utils.RestoreCheckpointConfig.path = []\n    \n#### Parameters for utils.SaveCheckpointConfig:\n\n    utils.SaveCheckpointConfig.dtype = 'float32'\n    utils.SaveCheckpointConfig.keep = None\n    utils.SaveCheckpointConfig.period = %SAVING_PERIOD\n    utils.SaveCheckpointConfig.save_dataset = False\n    \n#### Parameters for seqio.SentencePieceVocabulary:\n\n    seqio.SentencePieceVocabulary.extra_ids = 100\n    seqio.SentencePieceVocabulary.sentencepiece_model_file = \\\n        'gs://improved-t5/vocabs/tokenizer.model'\n    \n#### Parameters for partitioning.standard_logical_axis_rules:\n\n    partitioning.standard_logical_axis_rules.activation_partitioning_dims = 1\n    partitioning.standard_logical_axis_rules.parameter_partitioning_dims = 2\n    \n#### Parameters for network.T5Config:\n\n    network.T5Config.dropout_rate = %DROPOUT_RATE\n    network.T5Config.dtype = 'bfloat16'\n    network.T5Config.emb_dim = 4096\n    network.T5Config.head_dim = 64\n    network.T5Config.logits_via_embedding = False\n    network.T5Config.mlp_activations = ('gelu', 'linear')\n    network.T5Config.mlp_dim = 10240\n    network.T5Config.num_decoder_layers = 24\n    network.T5Config.num_encoder_layers = 24\n    network.T5Config.num_heads = 64\n    network.T5Config.remat_policy = 'minimal'\n    network.T5Config.scan_layers = True\n    network.T5Config.vocab_size = 32128\n    \n#### Parameters for train_script.train:\n\n    train_script.train.checkpoint_cfg = @utils.CheckpointConfig()\n    train_script.train.eval_period = %SAVING_PERIOD\n    train_script.train.eval_steps = 100\n    train_script.train.infer_eval_dataset_cfg = @infer_eval/utils.DatasetConfig()\n    train_script.train.inference_evaluator_cls = @seqio.Evaluator\n    train_script.train.model = %MODEL\n    train_script.train.model_dir = %MODEL_DIR\n    train_script.train.partitioner = @partitioning.PjitPartitioner()\n    train_script.train.random_seed = %RANDOM_SEED\n    train_script.train.run_eval_before_training = True\n    train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config\n    train_script.train.total_steps = %TRAIN_STEPS\n    train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()\n    train_script.train.train_eval_dataset_cfg = @train_eval/utils.DatasetConfig()\n    train_script.train.trainer_cls = @trainer.Trainer\n    train_script.train.use_hardware_rng = %USE_HARDWARE_RNG\n    \n#### Parameters for trainer.Trainer:\n\n    trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()\n    trainer.Trainer.num_microbatches = None\n    \n#### Parameters for network.Transformer:\n\n    network.Transformer.config = @network.T5Config()""",10.5484085,23.430714,1.4066696e-05,0.0013801508,0.001373607,441983.7,17.331846
xxl,540000,,-0.9924564,154493.12,-0.7881308,2048.006,2048.006,154514.92,-0.788242,-0.78690755,1.0016958,0.99202794,1.0016958,1.2862027,24529.865,20480000.0,834.9006,3.2613304,0.4076663,-79912.68,371.79166,21.871218,-0.000111573776,1.0351859038239766,,,4.090041,,0.0013671798,0.001360829,24690.064,21.879091
xxl,550000,,-0.99195254,154932.78,-0.7903737,2048.0068,2048.0068,154954.11,-0.79048246,-0.7891371,1.0017048,0.9919862,1.0017048,1.2877066,24526.312,20480000.0,835.0215,3.2618027,0.40772533,-79924.25,371.84552,21.567057,-0.00011002212,1.0351785704985492,,,4.0949125,,0.001354586,0.001348401,49298.766,18.118326
xxl,1620000,21.571056,-0.99985355,147064.55,-0.75023466,2048.0068,2048.0068,147073.75,-0.75028163,-0.7490226,1.0016809,0.99209803,1.0016809,1.2836871,13891.97,20480000.0,1474.233,2.8793614,0.71984035,-141106.5,328.2472,9.4379635,-4.8146798e-05,1.0337283472735799,"b""    from __gin__ import dynamic_registration\n    import __main__ as train_script\n    import data.pile.tasks\n    from models.scalable_t5 import network\n    import seqio\n    from t5x import adafactor\n    from t5x import gin_utils\n    from t5x import models\n    from t5x import partitioning\n    from t5x import trainer\n    from t5x import utils\n    \n#### Macros:\n\n    BATCH_SIZE = 2048\n    DROPOUT_RATE = 0.0\n    EVAL_BATCH_SIZE = 256\n    LABEL_SMOOTHING = 0.0\n    LOSS_NORMALIZING_FACTOR = None\n    MIXTURE_OR_TASK_MODULE = None\n    MIXTURE_OR_TASK_NAME = 'pile_r_denoiser'\n    MODEL = @models.EncoderDecoderModel()\n    MODEL_DIR = 'gs://improved-t5/ckpts/v2_xxl_mlm/'\n    OPTIMIZER = @adafactor.Adafactor()\n    RANDOM_SEED = None\n    SAVING_PERIOD = 10000\n    SHUFFLE_TRAIN_EXAMPLES = True\n    TASK_FEATURE_LENGTHS = {'inputs': 512, 'targets': 114}\n    TRAIN_STEPS = 2000000\n    USE_CACHED_TASKS = False\n    USE_HARDWARE_RNG = False\n    VOCABULARY = @seqio.SentencePieceVocabulary()\n    Z_LOSS = 0.0001\n    \n#### Parameters for adafactor.Adafactor:\n\n    adafactor.Adafactor.decay_rate = 0.8\n    adafactor.Adafactor.logical_factor_rules = \\\n        @adafactor.standard_logical_factor_rules()\n    adafactor.Adafactor.step_offset = 0\n    \n#### Parameters for utils.CheckpointConfig:\n\n    utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()\n    utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()\n    \n#### Parameters for utils.create_learning_rate_scheduler:\n\n    utils.create_learning_rate_scheduler.base_learning_rate = 1.0\n    utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'\n    utils.create_learning_rate_scheduler.warmup_steps = 10000\n    \n#### Parameters for infer_eval/utils.DatasetConfig:\n\n    infer_eval/utils.DatasetConfig.batch_size = %EVAL_BATCH_SIZE\n    infer_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    infer_eval/utils.DatasetConfig.seed = 0\n    infer_eval/utils.DatasetConfig.shuffle = False\n    infer_eval/utils.DatasetConfig.split = 'validation'\n    infer_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    \n#### Parameters for train/utils.DatasetConfig:\n\n    train/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train/utils.DatasetConfig.pack = True\n    train/utils.DatasetConfig.seed = None\n    train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES\n    train/utils.DatasetConfig.split = 'train'\n    train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for train_eval/utils.DatasetConfig:\n\n    train_eval/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train_eval/utils.DatasetConfig.pack = True\n    train_eval/utils.DatasetConfig.seed = 42\n    train_eval/utils.DatasetConfig.shuffle = False\n    train_eval/utils.DatasetConfig.split = 'validation'\n    train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for models.EncoderDecoderModel:\n\n    models.EncoderDecoderModel.input_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.label_smoothing = %LABEL_SMOOTHING\n    models.EncoderDecoderModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR\n    models.EncoderDecoderModel.module = @network.Transformer()\n    models.EncoderDecoderModel.optimizer_def = %OPTIMIZER\n    models.EncoderDecoderModel.output_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.z_loss = %Z_LOSS\n    \n#### Parameters for seqio.Evaluator:\n\n    seqio.Evaluator.logger_cls = \\\n        [@seqio.PyLoggingLogger, @seqio.TensorBoardLogger, @seqio.JSONLogger]\n    seqio.Evaluator.num_examples = 2048\n    \n#### Parameters for partitioning.PjitPartitioner:\n\n    partitioning.PjitPartitioner.logical_axis_rules = \\\n        @partitioning.standard_logical_axis_rules()\n    partitioning.PjitPartitioner.model_parallel_submesh = None\n    partitioning.PjitPartitioner.num_partitions = 4\n    \n#### Parameters for utils.RestoreCheckpointConfig:\n\n    utils.RestoreCheckpointConfig.path = []\n    \n#### Parameters for utils.SaveCheckpointConfig:\n\n    utils.SaveCheckpointConfig.dtype = 'float32'\n    utils.SaveCheckpointConfig.keep = None\n    utils.SaveCheckpointConfig.period = %SAVING_PERIOD\n    utils.SaveCheckpointConfig.save_dataset = False\n    \n#### Parameters for seqio.SentencePieceVocabulary:\n\n    seqio.SentencePieceVocabulary.extra_ids = 100\n    seqio.SentencePieceVocabulary.sentencepiece_model_file = \\\n        'gs://improved-t5/vocabs/tokenizer.model'\n    \n#### Parameters for network.T5Config:\n\n    network.T5Config.dropout_rate = %DROPOUT_RATE\n    network.T5Config.dtype = 'bfloat16'\n    network.T5Config.emb_dim = 4096\n    network.T5Config.head_dim = 64\n    network.T5Config.logits_via_embedding = False\n    network.T5Config.mlp_activations = ('gelu', 'linear')\n    network.T5Config.mlp_dim = 10240\n    network.T5Config.num_decoder_layers = 24\n    network.T5Config.num_encoder_layers = 24\n    network.T5Config.num_heads = 64\n    network.T5Config.remat_policy = 'minimal'\n    network.T5Config.scan_layers = True\n    network.T5Config.vocab_size = 32128\n    \n#### Parameters for train_script.train:\n\n    train_script.train.checkpoint_cfg = @utils.CheckpointConfig()\n    train_script.train.eval_period = %SAVING_PERIOD\n    train_script.train.eval_steps = 100\n    train_script.train.infer_eval_dataset_cfg = @infer_eval/utils.DatasetConfig()\n    train_script.train.inference_evaluator_cls = @seqio.Evaluator\n    train_script.train.model = %MODEL\n    train_script.train.model_dir = %MODEL_DIR\n    train_script.train.partitioner = @partitioning.PjitPartitioner()\n    train_script.train.random_seed = %RANDOM_SEED\n    train_script.train.run_eval_before_training = True\n    train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config\n    train_script.train.total_steps = %TRAIN_STEPS\n    train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()\n    train_script.train.train_eval_dataset_cfg = @train_eval/utils.DatasetConfig()\n    train_script.train.trainer_cls = @trainer.Trainer\n    train_script.train.use_hardware_rng = %USE_HARDWARE_RNG\n    \n#### Parameters for trainer.Trainer:\n\n    trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()\n    trainer.Trainer.num_microbatches = None\n    \n#### Parameters for network.Transformer:\n\n    network.Transformer.config = @network.T5Config()""",133.09445,0.97491217,1.5497208e-05,0.00078689476,0.00078567443,210661.94,42.873272
xxl,1000100,,0.8421099,147279.14,0.63082147,2048.0,2048.0,147289.42,0.6308655,0.631753,0.9985951,0.9985786,0.9985951,0.9985816,141.89394,204800.0,1443.3317,2.8190072,0.7047518,164539.81,321.36682,10.286579,4.4059158e-05,1.0341423809539492,,,0.9805989,,0.000999975,0.0009999505,293.48764,52.109657
xxl,1000200,,0.84041804,149277.95,0.63938266,2048.0,2048.0,149288.17,0.63942647,0.64032537,0.9985962,0.99857837,0.9985962,0.99858165,139.82086,204800.0,1464.7314,2.8608036,0.7152009,166979.38,326.1316,10.2123,4.3741005e-05,1.0341429883300055,,,1.307203,,0.0009999253,0.0009999005,521.33685,45.376404
xxl,1000300,,0.8398857,149923.19,0.64214635,2048.01,2048.01,149933.55,0.6421907,0.6431002,0.99858576,0.9985672,0.99858576,0.9985706,139.82146,204800.0,1464.7251,2.8607912,0.7151978,166978.66,326.1302,10.309402,4.4156914e-05,1.0341408171515605,,,0.97568536,,0.0009998753,0.0009998506,742.7297,373.30917
xxl,1110000,,-0.99768716,149260.6,-0.7614376,2048.0063,2048.0063,149270.44,-0.7614878,-0.7602004,1.0016935,0.9920382,1.0016935,1.2858331,13981.705,20480000.0,1464.7712,2.8608813,0.71522033,-140200.86,326.14047,10.173343,-5.1898263e-05,1.034080325648246,,,0.9827316,,0.00095131237,0.00094915845,14112.936,46.24077
xxl,1120000,20.931316,-0.99728596,149594.33,-0.7631401,2048.0088,2048.0088,149604.39,-0.7631914,-0.7618994,1.0016958,0.9920277,1.0016958,1.2862105,13979.858,20480000.0,1464.9647,2.8612592,0.7153148,-140219.39,326.18356,10.111316,-5.1581836e-05,1.034073693649343,"b""    from __gin__ import dynamic_registration\n    import __main__ as train_script\n    import data.pile.tasks\n    from models.scalable_t5 import network\n    import seqio\n    from t5x import adafactor\n    from t5x import gin_utils\n    from t5x import models\n    from t5x import partitioning\n    from t5x import trainer\n    from t5x import utils\n    \n#### Macros:\n\n    BATCH_SIZE = 2048\n    DROPOUT_RATE = 0.0\n    EVAL_BATCH_SIZE = 256\n    LABEL_SMOOTHING = 0.0\n    LOSS_NORMALIZING_FACTOR = None\n    MIXTURE_OR_TASK_MODULE = None\n    MIXTURE_OR_TASK_NAME = 'pile_r_denoiser'\n    MODEL = @models.EncoderDecoderModel()\n    MODEL_DIR = 'gs://improved-t5/ckpts/v2_xxl_mlm/'\n    OPTIMIZER = @adafactor.Adafactor()\n    RANDOM_SEED = None\n    SAVING_PERIOD = 10000\n    SHUFFLE_TRAIN_EXAMPLES = True\n    TASK_FEATURE_LENGTHS = {'inputs': 512, 'targets': 114}\n    TRAIN_STEPS = 2000000\n    USE_CACHED_TASKS = False\n    USE_HARDWARE_RNG = False\n    VOCABULARY = @seqio.SentencePieceVocabulary()\n    Z_LOSS = 0.0001\n    \n#### Parameters for adafactor.Adafactor:\n\n    adafactor.Adafactor.decay_rate = 0.8\n    adafactor.Adafactor.logical_factor_rules = \\\n        @adafactor.standard_logical_factor_rules()\n    adafactor.Adafactor.step_offset = 0\n    \n#### Parameters for utils.CheckpointConfig:\n\n    utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()\n    utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()\n    \n#### Parameters for utils.create_learning_rate_scheduler:\n\n    utils.create_learning_rate_scheduler.base_learning_rate = 1.0\n    utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'\n    utils.create_learning_rate_scheduler.warmup_steps = 10000\n    \n#### Parameters for infer_eval/utils.DatasetConfig:\n\n    infer_eval/utils.DatasetConfig.batch_size = %EVAL_BATCH_SIZE\n    infer_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    infer_eval/utils.DatasetConfig.seed = 0\n    infer_eval/utils.DatasetConfig.shuffle = False\n    infer_eval/utils.DatasetConfig.split = 'validation'\n    infer_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    \n#### Parameters for train/utils.DatasetConfig:\n\n    train/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train/utils.DatasetConfig.pack = True\n    train/utils.DatasetConfig.seed = None\n    train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES\n    train/utils.DatasetConfig.split = 'train'\n    train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for train_eval/utils.DatasetConfig:\n\n    train_eval/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train_eval/utils.DatasetConfig.pack = True\n    train_eval/utils.DatasetConfig.seed = 42\n    train_eval/utils.DatasetConfig.shuffle = False\n    train_eval/utils.DatasetConfig.split = 'validation'\n    train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for models.EncoderDecoderModel:\n\n    models.EncoderDecoderModel.input_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.label_smoothing = %LABEL_SMOOTHING\n    models.EncoderDecoderModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR\n    models.EncoderDecoderModel.module = @network.Transformer()\n    models.EncoderDecoderModel.optimizer_def = %OPTIMIZER\n    models.EncoderDecoderModel.output_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.z_loss = %Z_LOSS\n    \n#### Parameters for seqio.Evaluator:\n\n    seqio.Evaluator.logger_cls = \\\n        [@seqio.PyLoggingLogger, @seqio.TensorBoardLogger, @seqio.JSONLogger]\n    seqio.Evaluator.num_examples = 2048\n    \n#### Parameters for partitioning.PjitPartitioner:\n\n    partitioning.PjitPartitioner.logical_axis_rules = \\\n        @partitioning.standard_logical_axis_rules()\n    partitioning.PjitPartitioner.model_parallel_submesh = None\n    partitioning.PjitPartitioner.num_partitions = 4\n    \n#### Parameters for utils.RestoreCheckpointConfig:\n\n    utils.RestoreCheckpointConfig.path = []\n    \n#### Parameters for utils.SaveCheckpointConfig:\n\n    utils.SaveCheckpointConfig.dtype = 'float32'\n    utils.SaveCheckpointConfig.keep = None\n    utils.SaveCheckpointConfig.period = %SAVING_PERIOD\n    utils.SaveCheckpointConfig.save_dataset = False\n    \n#### Parameters for seqio.SentencePieceVocabulary:\n\n    seqio.SentencePieceVocabulary.extra_ids = 100\n    seqio.SentencePieceVocabulary.sentencepiece_model_file = \\\n        'gs://improved-t5/vocabs/tokenizer.model'\n    \n#### Parameters for network.T5Config:\n\n    network.T5Config.dropout_rate = %DROPOUT_RATE\n    network.T5Config.dtype = 'bfloat16'\n    network.T5Config.emb_dim = 4096\n    network.T5Config.head_dim = 64\n    network.T5Config.logits_via_embedding = False\n    network.T5Config.mlp_activations = ('gelu', 'linear')\n    network.T5Config.mlp_dim = 10240\n    network.T5Config.num_decoder_layers = 24\n    network.T5Config.num_encoder_layers = 24\n    network.T5Config.num_heads = 64\n    network.T5Config.remat_policy = 'minimal'\n    network.T5Config.scan_layers = True\n    network.T5Config.vocab_size = 32128\n    \n#### Parameters for train_script.train:\n\n    train_script.train.checkpoint_cfg = @utils.CheckpointConfig()\n    train_script.train.eval_period = %SAVING_PERIOD\n    train_script.train.eval_steps = 100\n    train_script.train.infer_eval_dataset_cfg = @infer_eval/utils.DatasetConfig()\n    train_script.train.inference_evaluator_cls = @seqio.Evaluator\n    train_script.train.model = %MODEL\n    train_script.train.model_dir = %MODEL_DIR\n    train_script.train.partitioner = @partitioning.PjitPartitioner()\n    train_script.train.random_seed = %RANDOM_SEED\n    train_script.train.run_eval_before_training = True\n    train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config\n    train_script.train.total_steps = %TRAIN_STEPS\n    train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()\n    train_script.train.train_eval_dataset_cfg = @train_eval/utils.DatasetConfig()\n    train_script.train.trainer_cls = @trainer.Trainer\n    train_script.train.use_hardware_rng = %USE_HARDWARE_RNG\n    \n#### Parameters for trainer.Trainer:\n\n    trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()\n    trainer.Trainer.num_microbatches = None\n    \n#### Parameters for network.Transformer:\n\n    network.Transformer.config = @network.T5Config()""",138.65668,0.97481775,2.4080276e-05,0.0009470065,0.00094491156,28174.896,373.32697
xxl,1060000,21.997932,-0.9976062,149234.25,-0.7613032,2048.0083,2048.0083,149244.12,-0.76135355,-0.76006836,1.0016909,0.99205077,1.0016909,1.2853837,13981.243,20480000.0,1464.8197,2.860976,0.715244,-140205.52,326.15128,10.108325,-5.156658e-05,1.0341018468041017,"b""    from __gin__ import dynamic_registration\n    import __main__ as train_script\n    import data.pile.tasks\n    from models.scalable_t5 import network\n    import seqio\n    from t5x import adafactor\n    from t5x import gin_utils\n    from t5x import models\n    from t5x import partitioning\n    from t5x import trainer\n    from t5x import utils\n    \n#### Macros:\n\n    BATCH_SIZE = 2048\n    DROPOUT_RATE = 0.0\n    EVAL_BATCH_SIZE = 256\n    LABEL_SMOOTHING = 0.0\n    LOSS_NORMALIZING_FACTOR = None\n    MIXTURE_OR_TASK_MODULE = None\n    MIXTURE_OR_TASK_NAME = 'pile_r_denoiser'\n    MODEL = @models.EncoderDecoderModel()\n    MODEL_DIR = 'gs://improved-t5/ckpts/v2_xxl_mlm/'\n    OPTIMIZER = @adafactor.Adafactor()\n    RANDOM_SEED = None\n    SAVING_PERIOD = 10000\n    SHUFFLE_TRAIN_EXAMPLES = True\n    TASK_FEATURE_LENGTHS = {'inputs': 512, 'targets': 114}\n    TRAIN_STEPS = 2000000\n    USE_CACHED_TASKS = False\n    USE_HARDWARE_RNG = False\n    VOCABULARY = @seqio.SentencePieceVocabulary()\n    Z_LOSS = 0.0001\n    \n#### Parameters for adafactor.Adafactor:\n\n    adafactor.Adafactor.decay_rate = 0.8\n    adafactor.Adafactor.logical_factor_rules = \\\n        @adafactor.standard_logical_factor_rules()\n    adafactor.Adafactor.step_offset = 0\n    \n#### Parameters for utils.CheckpointConfig:\n\n    utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()\n    utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()\n    \n#### Parameters for utils.create_learning_rate_scheduler:\n\n    utils.create_learning_rate_scheduler.base_learning_rate = 1.0\n    utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'\n    utils.create_learning_rate_scheduler.warmup_steps = 10000\n    \n#### Parameters for infer_eval/utils.DatasetConfig:\n\n    infer_eval/utils.DatasetConfig.batch_size = %EVAL_BATCH_SIZE\n    infer_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    infer_eval/utils.DatasetConfig.seed = 0\n    infer_eval/utils.DatasetConfig.shuffle = False\n    infer_eval/utils.DatasetConfig.split = 'validation'\n    infer_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    \n#### Parameters for train/utils.DatasetConfig:\n\n    train/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train/utils.DatasetConfig.pack = True\n    train/utils.DatasetConfig.seed = None\n    train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES\n    train/utils.DatasetConfig.split = 'train'\n    train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for train_eval/utils.DatasetConfig:\n\n    train_eval/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train_eval/utils.DatasetConfig.pack = True\n    train_eval/utils.DatasetConfig.seed = 42\n    train_eval/utils.DatasetConfig.shuffle = False\n    train_eval/utils.DatasetConfig.split = 'validation'\n    train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for models.EncoderDecoderModel:\n\n    models.EncoderDecoderModel.input_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.label_smoothing = %LABEL_SMOOTHING\n    models.EncoderDecoderModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR\n    models.EncoderDecoderModel.module = @network.Transformer()\n    models.EncoderDecoderModel.optimizer_def = %OPTIMIZER\n    models.EncoderDecoderModel.output_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.z_loss = %Z_LOSS\n    \n#### Parameters for seqio.Evaluator:\n\n    seqio.Evaluator.logger_cls = \\\n        [@seqio.PyLoggingLogger, @seqio.TensorBoardLogger, @seqio.JSONLogger]\n    seqio.Evaluator.num_examples = 2048\n    \n#### Parameters for partitioning.PjitPartitioner:\n\n    partitioning.PjitPartitioner.logical_axis_rules = \\\n        @partitioning.standard_logical_axis_rules()\n    partitioning.PjitPartitioner.model_parallel_submesh = None\n    partitioning.PjitPartitioner.num_partitions = 4\n    \n#### Parameters for utils.RestoreCheckpointConfig:\n\n    utils.RestoreCheckpointConfig.path = []\n    \n#### Parameters for utils.SaveCheckpointConfig:\n\n    utils.SaveCheckpointConfig.dtype = 'float32'\n    utils.SaveCheckpointConfig.keep = None\n    utils.SaveCheckpointConfig.period = %SAVING_PERIOD\n    utils.SaveCheckpointConfig.save_dataset = False\n    \n#### Parameters for seqio.SentencePieceVocabulary:\n\n    seqio.SentencePieceVocabulary.extra_ids = 100\n    seqio.SentencePieceVocabulary.sentencepiece_model_file = \\\n        'gs://improved-t5/vocabs/tokenizer.model'\n    \n#### Parameters for network.T5Config:\n\n    network.T5Config.dropout_rate = %DROPOUT_RATE\n    network.T5Config.dtype = 'bfloat16'\n    network.T5Config.emb_dim = 4096\n    network.T5Config.head_dim = 64\n    network.T5Config.logits_via_embedding = False\n    network.T5Config.mlp_activations = ('gelu', 'linear')\n    network.T5Config.mlp_dim = 10240\n    network.T5Config.num_decoder_layers = 24\n    network.T5Config.num_encoder_layers = 24\n    network.T5Config.num_heads = 64\n    network.T5Config.remat_policy = 'minimal'\n    network.T5Config.scan_layers = True\n    network.T5Config.vocab_size = 32128\n    \n#### Parameters for train_script.train:\n\n    train_script.train.checkpoint_cfg = @utils.CheckpointConfig()\n    train_script.train.eval_period = %SAVING_PERIOD\n    train_script.train.eval_steps = 100\n    train_script.train.infer_eval_dataset_cfg = @infer_eval/utils.DatasetConfig()\n    train_script.train.inference_evaluator_cls = @seqio.Evaluator\n    train_script.train.model = %MODEL\n    train_script.train.model_dir = %MODEL_DIR\n    train_script.train.partitioner = @partitioning.PjitPartitioner()\n    train_script.train.random_seed = %RANDOM_SEED\n    train_script.train.run_eval_before_training = True\n    train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config\n    train_script.train.total_steps = %TRAIN_STEPS\n    train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()\n    train_script.train.train_eval_dataset_cfg = @train_eval/utils.DatasetConfig()\n    train_script.train.trainer_cls = @trainer.Trainer\n    train_script.train.use_hardware_rng = %USE_HARDWARE_RNG\n    \n#### Parameters for trainer.Trainer:\n\n    trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()\n    trainer.Trainer.num_microbatches = None\n    \n#### Parameters for network.Transformer:\n\n    network.Transformer.config = @network.T5Config()""",116.141754,0.9821844,1.9311905e-05,0.0009735984,0.0009712863,28548.572,46.38939
xxl,1070000,21.433268,-0.9974235,149514.34,-0.762732,2048.007,2048.007,149524.14,-0.76278204,-0.7614862,1.0017018,0.9919997,1.0017018,1.2872181,13988.118,20480000.0,1464.0997,2.8595698,0.71489245,-140136.6,325.99097,10.171832,-5.1890558e-05,1.0341094924736116,"b""    from __gin__ import dynamic_registration\n    import __main__ as train_script\n    import data.pile.tasks\n    from models.scalable_t5 import network\n    import seqio\n    from t5x import adafactor\n    from t5x import gin_utils\n    from t5x import models\n    from t5x import partitioning\n    from t5x import trainer\n    from t5x import utils\n    \n#### Macros:\n\n    BATCH_SIZE = 2048\n    DROPOUT_RATE = 0.0\n    EVAL_BATCH_SIZE = 256\n    LABEL_SMOOTHING = 0.0\n    LOSS_NORMALIZING_FACTOR = None\n    MIXTURE_OR_TASK_MODULE = None\n    MIXTURE_OR_TASK_NAME = 'pile_r_denoiser'\n    MODEL = @models.EncoderDecoderModel()\n    MODEL_DIR = 'gs://improved-t5/ckpts/v2_xxl_mlm/'\n    OPTIMIZER = @adafactor.Adafactor()\n    RANDOM_SEED = None\n    SAVING_PERIOD = 10000\n    SHUFFLE_TRAIN_EXAMPLES = True\n    TASK_FEATURE_LENGTHS = {'inputs': 512, 'targets': 114}\n    TRAIN_STEPS = 2000000\n    USE_CACHED_TASKS = False\n    USE_HARDWARE_RNG = False\n    VOCABULARY = @seqio.SentencePieceVocabulary()\n    Z_LOSS = 0.0001\n    \n#### Parameters for adafactor.Adafactor:\n\n    adafactor.Adafactor.decay_rate = 0.8\n    adafactor.Adafactor.logical_factor_rules = \\\n        @adafactor.standard_logical_factor_rules()\n    adafactor.Adafactor.step_offset = 0\n    \n#### Parameters for utils.CheckpointConfig:\n\n    utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()\n    utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()\n    \n#### Parameters for utils.create_learning_rate_scheduler:\n\n    utils.create_learning_rate_scheduler.base_learning_rate = 1.0\n    utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'\n    utils.create_learning_rate_scheduler.warmup_steps = 10000\n    \n#### Parameters for infer_eval/utils.DatasetConfig:\n\n    infer_eval/utils.DatasetConfig.batch_size = %EVAL_BATCH_SIZE\n    infer_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    infer_eval/utils.DatasetConfig.seed = 0\n    infer_eval/utils.DatasetConfig.shuffle = False\n    infer_eval/utils.DatasetConfig.split = 'validation'\n    infer_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    \n#### Parameters for train/utils.DatasetConfig:\n\n    train/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train/utils.DatasetConfig.pack = True\n    train/utils.DatasetConfig.seed = None\n    train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES\n    train/utils.DatasetConfig.split = 'train'\n    train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for train_eval/utils.DatasetConfig:\n\n    train_eval/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train_eval/utils.DatasetConfig.pack = True\n    train_eval/utils.DatasetConfig.seed = 42\n    train_eval/utils.DatasetConfig.shuffle = False\n    train_eval/utils.DatasetConfig.split = 'validation'\n    train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for models.EncoderDecoderModel:\n\n    models.EncoderDecoderModel.input_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.label_smoothing = %LABEL_SMOOTHING\n    models.EncoderDecoderModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR\n    models.EncoderDecoderModel.module = @network.Transformer()\n    models.EncoderDecoderModel.optimizer_def = %OPTIMIZER\n    models.EncoderDecoderModel.output_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.z_loss = %Z_LOSS\n    \n#### Parameters for seqio.Evaluator:\n\n    seqio.Evaluator.logger_cls = \\\n        [@seqio.PyLoggingLogger, @seqio.TensorBoardLogger, @seqio.JSONLogger]\n    seqio.Evaluator.num_examples = 2048\n    \n#### Parameters for partitioning.PjitPartitioner:\n\n    partitioning.PjitPartitioner.logical_axis_rules = \\\n        @partitioning.standard_logical_axis_rules()\n    partitioning.PjitPartitioner.model_parallel_submesh = None\n    partitioning.PjitPartitioner.num_partitions = 4\n    \n#### Parameters for utils.RestoreCheckpointConfig:\n\n    utils.RestoreCheckpointConfig.path = []\n    \n#### Parameters for utils.SaveCheckpointConfig:\n\n    utils.SaveCheckpointConfig.dtype = 'float32'\n    utils.SaveCheckpointConfig.keep = None\n    utils.SaveCheckpointConfig.period = %SAVING_PERIOD\n    utils.SaveCheckpointConfig.save_dataset = False\n    \n#### Parameters for seqio.SentencePieceVocabulary:\n\n    seqio.SentencePieceVocabulary.extra_ids = 100\n    seqio.SentencePieceVocabulary.sentencepiece_model_file = \\\n        'gs://improved-t5/vocabs/tokenizer.model'\n    \n#### Parameters for network.T5Config:\n\n    network.T5Config.dropout_rate = %DROPOUT_RATE\n    network.T5Config.dtype = 'bfloat16'\n    network.T5Config.emb_dim = 4096\n    network.T5Config.head_dim = 64\n    network.T5Config.logits_via_embedding = False\n    network.T5Config.mlp_activations = ('gelu', 'linear')\n    network.T5Config.mlp_dim = 10240\n    network.T5Config.num_decoder_layers = 24\n    network.T5Config.num_encoder_layers = 24\n    network.T5Config.num_heads = 64\n    network.T5Config.remat_policy = 'minimal'\n    network.T5Config.scan_layers = True\n    network.T5Config.vocab_size = 32128\n    \n#### Parameters for train_script.train:\n\n    train_script.train.checkpoint_cfg = @utils.CheckpointConfig()\n    train_script.train.eval_period = %SAVING_PERIOD\n    train_script.train.eval_steps = 100\n    train_script.train.infer_eval_dataset_cfg = @infer_eval/utils.DatasetConfig()\n    train_script.train.inference_evaluator_cls = @seqio.Evaluator\n    train_script.train.model = %MODEL\n    train_script.train.model_dir = %MODEL_DIR\n    train_script.train.partitioner = @partitioning.PjitPartitioner()\n    train_script.train.random_seed = %RANDOM_SEED\n    train_script.train.run_eval_before_training = True\n    train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config\n    train_script.train.total_steps = %TRAIN_STEPS\n    train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()\n    train_script.train.train_eval_dataset_cfg = @train_eval/utils.DatasetConfig()\n    train_script.train.trainer_cls = @trainer.Trainer\n    train_script.train.use_hardware_rng = %USE_HARDWARE_RNG\n    \n#### Parameters for trainer.Trainer:\n\n    trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()\n    trainer.Trainer.num_microbatches = None\n    \n#### Parameters for network.Transformer:\n\n    network.Transformer.config = @network.T5Config()""",116.76895,5.820637,1.2397766e-05,0.0009689927,0.000966737,14135.024,47.029762
xxl,1040000,21.44356,-0.99778366,149195.77,-0.76110685,2048.006,2048.006,149205.95,-0.7611588,-0.7598737,1.0016912,0.9920493,1.0016912,1.2854366,13984.009,20480000.0,1464.5299,2.86041,0.7151025,-140177.78,326.08673,10.068562,-5.136373e-05,1.034115626983735,"b""    from __gin__ import dynamic_registration\n    import __main__ as train_script\n    import data.pile.tasks\n    from models.scalable_t5 import network\n    import seqio\n    from t5x import adafactor\n    from t5x import gin_utils\n    from t5x import models\n    from t5x import partitioning\n    from t5x import trainer\n    from t5x import utils\n    \n#### Macros:\n\n    BATCH_SIZE = 2048\n    DROPOUT_RATE = 0.0\n    EVAL_BATCH_SIZE = 256\n    LABEL_SMOOTHING = 0.0\n    LOSS_NORMALIZING_FACTOR = None\n    MIXTURE_OR_TASK_MODULE = None\n    MIXTURE_OR_TASK_NAME = 'pile_r_denoiser'\n    MODEL = @models.EncoderDecoderModel()\n    MODEL_DIR = 'gs://improved-t5/ckpts/v2_xxl_mlm/'\n    OPTIMIZER = @adafactor.Adafactor()\n    RANDOM_SEED = None\n    SAVING_PERIOD = 10000\n    SHUFFLE_TRAIN_EXAMPLES = True\n    TASK_FEATURE_LENGTHS = {'inputs': 512, 'targets': 114}\n    TRAIN_STEPS = 2000000\n    USE_CACHED_TASKS = False\n    USE_HARDWARE_RNG = False\n    VOCABULARY = @seqio.SentencePieceVocabulary()\n    Z_LOSS = 0.0001\n    \n#### Parameters for adafactor.Adafactor:\n\n    adafactor.Adafactor.decay_rate = 0.8\n    adafactor.Adafactor.logical_factor_rules = \\\n        @adafactor.standard_logical_factor_rules()\n    adafactor.Adafactor.step_offset = 0\n    \n#### Parameters for utils.CheckpointConfig:\n\n    utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()\n    utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()\n    \n#### Parameters for utils.create_learning_rate_scheduler:\n\n    utils.create_learning_rate_scheduler.base_learning_rate = 1.0\n    utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'\n    utils.create_learning_rate_scheduler.warmup_steps = 10000\n    \n#### Parameters for infer_eval/utils.DatasetConfig:\n\n    infer_eval/utils.DatasetConfig.batch_size = %EVAL_BATCH_SIZE\n    infer_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    infer_eval/utils.DatasetConfig.seed = 0\n    infer_eval/utils.DatasetConfig.shuffle = False\n    infer_eval/utils.DatasetConfig.split = 'validation'\n    infer_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    \n#### Parameters for train/utils.DatasetConfig:\n\n    train/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train/utils.DatasetConfig.pack = True\n    train/utils.DatasetConfig.seed = None\n    train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES\n    train/utils.DatasetConfig.split = 'train'\n    train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for train_eval/utils.DatasetConfig:\n\n    train_eval/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train_eval/utils.DatasetConfig.pack = True\n    train_eval/utils.DatasetConfig.seed = 42\n    train_eval/utils.DatasetConfig.shuffle = False\n    train_eval/utils.DatasetConfig.split = 'validation'\n    train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for models.EncoderDecoderModel:\n\n    models.EncoderDecoderModel.input_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.label_smoothing = %LABEL_SMOOTHING\n    models.EncoderDecoderModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR\n    models.EncoderDecoderModel.module = @network.Transformer()\n    models.EncoderDecoderModel.optimizer_def = %OPTIMIZER\n    models.EncoderDecoderModel.output_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.z_loss = %Z_LOSS\n    \n#### Parameters for seqio.Evaluator:\n\n    seqio.Evaluator.logger_cls = \\\n        [@seqio.PyLoggingLogger, @seqio.TensorBoardLogger, @seqio.JSONLogger]\n    seqio.Evaluator.num_examples = 2048\n    \n#### Parameters for partitioning.PjitPartitioner:\n\n    partitioning.PjitPartitioner.logical_axis_rules = \\\n        @partitioning.standard_logical_axis_rules()\n    partitioning.PjitPartitioner.model_parallel_submesh = None\n    partitioning.PjitPartitioner.num_partitions = 4\n    \n#### Parameters for utils.RestoreCheckpointConfig:\n\n    utils.RestoreCheckpointConfig.path = []\n    \n#### Parameters for utils.SaveCheckpointConfig:\n\n    utils.SaveCheckpointConfig.dtype = 'float32'\n    utils.SaveCheckpointConfig.keep = None\n    utils.SaveCheckpointConfig.period = %SAVING_PERIOD\n    utils.SaveCheckpointConfig.save_dataset = False\n    \n#### Parameters for seqio.SentencePieceVocabulary:\n\n    seqio.SentencePieceVocabulary.extra_ids = 100\n    seqio.SentencePieceVocabulary.sentencepiece_model_file = \\\n        'gs://improved-t5/vocabs/tokenizer.model'\n    \n#### Parameters for network.T5Config:\n\n    network.T5Config.dropout_rate = %DROPOUT_RATE\n    network.T5Config.dtype = 'bfloat16'\n    network.T5Config.emb_dim = 4096\n    network.T5Config.head_dim = 64\n    network.T5Config.logits_via_embedding = False\n    network.T5Config.mlp_activations = ('gelu', 'linear')\n    network.T5Config.mlp_dim = 10240\n    network.T5Config.num_decoder_layers = 24\n    network.T5Config.num_encoder_layers = 24\n    network.T5Config.num_heads = 64\n    network.T5Config.remat_policy = 'minimal'\n    network.T5Config.scan_layers = True\n    network.T5Config.vocab_size = 32128\n    \n#### Parameters for train_script.train:\n\n    train_script.train.checkpoint_cfg = @utils.CheckpointConfig()\n    train_script.train.eval_period = %SAVING_PERIOD\n    train_script.train.eval_steps = 100\n    train_script.train.infer_eval_dataset_cfg = @infer_eval/utils.DatasetConfig()\n    train_script.train.inference_evaluator_cls = @seqio.Evaluator\n    train_script.train.model = %MODEL\n    train_script.train.model_dir = %MODEL_DIR\n    train_script.train.partitioner = @partitioning.PjitPartitioner()\n    train_script.train.random_seed = %RANDOM_SEED\n    train_script.train.run_eval_before_training = True\n    train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config\n    train_script.train.total_steps = %TRAIN_STEPS\n    train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()\n    train_script.train.train_eval_dataset_cfg = @train_eval/utils.DatasetConfig()\n    train_script.train.trainer_cls = @trainer.Trainer\n    train_script.train.use_hardware_rng = %USE_HARDWARE_RNG\n    \n#### Parameters for trainer.Trainer:\n\n    trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()\n    trainer.Trainer.num_microbatches = None\n    \n#### Parameters for network.Transformer:\n\n    network.Transformer.config = @network.T5Config()""",121.85864,0.9854584,1.4305115e-05,0.0009829515,0.0009805812,56671.59,47.610752
xxl,1050000,,-0.9968485,150140.17,-0.76592463,2048.0059,2048.0059,150150.16,-0.7659756,-0.7646819,1.0016918,0.9920467,1.0016918,1.2855322,13984.014,20480000.0,1464.5295,2.8604093,0.7151023,-140177.73,326.08664,9.862128,-5.0310635e-05,1.0341142922638986,,,0.9797206,,0.0009782402,0.0009759005,70739.0,373.46848
xxl,90000,51.239674,-0.9587898,190344.08,-0.97102076,2048.0066,2048.0066,190419.17,-0.97140384,-0.96977085,1.001684,0.99208355,1.001684,1.2842048,24461.295,20480000.0,837.241,3.2704728,0.4088091,-80136.695,372.8339,74.9687,-0.0003824451,1.0437082797327064,"b""    from __gin__ import dynamic_registration\n    import __main__ as train_script\n    import data.pile.tasks\n    from models.scalable_t5 import network\n    import seqio\n    from t5x import adafactor\n    from t5x import gin_utils\n    from t5x import models\n    from t5x import partitioning\n    from t5x import trainer\n    from t5x import utils\n    \n#### Macros:\n\n    BATCH_SIZE = 2048\n    DROPOUT_RATE = 0.0\n    EVAL_BATCH_SIZE = 256\n    LABEL_SMOOTHING = 0.0\n    LOSS_NORMALIZING_FACTOR = None\n    MIXTURE_OR_TASK_MODULE = None\n    MIXTURE_OR_TASK_NAME = 'pile_r_denoiser'\n    MODEL = @models.EncoderDecoderModel()\n    MODEL_DIR = 'gs://improved-t5/ckpts/v2_xxl_mlm/'\n    OPTIMIZER = @adafactor.Adafactor()\n    RANDOM_SEED = None\n    SAVING_PERIOD = 10000\n    SHUFFLE_TRAIN_EXAMPLES = True\n    TASK_FEATURE_LENGTHS = {'inputs': 512, 'targets': 114}\n    TRAIN_STEPS = 1000000\n    USE_CACHED_TASKS = False\n    USE_HARDWARE_RNG = False\n    VOCABULARY = @seqio.SentencePieceVocabulary()\n    Z_LOSS = 0.0001\n    \n#### Parameters for adafactor.Adafactor:\n\n    adafactor.Adafactor.decay_rate = 0.8\n    adafactor.Adafactor.logical_factor_rules = \\\n        @adafactor.standard_logical_factor_rules()\n    adafactor.Adafactor.step_offset = 0\n    \n#### Parameters for utils.CheckpointConfig:\n\n    utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()\n    utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()\n    \n#### Parameters for utils.create_learning_rate_scheduler:\n\n    utils.create_learning_rate_scheduler.base_learning_rate = 1.0\n    utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'\n    utils.create_learning_rate_scheduler.warmup_steps = 10000\n    \n#### Parameters for infer_eval/utils.DatasetConfig:\n\n    infer_eval/utils.DatasetConfig.batch_size = %EVAL_BATCH_SIZE\n    infer_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    infer_eval/utils.DatasetConfig.seed = 0\n    infer_eval/utils.DatasetConfig.shuffle = False\n    infer_eval/utils.DatasetConfig.split = 'validation'\n    infer_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    \n#### Parameters for train/utils.DatasetConfig:\n\n    train/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train/utils.DatasetConfig.pack = True\n    train/utils.DatasetConfig.seed = None\n    train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES\n    train/utils.DatasetConfig.split = 'train'\n    train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for train_eval/utils.DatasetConfig:\n\n    train_eval/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train_eval/utils.DatasetConfig.pack = True\n    train_eval/utils.DatasetConfig.seed = 42\n    train_eval/utils.DatasetConfig.shuffle = False\n    train_eval/utils.DatasetConfig.split = 'validation'\n    train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for models.EncoderDecoderModel:\n\n    models.EncoderDecoderModel.input_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.label_smoothing = %LABEL_SMOOTHING\n    models.EncoderDecoderModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR\n    models.EncoderDecoderModel.module = @network.Transformer()\n    models.EncoderDecoderModel.optimizer_def = %OPTIMIZER\n    models.EncoderDecoderModel.output_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.z_loss = %Z_LOSS\n    \n#### Parameters for seqio.Evaluator:\n\n    seqio.Evaluator.logger_cls = \\\n        [@seqio.PyLoggingLogger, @seqio.TensorBoardLogger, @seqio.JSONLogger]\n    seqio.Evaluator.num_examples = 2048\n    \n#### Parameters for partitioning.PjitPartitioner:\n\n    partitioning.PjitPartitioner.logical_axis_rules = \\\n        @partitioning.standard_logical_axis_rules()\n    partitioning.PjitPartitioner.model_parallel_submesh = None\n    partitioning.PjitPartitioner.num_partitions = 1\n    \n#### Parameters for utils.RestoreCheckpointConfig:\n\n    utils.RestoreCheckpointConfig.path = []\n    \n#### Parameters for utils.SaveCheckpointConfig:\n\n    utils.SaveCheckpointConfig.dtype = 'float32'\n    utils.SaveCheckpointConfig.keep = None\n    utils.SaveCheckpointConfig.period = %SAVING_PERIOD\n    utils.SaveCheckpointConfig.save_dataset = False\n    \n#### Parameters for seqio.SentencePieceVocabulary:\n\n    seqio.SentencePieceVocabulary.extra_ids = 100\n    seqio.SentencePieceVocabulary.sentencepiece_model_file = \\\n        'gs://improved-t5/vocabs/tokenizer.model'\n    \n#### Parameters for partitioning.standard_logical_axis_rules:\n\n    partitioning.standard_logical_axis_rules.activation_partitioning_dims = 1\n    partitioning.standard_logical_axis_rules.parameter_partitioning_dims = 2\n    \n#### Parameters for network.T5Config:\n\n    network.T5Config.dropout_rate = %DROPOUT_RATE\n    network.T5Config.dtype = 'bfloat16'\n    network.T5Config.emb_dim = 4096\n    network.T5Config.head_dim = 64\n    network.T5Config.logits_via_embedding = False\n    network.T5Config.mlp_activations = ('gelu', 'linear')\n    network.T5Config.mlp_dim = 10240\n    network.T5Config.num_decoder_layers = 24\n    network.T5Config.num_encoder_layers = 24\n    network.T5Config.num_heads = 64\n    network.T5Config.remat_policy = 'minimal'\n    network.T5Config.scan_layers = True\n    network.T5Config.vocab_size = 32128\n    \n#### Parameters for train_script.train:\n\n    train_script.train.checkpoint_cfg = @utils.CheckpointConfig()\n    train_script.train.eval_period = %SAVING_PERIOD\n    train_script.train.eval_steps = 100\n    train_script.train.infer_eval_dataset_cfg = @infer_eval/utils.DatasetConfig()\n    train_script.train.inference_evaluator_cls = @seqio.Evaluator\n    train_script.train.model = %MODEL\n    train_script.train.model_dir = %MODEL_DIR\n    train_script.train.partitioner = @partitioning.PjitPartitioner()\n    train_script.train.random_seed = %RANDOM_SEED\n    train_script.train.run_eval_before_training = True\n    train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config\n    train_script.train.total_steps = %TRAIN_STEPS\n    train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()\n    train_script.train.train_eval_dataset_cfg = @train_eval/utils.DatasetConfig()\n    train_script.train.trainer_cls = @trainer.Trainer\n    train_script.train.use_hardware_rng = %USE_HARDWARE_RNG\n    \n#### Parameters for trainer.Trainer:\n\n    trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()\n    trainer.Trainer.num_microbatches = None\n    \n#### Parameters for network.Transformer:\n\n    network.Transformer.config = @network.T5Config()""",4.1540213,23.538275,2.2888184e-05,0.0034314673,0.0033333518,147359.75,
xxl,100000,,-0.96070117,188330.25,-0.9607474,2048.0066,2048.0066,188400.92,-0.96110797,-0.95948523,1.0016913,0.9920487,1.0016913,1.2854577,24442.785,20480000.0,837.87506,3.2729495,0.40911868,-80197.38,373.11624,70.99573,-0.0003621774,1.0432444828279424,,,4.1082444,,0.0032455528,0.0031622935,24610.973,19.192827
xxl,110000,,-0.9625803,186133.25,-0.9495396,2048.0073,2048.0073,186200.34,-0.9498819,-0.94827396,1.0016956,0.9920284,1.0016956,1.2861854,24439.287,20480000.0,837.99493,3.2734177,0.4091772,-80208.86,373.16962,67.43558,-0.0003440157,1.0425248470858353,,,4.1019278,,0.0030869492,0.0030151273,49130.22,20.282986
xxl,720000,,-0.99642277,150494.11,-0.7677302,2048.0088,2048.0088,150505.78,-0.7677898,-0.76648843,1.0016978,0.9920187,1.0016978,1.286536,24047.64,20480000.0,851.64276,3.3267295,0.4158412,-81515.16,379.2472,11.316715,-5.773106e-05,1.034379544662251,,,4.105633,,0.0011826245,0.0011785121,24264.729,61.62477
xxl,730000,,-0.9961285,150716.66,-0.7688655,2048.007,2048.007,150727.69,-0.76892185,-0.76761824,1.0016983,0.99201655,1.0016983,1.2866127,24045.232,20480000.0,851.7281,3.3270628,0.41588286,-81523.33,379.28516,11.227276,-5.7274796e-05,1.0343737577075298,,,4.0853643,,0.0011744511,0.0011704123,48431.027,15.921994
xxl,740000,,-0.9966191,150305.78,-0.7667695,2048.0076,2048.0076,150316.73,-0.7668254,-0.76552993,1.0016922,0.99204415,1.0016922,1.285619,24045.328,20480000.0,851.7247,3.3270495,0.4158812,-81523.0,379.28363,11.179181,-5.7029443e-05,1.0343698132018309,,,4.078477,,0.0011664189,0.0011624771,72551.766,373.7642
xxl,750000,,-0.9967315,150247.33,-0.7664713,2048.0068,2048.0068,150258.17,-0.76652664,-0.7652286,1.0016962,0.9920262,1.0016962,1.2862672,24045.477,20480000.0,851.7195,3.3270292,0.41587865,-81522.5,379.2813,11.042065,-5.6329958e-05,1.0343664496897735,,,4.0716505,,0.0011585751,0.0011547012,97030.47,16.613077
xxl,760000,,-0.9956458,151350.44,-0.7720987,2048.0068,2048.0068,151361.3,-0.77215415,-0.7708467,1.001696,0.99202704,1.001696,1.2862376,24045.035,20480000.0,851.7351,3.3270903,0.41588628,-81523.99,379.2883,10.760776,-5.489499e-05,1.0343396450092208,,,4.0753126,,0.0011508791,0.0011470794,121151.52,374.16476
xxl,780000,,-0.99733454,149565.11,-0.762991,2048.0068,2048.0068,149576.28,-0.763048,-0.7617642,1.0016853,0.99207723,1.0016853,1.2844346,23712.85,20480000.0,863.66675,3.3736982,0.42171228,-82666.04,384.6016,10.862525,-5.5414053e-05,1.0343210736499757,,,4.0904956,,0.0011359173,0.0011322778,23915.008,34.314274
xxl,360000,,-0.987704,159380.11,-0.81306124,2048.0068,2048.0068,159408.7,-0.8132071,-0.81184,1.001684,0.9920834,1.001684,1.2842116,24471.629,20480000.0,836.8875,3.2690918,0.40863648,-80102.85,372.67648,28.85426,-0.00014719703,1.0367346115920053,,,4.071472,,0.001678393,0.001666669,24636.518,22.658016
xxl,370000,,-0.98754746,159454.88,-0.8134427,2048.0068,2048.0068,159483.55,-0.813589,-0.81221896,1.0016868,0.99207056,1.0016868,1.2846752,24470.25,20480000.0,836.9347,3.2692761,0.40865952,-80107.37,372.69748,28.319206,-0.00014446751,1.0365850853346574,,,4.089567,,0.0016552461,0.001643992,49189.83,18.636318
xxl,380000,,-0.988128,158892.67,-0.81057465,2048.0068,2048.0068,158920.08,-0.8107144,-0.80934244,1.0016952,0.9920311,1.0016952,1.2860942,24469.812,20480000.0,836.9496,3.2693343,0.4086668,-80108.8,372.7041,27.732937,-0.00014147673,1.0365081327346035,,,4.0998497,,0.0016330457,0.0016222164,73738.81,18.258366
xxl,390000,,-0.9885691,158498.05,-0.8085615,2048.0063,2048.0063,158524.95,-0.8086988,-0.8073408,1.0016819,0.99209285,1.0016819,1.2838738,24470.107,20480000.0,836.9395,3.269295,0.40866187,-80107.836,372.69962,27.210625,-0.0001388122,1.0364096741297055,,,4.130408,,0.001611688,0.0016012837,98287.76,22.199776
xxl,400000,,-0.9884571,158613.5,-0.80915046,2048.0076,2048.0076,158640.52,-0.8092883,-0.80792147,1.0016918,0.9920466,1.0016918,1.2855346,24469.855,20480000.0,836.9482,3.2693288,0.4086661,-80108.664,372.70346,26.72206,-0.00013631984,1.0363269263115955,,,4.0958805,,0.0015911432,0.0015811408,122840.38,18.940815
xxl,410000,,-0.98915905,157907.47,-0.8055487,2048.0066,2048.0066,157933.75,-0.80568284,-0.8043287,1.0016836,0.9920853,1.0016836,1.2841454,24470.152,20480000.0,836.9379,3.2692888,0.4086611,-80107.68,372.69894,26.251945,-0.00013392161,1.0361704974384711,,,4.4397306,,0.0015713765,0.0015617395,147389.92,21.796476
xxl,420000,,-0.98946136,157609.0,-0.8040262,2048.0068,2048.0068,157635.3,-0.8041603,-0.8028041,1.0016893,0.9920589,1.0016893,1.2850965,24470.084,20480000.0,836.9403,3.269298,0.40866226,-80107.914,372.69998,25.785633,-0.00013154275,1.0361172695396577,,,4.0830393,,0.0015523254,0.0015430354,171942.62,20.347963
xxl,430000,,-0.9898093,157219.97,-0.8020415,2048.0068,2048.0068,157245.47,-0.80217165,-0.80081993,1.0016879,0.99206555,1.0016879,1.2848566,24469.945,20480000.0,836.9451,3.269317,0.4086646,-80108.37,372.70212,25.397755,-0.00012956404,1.0360034586256268,,,4.105482,,0.0015339577,0.0015249875,196493.45,18.734283
xxl,440000,,-0.98990506,157099.8,-0.8014285,2048.0063,2048.0063,157124.44,-0.8015542,-0.80020607,1.0016847,0.9920799,1.0016847,1.284337,24471.088,20480000.0,836.906,3.269164,0.4086455,-80104.625,372.6847,25.020447,-0.00012763924,1.0359489063143208,,,4.081176,,0.0015162186,0.0015075585,221043.83,22.413239
xxl,450000,,-0.990502,156525.47,-0.79849863,2048.0076,2048.0076,156549.48,-0.7986211,-0.79728216,1.0016794,0.9921051,1.0016794,1.283434,24469.277,20480000.0,836.9679,3.2694058,0.40867573,-80110.55,372.71228,24.682499,-0.00012591523,1.0358542033955753,,,4.0970545,,0.0014990865,0.0014907136,245595.95,17.71618
xxl,460000,,-0.9909473,156030.92,-0.79597574,2048.008,2048.008,156054.06,-0.79609376,-0.7947588,1.0016798,0.9921031,1.0016798,1.2835038,24470.117,20480000.0,836.9392,3.2692938,0.40866172,-80107.805,372.6995,23.919779,-0.0001220243,1.0357754037131681,,,4.085209,,0.0014825218,0.0014744212,270144.3,17.798998
xxl,470000,,-0.9912223,155750.67,-0.794546,2048.0076,2048.0076,155774.31,-0.79466665,-0.7933352,1.0016783,0.9921098,1.0016783,1.2832639,24469.385,20480000.0,836.96423,3.2693915,0.40867394,-80110.2,372.71063,23.594736,-0.00012036613,1.0356906223347258,,,4.0900383,,0.0014665002,0.0014586514,294691.94,17.231441
xxl,480000,,-0.9914761,155457.89,-0.79305243,2048.0063,2048.0063,155480.9,-0.79316986,-0.7918424,1.0016764,0.9921183,1.0016764,1.2829558,24469.906,20480000.0,836.94635,3.2693217,0.4086652,-80108.49,372.70267,23.28363,-0.00011877905,1.0356223017831727,,,4.07182,,0.0014509773,0.0014433772,319239.56,18.235418
xxl,490000,,-0.99203384,154930.08,-0.7903599,2048.0059,2048.0059,154952.56,-0.7904746,-0.7891541,1.0016732,0.992134,1.0016732,1.282398,24469.564,20480000.0,836.9581,3.2693677,0.40867096,-80109.62,372.70792,22.966833,-0.000117162934,1.0355913902679035,,,4.0738716,,0.0014359363,0.0014285729,343787.94,20.475063
xxl,500000,,-0.9908643,156223.0,-0.7969556,2048.006,2048.006,156245.44,-0.79707,-0.7957239,1.0016917,0.99204725,1.0016917,1.2855114,24469.613,20480000.0,836.9564,3.269361,0.40867013,-80109.45,372.70715,22.764969,-0.000116133146,1.0354693650201625,,,4.087844,,0.0014213494,0.001414215,368338.44,17.835506
xxl,510000,,-0.9875234,159759.1,-0.8149946,2048.0076,2048.0076,159781.84,-0.8151107,-0.8136952,1.0017395,0.9918223,1.0017395,1.2935877,24469.592,20480000.0,836.95715,3.2693639,0.40867049,-80109.52,372.7075,22.938128,-0.000117016505,1.0354771565937217,,,4.088049,,0.0014072084,0.0014002814,392886.38,17.605095
xxl,520000,,-0.98859507,158647.48,-0.80932385,2048.0068,2048.0068,158670.27,-0.8094401,-0.8080465,1.0017246,0.9918917,1.0017246,1.2910912,24469.158,20480000.0,836.972,3.2694218,0.40867773,-80110.945,372.71408,22.577065,-0.00011517458,1.0353494850956553,,,4.079114,,0.0013934822,0.0013867519,417433.66,17.683664
xxl,1150000,21.926699,-0.9976261,149302.8,-0.7616529,2048.0063,2048.0063,149313.08,-0.76170534,-0.7604185,1.0016923,0.99204385,1.0016923,1.2856305,13958.978,20480000.0,1467.1561,2.8655393,0.7163848,-140429.14,326.67148,10.067142,-5.1356492e-05,1.0340801332038083,"b""    from __gin__ import dynamic_registration\n    import __main__ as train_script\n    import data.pile.tasks\n    from models.scalable_t5 import network\n    import seqio\n    from t5x import adafactor\n    from t5x import gin_utils\n    from t5x import models\n    from t5x import partitioning\n    from t5x import trainer\n    from t5x import utils\n    \n#### Macros:\n\n    BATCH_SIZE = 2048\n    DROPOUT_RATE = 0.0\n    EVAL_BATCH_SIZE = 256\n    LABEL_SMOOTHING = 0.0\n    LOSS_NORMALIZING_FACTOR = None\n    MIXTURE_OR_TASK_MODULE = None\n    MIXTURE_OR_TASK_NAME = 'pile_r_denoiser'\n    MODEL = @models.EncoderDecoderModel()\n    MODEL_DIR = 'gs://improved-t5/ckpts/v2_xxl_mlm/'\n    OPTIMIZER = @adafactor.Adafactor()\n    RANDOM_SEED = None\n    SAVING_PERIOD = 10000\n    SHUFFLE_TRAIN_EXAMPLES = True\n    TASK_FEATURE_LENGTHS = {'inputs': 512, 'targets': 114}\n    TRAIN_STEPS = 2000000\n    USE_CACHED_TASKS = False\n    USE_HARDWARE_RNG = False\n    VOCABULARY = @seqio.SentencePieceVocabulary()\n    Z_LOSS = 0.0001\n    \n#### Parameters for adafactor.Adafactor:\n\n    adafactor.Adafactor.decay_rate = 0.8\n    adafactor.Adafactor.logical_factor_rules = \\\n        @adafactor.standard_logical_factor_rules()\n    adafactor.Adafactor.step_offset = 0\n    \n#### Parameters for utils.CheckpointConfig:\n\n    utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()\n    utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()\n    \n#### Parameters for utils.create_learning_rate_scheduler:\n\n    utils.create_learning_rate_scheduler.base_learning_rate = 1.0\n    utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'\n    utils.create_learning_rate_scheduler.warmup_steps = 10000\n    \n#### Parameters for infer_eval/utils.DatasetConfig:\n\n    infer_eval/utils.DatasetConfig.batch_size = %EVAL_BATCH_SIZE\n    infer_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    infer_eval/utils.DatasetConfig.seed = 0\n    infer_eval/utils.DatasetConfig.shuffle = False\n    infer_eval/utils.DatasetConfig.split = 'validation'\n    infer_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    \n#### Parameters for train/utils.DatasetConfig:\n\n    train/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train/utils.DatasetConfig.pack = True\n    train/utils.DatasetConfig.seed = None\n    train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES\n    train/utils.DatasetConfig.split = 'train'\n    train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for train_eval/utils.DatasetConfig:\n\n    train_eval/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train_eval/utils.DatasetConfig.pack = True\n    train_eval/utils.DatasetConfig.seed = 42\n    train_eval/utils.DatasetConfig.shuffle = False\n    train_eval/utils.DatasetConfig.split = 'validation'\n    train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for models.EncoderDecoderModel:\n\n    models.EncoderDecoderModel.input_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.label_smoothing = %LABEL_SMOOTHING\n    models.EncoderDecoderModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR\n    models.EncoderDecoderModel.module = @network.Transformer()\n    models.EncoderDecoderModel.optimizer_def = %OPTIMIZER\n    models.EncoderDecoderModel.output_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.z_loss = %Z_LOSS\n    \n#### Parameters for seqio.Evaluator:\n\n    seqio.Evaluator.logger_cls = \\\n        [@seqio.PyLoggingLogger, @seqio.TensorBoardLogger, @seqio.JSONLogger]\n    seqio.Evaluator.num_examples = 2048\n    \n#### Parameters for partitioning.PjitPartitioner:\n\n    partitioning.PjitPartitioner.logical_axis_rules = \\\n        @partitioning.standard_logical_axis_rules()\n    partitioning.PjitPartitioner.model_parallel_submesh = None\n    partitioning.PjitPartitioner.num_partitions = 4\n    \n#### Parameters for utils.RestoreCheckpointConfig:\n\n    utils.RestoreCheckpointConfig.path = []\n    \n#### Parameters for utils.SaveCheckpointConfig:\n\n    utils.SaveCheckpointConfig.dtype = 'float32'\n    utils.SaveCheckpointConfig.keep = None\n    utils.SaveCheckpointConfig.period = %SAVING_PERIOD\n    utils.SaveCheckpointConfig.save_dataset = False\n    \n#### Parameters for seqio.SentencePieceVocabulary:\n\n    seqio.SentencePieceVocabulary.extra_ids = 100\n    seqio.SentencePieceVocabulary.sentencepiece_model_file = \\\n        'gs://improved-t5/vocabs/tokenizer.model'\n    \n#### Parameters for network.T5Config:\n\n    network.T5Config.dropout_rate = %DROPOUT_RATE\n    network.T5Config.dtype = 'bfloat16'\n    network.T5Config.emb_dim = 4096\n    network.T5Config.head_dim = 64\n    network.T5Config.logits_via_embedding = False\n    network.T5Config.mlp_activations = ('gelu', 'linear')\n    network.T5Config.mlp_dim = 10240\n    network.T5Config.num_decoder_layers = 24\n    network.T5Config.num_encoder_layers = 24\n    network.T5Config.num_heads = 64\n    network.T5Config.remat_policy = 'minimal'\n    network.T5Config.scan_layers = True\n    network.T5Config.vocab_size = 32128\n    \n#### Parameters for train_script.train:\n\n    train_script.train.checkpoint_cfg = @utils.CheckpointConfig()\n    train_script.train.eval_period = %SAVING_PERIOD\n    train_script.train.eval_steps = 100\n    train_script.train.infer_eval_dataset_cfg = @infer_eval/utils.DatasetConfig()\n    train_script.train.inference_evaluator_cls = @seqio.Evaluator\n    train_script.train.model = %MODEL\n    train_script.train.model_dir = %MODEL_DIR\n    train_script.train.partitioner = @partitioning.PjitPartitioner()\n    train_script.train.random_seed = %RANDOM_SEED\n    train_script.train.run_eval_before_training = True\n    train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config\n    train_script.train.total_steps = %TRAIN_STEPS\n    train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()\n    train_script.train.train_eval_dataset_cfg = @train_eval/utils.DatasetConfig()\n    train_script.train.trainer_cls = @trainer.Trainer\n    train_script.train.use_hardware_rng = %USE_HARDWARE_RNG\n    \n#### Parameters for trainer.Trainer:\n\n    trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()\n    trainer.Trainer.num_microbatches = None\n    \n#### Parameters for network.Transformer:\n\n    network.Transformer.config = @network.T5Config()""",115.21844,5.782832,1.50203705e-05,0.0009345193,0.00093250524,42512.23,372.89746
xxl,1160000,,-0.99879587,148066.25,-0.75534475,2048.0076,2048.0076,148076.33,-0.7553961,-0.75412273,1.0016886,0.99206114,1.0016886,1.2850097,13960.408,20480000.0,1467.0057,2.8652456,0.7163114,-140414.75,326.638,10.028328,-5.1158484e-05,1.034058716404305,,,0.97195196,,0.00093050546,0.0009284771,14106.848,48.46454
xxl,1130000,,-0.9975963,149315.48,-0.76171756,2048.0059,2048.0059,149325.53,-0.7617688,-0.7604698,1.0017081,0.9919696,1.0017081,1.2882985,13961.078,20480000.0,1466.9354,2.8651083,0.71627706,-140408.02,326.62234,10.150071,-5.1779545e-05,1.0340841897090043,,,0.9641495,,0.00094282895,0.0009407213,14098.683,53.03597
xxl,1140000,,-0.99735445,149527.31,-0.7627982,2048.007,2048.007,149537.5,-0.76285017,-0.76155317,1.0017031,0.99199355,1.0017031,1.2874398,13958.865,20480000.0,1467.1681,2.8655627,0.71639067,-140430.28,326.67413,10.093375,-5.1490315e-05,1.0340781665194954,,,0.98569727,,0.00093864615,0.0009365862,28146.27,371.3095
xxl,1080000,,-0.99878126,148062.45,-0.7553254,2048.007,2048.007,148072.2,-0.75537515,-0.7540998,1.0016912,0.99205,1.0016912,1.2854145,13989.384,20480000.0,1463.9673,2.859311,0.7148278,-140123.92,325.96146,10.106148,-5.155548e-05,1.034094063152848,,,0.98067904,,0.0009644823,0.0009622509,14143.631,372.0062
xxl,560000,49.46511,-0.9924347,154508.48,-0.7882092,2048.006,2048.006,154529.67,-0.78831726,-0.78698534,1.0016924,0.9920438,1.0016924,1.2856344,24526.422,20480000.0,835.0179,3.2617886,0.40772358,-79923.9,371.8439,21.33334,-0.000108829845,1.0351172258604446,"b""    from __gin__ import dynamic_registration\n    import __main__ as train_script\n    import data.pile.tasks\n    from models.scalable_t5 import network\n    import seqio\n    from t5x import adafactor\n    from t5x import gin_utils\n    from t5x import models\n    from t5x import partitioning\n    from t5x import trainer\n    from t5x import utils\n    \n#### Macros:\n\n    BATCH_SIZE = 2048\n    DROPOUT_RATE = 0.0\n    EVAL_BATCH_SIZE = 256\n    LABEL_SMOOTHING = 0.0\n    LOSS_NORMALIZING_FACTOR = None\n    MIXTURE_OR_TASK_MODULE = None\n    MIXTURE_OR_TASK_NAME = 'pile_r_denoiser'\n    MODEL = @models.EncoderDecoderModel()\n    MODEL_DIR = 'gs://improved-t5/ckpts/v2_xxl_mlm/'\n    OPTIMIZER = @adafactor.Adafactor()\n    RANDOM_SEED = None\n    SAVING_PERIOD = 10000\n    SHUFFLE_TRAIN_EXAMPLES = True\n    TASK_FEATURE_LENGTHS = {'inputs': 512, 'targets': 114}\n    TRAIN_STEPS = 1000000\n    USE_CACHED_TASKS = False\n    USE_HARDWARE_RNG = False\n    VOCABULARY = @seqio.SentencePieceVocabulary()\n    Z_LOSS = 0.0001\n    \n#### Parameters for adafactor.Adafactor:\n\n    adafactor.Adafactor.decay_rate = 0.8\n    adafactor.Adafactor.logical_factor_rules = \\\n        @adafactor.standard_logical_factor_rules()\n    adafactor.Adafactor.step_offset = 0\n    \n#### Parameters for utils.CheckpointConfig:\n\n    utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()\n    utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()\n    \n#### Parameters for utils.create_learning_rate_scheduler:\n\n    utils.create_learning_rate_scheduler.base_learning_rate = 1.0\n    utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'\n    utils.create_learning_rate_scheduler.warmup_steps = 10000\n    \n#### Parameters for infer_eval/utils.DatasetConfig:\n\n    infer_eval/utils.DatasetConfig.batch_size = %EVAL_BATCH_SIZE\n    infer_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    infer_eval/utils.DatasetConfig.seed = 0\n    infer_eval/utils.DatasetConfig.shuffle = False\n    infer_eval/utils.DatasetConfig.split = 'validation'\n    infer_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    \n#### Parameters for train/utils.DatasetConfig:\n\n    train/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train/utils.DatasetConfig.pack = True\n    train/utils.DatasetConfig.seed = None\n    train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES\n    train/utils.DatasetConfig.split = 'train'\n    train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for train_eval/utils.DatasetConfig:\n\n    train_eval/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train_eval/utils.DatasetConfig.pack = True\n    train_eval/utils.DatasetConfig.seed = 42\n    train_eval/utils.DatasetConfig.shuffle = False\n    train_eval/utils.DatasetConfig.split = 'validation'\n    train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for models.EncoderDecoderModel:\n\n    models.EncoderDecoderModel.input_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.label_smoothing = %LABEL_SMOOTHING\n    models.EncoderDecoderModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR\n    models.EncoderDecoderModel.module = @network.Transformer()\n    models.EncoderDecoderModel.optimizer_def = %OPTIMIZER\n    models.EncoderDecoderModel.output_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.z_loss = %Z_LOSS\n    \n#### Parameters for seqio.Evaluator:\n\n    seqio.Evaluator.logger_cls = \\\n        [@seqio.PyLoggingLogger, @seqio.TensorBoardLogger, @seqio.JSONLogger]\n    seqio.Evaluator.num_examples = 2048\n    \n#### Parameters for partitioning.PjitPartitioner:\n\n    partitioning.PjitPartitioner.logical_axis_rules = \\\n        @partitioning.standard_logical_axis_rules()\n    partitioning.PjitPartitioner.model_parallel_submesh = None\n    partitioning.PjitPartitioner.num_partitions = 1\n    \n#### Parameters for utils.RestoreCheckpointConfig:\n\n    utils.RestoreCheckpointConfig.path = []\n    \n#### Parameters for utils.SaveCheckpointConfig:\n\n    utils.SaveCheckpointConfig.dtype = 'float32'\n    utils.SaveCheckpointConfig.keep = None\n    utils.SaveCheckpointConfig.period = %SAVING_PERIOD\n    utils.SaveCheckpointConfig.save_dataset = False\n    \n#### Parameters for seqio.SentencePieceVocabulary:\n\n    seqio.SentencePieceVocabulary.extra_ids = 100\n    seqio.SentencePieceVocabulary.sentencepiece_model_file = \\\n        'gs://improved-t5/vocabs/tokenizer.model'\n    \n#### Parameters for partitioning.standard_logical_axis_rules:\n\n    partitioning.standard_logical_axis_rules.activation_partitioning_dims = 1\n    partitioning.standard_logical_axis_rules.parameter_partitioning_dims = 2\n    \n#### Parameters for network.T5Config:\n\n    network.T5Config.dropout_rate = %DROPOUT_RATE\n    network.T5Config.dtype = 'bfloat16'\n    network.T5Config.emb_dim = 4096\n    network.T5Config.head_dim = 64\n    network.T5Config.logits_via_embedding = False\n    network.T5Config.mlp_activations = ('gelu', 'linear')\n    network.T5Config.mlp_dim = 10240\n    network.T5Config.num_decoder_layers = 24\n    network.T5Config.num_encoder_layers = 24\n    network.T5Config.num_heads = 64\n    network.T5Config.remat_policy = 'minimal'\n    network.T5Config.scan_layers = True\n    network.T5Config.vocab_size = 32128\n    \n#### Parameters for train_script.train:\n\n    train_script.train.checkpoint_cfg = @utils.CheckpointConfig()\n    train_script.train.eval_period = %SAVING_PERIOD\n    train_script.train.eval_steps = 100\n    train_script.train.infer_eval_dataset_cfg = @infer_eval/utils.DatasetConfig()\n    train_script.train.inference_evaluator_cls = @seqio.Evaluator\n    train_script.train.model = %MODEL\n    train_script.train.model_dir = %MODEL_DIR\n    train_script.train.partitioner = @partitioning.PjitPartitioner()\n    train_script.train.random_seed = %RANDOM_SEED\n    train_script.train.run_eval_before_training = True\n    train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config\n    train_script.train.total_steps = %TRAIN_STEPS\n    train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()\n    train_script.train.train_eval_dataset_cfg = @train_eval/utils.DatasetConfig()\n    train_script.train.trainer_cls = @trainer.Trainer\n    train_script.train.use_hardware_rng = %USE_HARDWARE_RNG\n    \n#### Parameters for trainer.Trainer:\n\n    trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()\n    trainer.Trainer.num_microbatches = None\n    \n#### Parameters for network.Transformer:\n\n    network.Transformer.config = @network.T5Config()""",9.297601,23.775208,2.2888184e-05,0.0013423256,0.0013363075,73904.07,
xxl,570000,,-0.99318963,153711.52,-0.7841435,2048.0083,2048.0083,153732.7,-0.78425163,-0.78292704,1.0016918,0.99204564,1.0016918,1.285564,24526.62,20480000.0,835.0111,3.2617621,0.40772027,-79923.25,371.84088,21.202408,-0.000108161905,1.03511356425339,,,4.0826225,,0.0013303938,0.0013245335,24690.883,18.883747
xxl,580000,,-0.9929158,153952.78,-0.7853743,2048.0063,2048.0063,153974.28,-0.78548396,-0.78415424,1.0016958,0.9920276,1.0016958,1.2862155,24522.682,20480000.0,835.1452,3.262286,0.40778574,-79936.086,371.9006,20.962927,-0.00010694021,1.0351088342694774,,,4.0913057,,0.0013187655,0.0013130654,49292.934,27.438929
xxl,590000,,-0.99320304,153671.3,-0.7839383,2048.0073,2048.0073,153691.81,-0.784043,-0.7827155,1.001696,0.9920269,1.001696,1.2862393,24522.613,20480000.0,835.1476,3.2622952,0.4077869,-79936.32,371.90164,20.72693,-0.00010573631,1.0350031920070266,,,4.08237,,0.0013074477,0.0013018901,73903.47,18.431187
xxl,1480000,,-0.998535,148399.02,-0.7570423,2048.008,2048.008,148408.66,-0.7570915,-0.75580806,1.0016981,0.992016,1.0016981,1.2866262,13892.763,20480000.0,1474.1489,2.8791971,0.7197993,-141098.45,328.2285,9.832111,-5.015751e-05,1.0338527823361887,,,0.9619508,,0.00082338724,0.00082199514,14037.512,54.917587
xxl,1490000,,-0.9982758,148610.89,-0.75812316,2048.008,2048.008,148620.6,-0.7581727,-0.75688875,1.0016963,0.9920244,1.0016963,1.2863247,13891.782,20480000.0,1474.2529,2.8794003,0.71985006,-141108.42,328.25162,9.813253,-5.0061306e-05,1.0338205246133745,,,0.9880748,,0.0008206108,0.0008192322,28019.857,48.493126
xxl,1500000,,-0.9986036,148294.47,-0.756509,2048.0078,2048.0078,148304.4,-0.75655967,-0.75527275,1.001704,0.99199,1.001704,1.2875686,13891.875,20480000.0,1474.2432,2.8793812,0.7198453,-141107.47,328.24945,9.809926,-5.004433e-05,1.0338172755740733,,,1.006068,,0.00081786537,0.00081649685,41995.957,44.662758
xxl,1510000,,-0.9984571,148528.47,-0.7577027,2048.0083,2048.0083,148537.98,-0.7577512,-0.7564731,1.0016896,0.9920565,1.0016896,1.2851752,13891.833,20480000.0,1474.2476,2.8793898,0.71984744,-141107.9,328.25043,9.7013855,-4.9490623e-05,1.0338130974432957,,,0.9731653,,0.0008151271,0.00081378873,55968.266,373.12964
xxl,1520000,,-0.9976963,149289.06,-0.7615828,2048.007,2048.007,149298.56,-0.76163125,-0.7603316,1.0017093,0.9919643,1.0017093,1.2884904,13891.81,20480000.0,1474.25,2.8793945,0.71984863,-141108.12,328.25098,9.506992,-4.8498943e-05,1.033803087583372,,,0.9613545,,0.0008124503,0.0008111074,70268.875,44.953304
xxl,1530000,,-0.997929,149114.34,-0.7606915,2048.008,2048.008,149123.52,-0.7607383,-0.75945336,1.0016919,0.99204606,1.0016919,1.2855538,13891.935,20480000.0,1474.2367,2.8793685,0.71984214,-141106.86,328.24802,9.486463,-4.8394217e-05,1.0338082096827772,,,0.97283626,,0.00080978975,0.0008084524,84241.47,47.17335
xxl,1540000,,-0.9980225,149004.81,-0.76013273,2048.0083,2048.0083,149013.81,-0.7601787,-0.7588882,1.0017005,0.992006,1.0017005,1.2869947,13891.858,20480000.0,1474.2448,2.8793843,0.71984607,-141107.62,328.24982,9.483428,-4.8378733e-05,1.0337904826311015,,,0.9682212,,0.00080713385,0.00080582325,98216.18,46.3334
xxl,1550000,,-0.9979614,149064.66,-0.7604381,2048.0066,2048.0066,149073.78,-0.7604846,-0.7591989,1.0016934,0.9920386,1.0016934,1.2858175,13891.83,20480000.0,1474.2479,2.8793905,0.7198476,-141107.94,328.2505,9.455804,-4.8237813e-05,1.0337633363421102,,,0.96706796,,0.0008045286,0.0008032196,112190.03,43.33513
xxl,1560000,,-0.9979993,148973.69,-0.759974,2048.0056,2048.0056,148983.53,-0.7600242,-0.7587367,1.0016968,0.99202245,1.0016968,1.2863983,13891.961,20480000.0,1474.2339,2.879363,0.71984076,-141106.6,328.24738,9.487337,-4.8398677e-05,1.0337590113042532,,,0.9906812,,0.0008019294,0.000800641,126161.0,371.89474
xxl,1570000,,-0.99824667,148761.62,-0.7588921,2048.0059,2048.0059,148771.27,-0.75894135,-0.7576594,1.001692,0.9920454,1.001692,1.285576,13891.8545,20480000.0,1474.2452,2.8793852,0.7198463,-141107.67,328.2499,9.478698,-4.8354603e-05,1.0337427476454104,,,0.9872751,,0.0007993572,0.00079808716,140460.48,53.933594
xxl,1580000,,-0.9984542,148524.22,-0.7576811,2048.0078,2048.0078,148533.42,-0.757728,-0.7564455,1.0016954,0.9920301,1.0016954,1.2861273,13891.836,20480000.0,1474.2471,2.8793888,0.7198472,-141107.86,328.25034,9.456617,-4.824196e-05,1.033750934134623,,,0.9755027,,0.0007968276,0.0007955575,154441.97,47.777843
xxl,1590000,,-0.99852157,148474.84,-0.7574291,2048.0056,2048.0056,148484.27,-0.7574773,-0.7561992,1.0016901,0.9920538,1.0016901,1.2852716,13892.215,20480000.0,1474.2069,2.8793104,0.7198276,-141104.0,328.2414,9.440202,-4.815822e-05,1.0337557241340773,,,0.971864,,0.0007943035,0.00079305185,168417.64,45.22174
xxl,1600000,,-0.99866843,148302.45,-0.7565497,2048.0083,2048.0083,148311.81,-0.7565975,-0.75532126,1.0016897,0.99205583,1.0016897,1.2851957,13891.855,20480000.0,1474.2452,2.8793852,0.7198463,-141107.67,328.2499,9.443844,-4.81768e-05,1.033741142710112,,,0.9618926,,0.00079179695,0.00079056964,182390.36,43.565735
xxl,1610000,,-0.99885035,148119.02,-0.7556139,2048.0076,2048.0076,148128.11,-0.75566036,-0.75438565,1.0016897,0.9920569,1.0016897,1.2851659,13891.792,20480000.0,1474.252,2.8793983,0.7198496,-141108.31,328.2514,9.43251,-4.8118985e-05,1.033746239651985,,,0.9668937,,0.00078933634,0.0007881106,196361.39,372.93472
xxl,30000,50.901543,-0.9183744,235594.2,-1.2018597,2048.008,2048.008,235830.14,-1.2030632,-1.2010299,1.001693,0.99204105,1.001693,1.2857339,24447.408,20480000.0,837.7166,3.2723305,0.40904132,-80182.21,373.0457,234.90869,-0.0011983626,1.0529967036364467,"b""    from __gin__ import dynamic_registration\n    import __main__ as train_script\n    import data.pile.tasks\n    from models.scalable_t5 import network\n    import seqio\n    from t5x import adafactor\n    from t5x import gin_utils\n    from t5x import models\n    from t5x import partitioning\n    from t5x import trainer\n    from t5x import utils\n    \n#### Macros:\n\n    BATCH_SIZE = 2048\n    DROPOUT_RATE = 0.0\n    EVAL_BATCH_SIZE = 256\n    LABEL_SMOOTHING = 0.0\n    LOSS_NORMALIZING_FACTOR = None\n    MIXTURE_OR_TASK_MODULE = None\n    MIXTURE_OR_TASK_NAME = 'pile_r_denoiser'\n    MODEL = @models.EncoderDecoderModel()\n    MODEL_DIR = 'gs://improved-t5/ckpts/v2_xxl_mlm/'\n    OPTIMIZER = @adafactor.Adafactor()\n    RANDOM_SEED = None\n    SAVING_PERIOD = 10000\n    SHUFFLE_TRAIN_EXAMPLES = True\n    TASK_FEATURE_LENGTHS = {'inputs': 512, 'targets': 114}\n    TRAIN_STEPS = 1000000\n    USE_CACHED_TASKS = False\n    USE_HARDWARE_RNG = False\n    VOCABULARY = @seqio.SentencePieceVocabulary()\n    Z_LOSS = 0.0001\n    \n#### Parameters for adafactor.Adafactor:\n\n    adafactor.Adafactor.decay_rate = 0.8\n    adafactor.Adafactor.logical_factor_rules = \\\n        @adafactor.standard_logical_factor_rules()\n    adafactor.Adafactor.step_offset = 0\n    \n#### Parameters for utils.CheckpointConfig:\n\n    utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()\n    utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()\n    \n#### Parameters for utils.create_learning_rate_scheduler:\n\n    utils.create_learning_rate_scheduler.base_learning_rate = 1.0\n    utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'\n    utils.create_learning_rate_scheduler.warmup_steps = 10000\n    \n#### Parameters for infer_eval/utils.DatasetConfig:\n\n    infer_eval/utils.DatasetConfig.batch_size = %EVAL_BATCH_SIZE\n    infer_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    infer_eval/utils.DatasetConfig.seed = 0\n    infer_eval/utils.DatasetConfig.shuffle = False\n    infer_eval/utils.DatasetConfig.split = 'validation'\n    infer_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    \n#### Parameters for train/utils.DatasetConfig:\n\n    train/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train/utils.DatasetConfig.pack = True\n    train/utils.DatasetConfig.seed = None\n    train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES\n    train/utils.DatasetConfig.split = 'train'\n    train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for train_eval/utils.DatasetConfig:\n\n    train_eval/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train_eval/utils.DatasetConfig.pack = True\n    train_eval/utils.DatasetConfig.seed = 42\n    train_eval/utils.DatasetConfig.shuffle = False\n    train_eval/utils.DatasetConfig.split = 'validation'\n    train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for models.EncoderDecoderModel:\n\n    models.EncoderDecoderModel.input_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.label_smoothing = %LABEL_SMOOTHING\n    models.EncoderDecoderModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR\n    models.EncoderDecoderModel.module = @network.Transformer()\n    models.EncoderDecoderModel.optimizer_def = %OPTIMIZER\n    models.EncoderDecoderModel.output_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.z_loss = %Z_LOSS\n    \n#### Parameters for seqio.Evaluator:\n\n    seqio.Evaluator.logger_cls = \\\n        [@seqio.PyLoggingLogger, @seqio.TensorBoardLogger, @seqio.JSONLogger]\n    seqio.Evaluator.num_examples = 2048\n    \n#### Parameters for partitioning.PjitPartitioner:\n\n    partitioning.PjitPartitioner.logical_axis_rules = \\\n        @partitioning.standard_logical_axis_rules()\n    partitioning.PjitPartitioner.model_parallel_submesh = None\n    partitioning.PjitPartitioner.num_partitions = 1\n    \n#### Parameters for utils.RestoreCheckpointConfig:\n\n    utils.RestoreCheckpointConfig.path = []\n    \n#### Parameters for utils.SaveCheckpointConfig:\n\n    utils.SaveCheckpointConfig.dtype = 'float32'\n    utils.SaveCheckpointConfig.keep = None\n    utils.SaveCheckpointConfig.period = %SAVING_PERIOD\n    utils.SaveCheckpointConfig.save_dataset = False\n    \n#### Parameters for seqio.SentencePieceVocabulary:\n\n    seqio.SentencePieceVocabulary.extra_ids = 100\n    seqio.SentencePieceVocabulary.sentencepiece_model_file = \\\n        'gs://improved-t5/vocabs/tokenizer.model'\n    \n#### Parameters for partitioning.standard_logical_axis_rules:\n\n    partitioning.standard_logical_axis_rules.activation_partitioning_dims = 1\n    partitioning.standard_logical_axis_rules.parameter_partitioning_dims = 2\n    \n#### Parameters for network.T5Config:\n\n    network.T5Config.dropout_rate = %DROPOUT_RATE\n    network.T5Config.dtype = 'bfloat16'\n    network.T5Config.emb_dim = 4096\n    network.T5Config.head_dim = 64\n    network.T5Config.logits_via_embedding = False\n    network.T5Config.mlp_activations = ('gelu', 'linear')\n    network.T5Config.mlp_dim = 10240\n    network.T5Config.num_decoder_layers = 24\n    network.T5Config.num_encoder_layers = 24\n    network.T5Config.num_heads = 64\n    network.T5Config.remat_policy = 'minimal'\n    network.T5Config.scan_layers = True\n    network.T5Config.vocab_size = 32128\n    \n#### Parameters for train_script.train:\n\n    train_script.train.checkpoint_cfg = @utils.CheckpointConfig()\n    train_script.train.eval_period = %SAVING_PERIOD\n    train_script.train.eval_steps = 100\n    train_script.train.infer_eval_dataset_cfg = @infer_eval/utils.DatasetConfig()\n    train_script.train.inference_evaluator_cls = @seqio.Evaluator\n    train_script.train.model = %MODEL\n    train_script.train.model_dir = %MODEL_DIR\n    train_script.train.partitioner = @partitioning.PjitPartitioner()\n    train_script.train.random_seed = %RANDOM_SEED\n    train_script.train.run_eval_before_training = True\n    train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config\n    train_script.train.total_steps = %TRAIN_STEPS\n    train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()\n    train_script.train.train_eval_dataset_cfg = @train_eval/utils.DatasetConfig()\n    train_script.train.trainer_cls = @trainer.Trainer\n    train_script.train.use_hardware_rng = %USE_HARDWARE_RNG\n    \n#### Parameters for trainer.Trainer:\n\n    trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()\n    trainer.Trainer.num_microbatches = None\n    \n#### Parameters for network.Transformer:\n\n    network.Transformer.config = @network.T5Config()""",3.6218364,23.17254,1.5735626e-05,0.00635681,0.0057735993,73667.09,
xxl,40000,,-0.93140525,220776.28,-1.1262676,2048.0073,2048.0073,220929.06,-1.127047,-1.1251456,1.0016899,0.9920554,1.0016899,1.2852149,24464.844,20480000.0,837.11957,3.2699983,0.4087498,-80125.07,372.77982,152.64859,-0.0007787211,1.0502829416225297,,,4.09494,,0.005359021,0.0050000623,24631.797,19.556063
xxl,50000,,-0.9397664,211237.67,-1.0776073,2048.006,2048.006,211358.17,-1.078222,-1.0763907,1.0017014,0.992002,1.0017014,1.2871376,24461.309,20480000.0,837.2406,3.270471,0.4088089,-80136.65,372.8337,120.514984,-0.00061479484,1.0482180168325492,,,4.094514,,0.004721385,0.0044721807,49198.766,18.171894
xxl,60000,,-0.94641685,203868.53,-1.0400144,2048.0076,2048.0076,203970.02,-1.0405321,-1.0387739,1.0016925,0.992043,1.0016925,1.2856603,24461.33,20480000.0,837.23987,3.2704682,0.40880853,-80136.58,372.83337,101.54746,-0.000518034,1.046779976452468,,,4.0925255,,0.0042684553,0.004082517,73738.84,18.61602
xxl,70000,,-0.95121753,198668.1,-1.0134848,2048.0068,2048.0068,198759.94,-1.0139533,-1.012245,1.0016876,0.99206585,1.0016876,1.2848407,24461.342,20480000.0,837.2395,3.2704668,0.40880835,-80136.55,372.8332,91.74048,-0.00046800464,1.045611911450959,,,4.0935574,,0.003925243,0.0037796716,98279.484,18.660067
xxl,80000,,-0.954963,194524.52,-0.9923468,2048.0059,2048.0059,194608.72,-0.99277633,-0.99110234,1.0016891,0.99206,1.0016891,1.2850552,24461.324,20480000.0,837.24,3.2704687,0.4088086,-80136.59,372.83344,84.415794,-0.00043063852,1.0445208775608121,,,4.0989084,,0.0036535263,0.003535556,122820.1,17.73143
xxl,830000,,-0.99787265,148995.77,-0.76008666,2048.0078,2048.0078,149006.27,-0.7601402,-0.7588631,1.0016829,0.99208856,1.0016829,1.2840291,23888.17,20480000.0,857.3282,3.3489382,0.41861728,-82059.336,381.77896,10.625129,-5.4203003e-05,1.034290840817165,,,4.082344,,0.0011009688,0.0010976433,24085.928,18.622383
xxl,840000,,-0.99760664,149188.36,-0.7610691,2048.0068,2048.0068,149198.38,-0.76112014,-0.7598372,1.0016885,0.99206275,1.0016885,1.2849562,23886.787,20480000.0,857.37775,3.3491318,0.41864148,-82064.08,381.80103,10.58119,-5.3978856e-05,1.0342850620006305,,,4.0794,,0.0010943569,0.0010910901,48050.83,16.171717
xxl,850000,,-0.99804187,148809.39,-0.75913584,2048.007,2048.007,148820.31,-0.7591915,-0.7579189,1.0016791,0.9921058,1.0016791,1.2834061,23886.643,20480000.0,857.38293,3.349152,0.418644,-82064.58,381.80334,10.562724,-5.388465e-05,1.0342612716459412,,,4.042861,,0.0010878694,0.001084653,72013.164,16.073597
xxl,860000,,-0.9980748,148868.0,-0.7594348,2048.0059,2048.0059,148877.7,-0.75948435,-0.75820774,1.0016837,0.99208516,1.0016837,1.2841513,23886.639,20480000.0,857.3831,3.3491528,0.4186441,-82064.59,381.8034,10.445711,-5.3287724e-05,1.0342654008969066,,,4.060582,,0.0010814735,0.0010783284,95975.34,373.9177
xxl,870000,,-0.99713445,149832.44,-0.7643548,2048.0076,2048.0076,149842.5,-0.7644061,-0.76312006,1.0016853,0.9920778,1.0016853,1.2844138,23886.803,20480000.0,857.3772,3.3491297,0.4186412,-82064.03,381.80078,10.215522,-5.2113435e-05,1.0342495326829972,,,4.126259,,0.0010752105,0.0010721132,120295.58,16.080717
xxl,880000,,-0.99745464,149522.55,-0.7627739,2048.0063,2048.0063,149532.62,-0.76282537,-0.7615457,1.0016804,0.9921004,1.0016804,1.283601,23886.822,20480000.0,857.37646,3.3491268,0.41864085,-82063.96,381.80045,10.150019,-5.1779276e-05,1.03423339523975,,,4.080284,,0.0010690391,0.0010660042,144258.03,374.37106
xxl,890000,,-0.99728113,149681.3,-0.7635838,2048.0068,2048.0068,149691.34,-0.76363504,-0.7623535,1.0016811,0.9920976,1.0016811,1.2837049,23886.703,20480000.0,857.3808,3.3491437,0.41864297,-82064.375,381.80237,10.137195,-5.1713858e-05,1.0342230606012344,,,4.1037054,,0.0010630098,0.0010599985,168578.6,16.27938
xxl,900000,,-0.9974039,149553.5,-0.76293176,2048.0068,2048.0068,149563.94,-0.76298505,-0.7617023,1.0016841,0.9920831,1.0016841,1.2842251,23886.62,20480000.0,857.38385,3.3491557,0.41864446,-82064.664,381.80374,10.096729,-5.1507428e-05,1.0342211463964193,,,4.0657253,,0.0010570436,0.0010540931,192541.02,15.99501
xxl,910000,,-0.9973279,149585.81,-0.7630967,2048.0056,2048.0056,149595.7,-0.7631471,-0.7618652,1.0016826,0.9920901,1.0016826,1.283975,23886.742,20480000.0,857.37933,3.349138,0.41864225,-82064.234,381.80176,10.065808,-5.134969e-05,1.0342103377592444,,,4.05758,,0.0010511758,0.0010482854,216503.3,17.556368
xxl,920000,,-0.99758697,149361.4,-0.76195186,2048.0078,2048.0078,149371.62,-0.762004,-0.7607278,1.0016776,0.9921134,1.0016776,1.2831371,23886.812,20480000.0,857.37683,3.3491282,0.41864103,-82063.99,381.80063,10.043313,-5.123493e-05,1.0341999583558938,,,4.087054,,0.0010454139,0.0010425727,240467.14,15.872915
xxl,930000,,-0.9977058,149243.48,-0.76135033,2048.0063,2048.0063,149253.27,-0.7614002,-0.7601264,1.0016758,0.99212164,1.0016758,1.28284,23889.293,20480000.0,857.28784,3.3487806,0.41859758,-82055.48,381.76102,9.997879,-5.1003157e-05,1.0341836852554422,,,4.0874434,,0.0010397567,0.0010369522,264431.88,15.848333
xxl,940000,,-0.99779534,149142.97,-0.76083755,2048.0078,2048.0078,149153.22,-0.76088977,-0.75961584,1.001677,0.9921151,1.001677,1.2830701,23886.576,20480000.0,857.3853,3.3491614,0.41864517,-82064.805,381.8044,9.969084,-5.085626e-05,1.0341970141768,,,4.0555263,,0.001034198,0.0010314218,288393.8,15.602337
xxl,950000,,-0.9978646,149055.45,-0.76039106,2048.0051,2048.0051,149065.34,-0.76044154,-0.75917035,1.0016744,0.99212855,1.0016744,1.2825917,23886.74,20480000.0,857.37946,3.3491385,0.4186423,-82064.24,381.8018,9.949085,-5.0754235e-05,1.0341839831131436,,,4.093444,,0.0010286918,0.0010259789,312355.62,15.066502
xxl,960000,,-0.99792016,148981.88,-0.7600157,2048.007,2048.007,148992.16,-0.7600682,-0.7587965,1.0016758,0.9921222,1.0016758,1.2828236,23886.771,20480000.0,857.3783,3.349134,0.41864175,-82064.14,381.80127,9.932182,-5.0668008e-05,1.0341838854526613,,,4.075065,,0.0010232817,0.0010206213,336316.97,15.375542
xxl,970000,,-0.996759,150286.77,-0.7666725,2048.0056,2048.0056,150296.67,-0.76672304,-0.76542723,1.001693,0.99204105,1.001693,1.2857323,23886.637,20480000.0,857.3832,3.349153,0.41864413,-82064.6,381.80344,10.073559,-5.1389223e-05,1.0341738304407513,,,4.0730724,,0.0010179855,0.0010153466,360278.47,373.64124
xxl,1010000,,-0.9970806,149851.84,-0.7644538,2048.0076,2048.0076,149861.77,-0.76450443,-0.76320696,1.0017,0.9920083,1.0017,1.2869084,13981.094,20480000.0,1464.8354,2.8610067,0.7152517,-140207.02,326.15475,10.26022,-5.2341456e-05,1.034142059361934,,,0.9859891,,0.0009975114,0.0009950376,14123.367,372.63782
xxl,1020000,,-0.9973819,149465.28,-0.76248175,2048.0076,2048.0076,149475.58,-0.7625343,-0.76124394,1.001695,0.9920314,1.001695,1.2860798,13986.386,20480000.0,1464.2811,2.859924,0.714981,-140153.95,326.03134,10.180609,-5.1935327e-05,1.0341384485183065,,,0.9971707,,0.000992588,0.000990148,28531.4,51.65641
xxl,1030000,,-0.99777156,149111.3,-0.76067597,2048.0063,2048.0063,149121.64,-0.7607288,-0.7594434,1.0016924,0.9920431,1.0016924,1.2856569,13984.061,20480000.0,1464.5245,2.8603995,0.7150999,-140177.27,326.08554,10.168705,-5.1874606e-05,1.0341225443173445,,,1.0183632,,0.0009877331,0.0009853297,42602.9,48.85498
large,0,24.80904,1.9343914e-05,2547840.2,10.912829,2048.0,2048.0,2550600.2,10.924652,10.93982,0.9986135,0.99859595,0.9986135,0.9985992,97.93206,204800.0,2091.2458,8.168929,1.0211161,238402.02,931.2579,2758.885,0.011816771,1.837822642202015,"b""    from __gin__ import dynamic_registration\n    import __main__ as train_script\n    import data.pile.tasks\n    from models.scalable_t5 import network\n    import seqio\n    from t5x import adafactor\n    from t5x import gin_utils\n    from t5x import models\n    from t5x import partitioning\n    from t5x import trainer\n    from t5x import utils\n    \n#### Macros:\n\n    BATCH_SIZE = 2048\n    DROPOUT_RATE = 0.0\n    EVAL_BATCH_SIZE = 256\n    LABEL_SMOOTHING = 0.0\n    LOSS_NORMALIZING_FACTOR = None\n    MIXTURE_OR_TASK_MODULE = None\n    MIXTURE_OR_TASK_NAME = 'pile_r_denoiser'\n    MODEL = @models.EncoderDecoderModel()\n    MODEL_DIR = 'gs://improved-t5/ckpts/v2_large_mlm/'\n    OPTIMIZER = @adafactor.Adafactor()\n    RANDOM_SEED = None\n    SAVING_PERIOD = 10000\n    SHUFFLE_TRAIN_EXAMPLES = True\n    TASK_FEATURE_LENGTHS = {'inputs': 512, 'targets': 114}\n    TRAIN_STEPS = 1000000\n    USE_CACHED_TASKS = False\n    USE_HARDWARE_RNG = False\n    VOCABULARY = @seqio.SentencePieceVocabulary()\n    Z_LOSS = 0.0001\n    \n#### Parameters for adafactor.Adafactor:\n\n    adafactor.Adafactor.decay_rate = 0.8\n    adafactor.Adafactor.logical_factor_rules = \\\n        @adafactor.standard_logical_factor_rules()\n    adafactor.Adafactor.step_offset = 0\n    \n#### Parameters for utils.CheckpointConfig:\n\n    utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()\n    utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()\n    \n#### Parameters for utils.create_learning_rate_scheduler:\n\n    utils.create_learning_rate_scheduler.base_learning_rate = 1.0\n    utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'\n    utils.create_learning_rate_scheduler.warmup_steps = 10000\n    \n#### Parameters for infer_eval/utils.DatasetConfig:\n\n    infer_eval/utils.DatasetConfig.batch_size = %EVAL_BATCH_SIZE\n    infer_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    infer_eval/utils.DatasetConfig.seed = 0\n    infer_eval/utils.DatasetConfig.shuffle = False\n    infer_eval/utils.DatasetConfig.split = 'validation'\n    infer_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    \n#### Parameters for train/utils.DatasetConfig:\n\n    train/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train/utils.DatasetConfig.pack = True\n    train/utils.DatasetConfig.seed = None\n    train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES\n    train/utils.DatasetConfig.split = 'train'\n    train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for train_eval/utils.DatasetConfig:\n\n    train_eval/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train_eval/utils.DatasetConfig.pack = True\n    train_eval/utils.DatasetConfig.seed = 42\n    train_eval/utils.DatasetConfig.shuffle = False\n    train_eval/utils.DatasetConfig.split = 'validation'\n    train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for models.EncoderDecoderModel:\n\n    models.EncoderDecoderModel.input_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.label_smoothing = %LABEL_SMOOTHING\n    models.EncoderDecoderModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR\n    models.EncoderDecoderModel.module = @network.Transformer()\n    models.EncoderDecoderModel.optimizer_def = %OPTIMIZER\n    models.EncoderDecoderModel.output_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.z_loss = %Z_LOSS\n    \n#### Parameters for seqio.Evaluator:\n\n    seqio.Evaluator.logger_cls = \\\n        [@seqio.PyLoggingLogger, @seqio.TensorBoardLogger, @seqio.JSONLogger]\n    seqio.Evaluator.num_examples = 2048\n    \n#### Parameters for partitioning.PjitPartitioner:\n\n    partitioning.PjitPartitioner.logical_axis_rules = \\\n        @partitioning.standard_logical_axis_rules()\n    partitioning.PjitPartitioner.model_parallel_submesh = None\n    partitioning.PjitPartitioner.num_partitions = 1\n    \n#### Parameters for utils.RestoreCheckpointConfig:\n\n    utils.RestoreCheckpointConfig.path = []\n    \n#### Parameters for utils.SaveCheckpointConfig:\n\n    utils.SaveCheckpointConfig.dtype = 'float32'\n    utils.SaveCheckpointConfig.keep = None\n    utils.SaveCheckpointConfig.period = %SAVING_PERIOD\n    utils.SaveCheckpointConfig.save_dataset = False\n    \n#### Parameters for seqio.SentencePieceVocabulary:\n\n    seqio.SentencePieceVocabulary.extra_ids = 100\n    seqio.SentencePieceVocabulary.sentencepiece_model_file = \\\n        'gs://improved-t5/vocabs/tokenizer.model'\n    \n#### Parameters for network.T5Config:\n\n    network.T5Config.dropout_rate = %DROPOUT_RATE\n    network.T5Config.dtype = 'bfloat16'\n    network.T5Config.emb_dim = 1024\n    network.T5Config.head_dim = 64\n    network.T5Config.logits_via_embedding = False\n    network.T5Config.mlp_activations = ('gelu', 'linear')\n    network.T5Config.mlp_dim = 2816\n    network.T5Config.num_decoder_layers = 24\n    network.T5Config.num_encoder_layers = 24\n    network.T5Config.num_heads = 16\n    network.T5Config.remat_policy = 'minimal'\n    network.T5Config.scan_layers = True\n    network.T5Config.vocab_size = 32128\n    \n#### Parameters for train_script.train:\n\n    train_script.train.checkpoint_cfg = @utils.CheckpointConfig()\n    train_script.train.eval_period = %SAVING_PERIOD\n    train_script.train.eval_steps = 100\n    train_script.train.infer_eval_dataset_cfg = @infer_eval/utils.DatasetConfig()\n    train_script.train.inference_evaluator_cls = @seqio.Evaluator\n    train_script.train.model = %MODEL\n    train_script.train.model_dir = %MODEL_DIR\n    train_script.train.partitioner = @partitioning.PjitPartitioner()\n    train_script.train.random_seed = %RANDOM_SEED\n    train_script.train.run_eval_before_training = True\n    train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config\n    train_script.train.total_steps = %TRAIN_STEPS\n    train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()\n    train_script.train.train_eval_dataset_cfg = @train_eval/utils.DatasetConfig()\n    train_script.train.trainer_cls = @trainer.Trainer\n    train_script.train.use_hardware_rng = %USE_HARDWARE_RNG\n    \n#### Parameters for trainer.Trainer:\n\n    trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()\n    trainer.Trainer.num_microbatches = None\n    \n#### Parameters for network.Transformer:\n\n    network.Transformer.config = @network.T5Config()""",5.333794,4.864004,2.002716e-05,,,,
large,1510000,34.407234,-0.9591628,191727.0,-0.97807556,2048.0068,2048.0068,191746.06,-0.9781729,-0.97652495,1.0016875,0.99206626,1.0016875,1.2848258,4865.2563,20480000.0,4209.439,65.772484,2.0553901,-402907.3,7498.0635,18.66405,-9.521273e-05,1.0444554171811982,"b""    from __gin__ import dynamic_registration\n    import __main__ as train_script\n    import data.pile.tasks\n    from models.scalable_t5 import network\n    import seqio\n    from t5x import adafactor\n    from t5x import gin_utils\n    from t5x import models\n    from t5x import partitioning\n    from t5x import trainer\n    from t5x import utils\n    \n#### Macros:\n\n    BATCH_SIZE = 2048\n    DROPOUT_RATE = 0.0\n    EVAL_BATCH_SIZE = 256\n    LABEL_SMOOTHING = 0.0\n    LOSS_NORMALIZING_FACTOR = None\n    MIXTURE_OR_TASK_MODULE = None\n    MIXTURE_OR_TASK_NAME = 'pile_r_denoiser'\n    MODEL = @models.EncoderDecoderModel()\n    MODEL_DIR = 'gs://improved-t5/ckpts/v2_large_mlm/'\n    OPTIMIZER = @adafactor.Adafactor()\n    RANDOM_SEED = None\n    SAVING_PERIOD = 10000\n    SHUFFLE_TRAIN_EXAMPLES = True\n    TASK_FEATURE_LENGTHS = {'inputs': 512, 'targets': 114}\n    TRAIN_STEPS = 2000000\n    USE_CACHED_TASKS = False\n    USE_HARDWARE_RNG = False\n    VOCABULARY = @seqio.SentencePieceVocabulary()\n    Z_LOSS = 0.0001\n    \n#### Parameters for adafactor.Adafactor:\n\n    adafactor.Adafactor.decay_rate = 0.8\n    adafactor.Adafactor.logical_factor_rules = \\\n        @adafactor.standard_logical_factor_rules()\n    adafactor.Adafactor.step_offset = 0\n    \n#### Parameters for utils.CheckpointConfig:\n\n    utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()\n    utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()\n    \n#### Parameters for utils.create_learning_rate_scheduler:\n\n    utils.create_learning_rate_scheduler.base_learning_rate = 1.0\n    utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'\n    utils.create_learning_rate_scheduler.warmup_steps = 10000\n    \n#### Parameters for infer_eval/utils.DatasetConfig:\n\n    infer_eval/utils.DatasetConfig.batch_size = %EVAL_BATCH_SIZE\n    infer_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    infer_eval/utils.DatasetConfig.seed = 0\n    infer_eval/utils.DatasetConfig.shuffle = False\n    infer_eval/utils.DatasetConfig.split = 'validation'\n    infer_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    \n#### Parameters for train/utils.DatasetConfig:\n\n    train/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train/utils.DatasetConfig.pack = True\n    train/utils.DatasetConfig.seed = None\n    train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES\n    train/utils.DatasetConfig.split = 'train'\n    train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for train_eval/utils.DatasetConfig:\n\n    train_eval/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train_eval/utils.DatasetConfig.pack = True\n    train_eval/utils.DatasetConfig.seed = 42\n    train_eval/utils.DatasetConfig.shuffle = False\n    train_eval/utils.DatasetConfig.split = 'validation'\n    train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for models.EncoderDecoderModel:\n\n    models.EncoderDecoderModel.input_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.label_smoothing = %LABEL_SMOOTHING\n    models.EncoderDecoderModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR\n    models.EncoderDecoderModel.module = @network.Transformer()\n    models.EncoderDecoderModel.optimizer_def = %OPTIMIZER\n    models.EncoderDecoderModel.output_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.z_loss = %Z_LOSS\n    \n#### Parameters for seqio.Evaluator:\n\n    seqio.Evaluator.logger_cls = \\\n        [@seqio.PyLoggingLogger, @seqio.TensorBoardLogger, @seqio.JSONLogger]\n    seqio.Evaluator.num_examples = 2048\n    \n#### Parameters for partitioning.PjitPartitioner:\n\n    partitioning.PjitPartitioner.logical_axis_rules = \\\n        @partitioning.standard_logical_axis_rules()\n    partitioning.PjitPartitioner.model_parallel_submesh = None\n    partitioning.PjitPartitioner.num_partitions = 1\n    \n#### Parameters for utils.RestoreCheckpointConfig:\n\n    utils.RestoreCheckpointConfig.path = []\n    \n#### Parameters for utils.SaveCheckpointConfig:\n\n    utils.SaveCheckpointConfig.dtype = 'float32'\n    utils.SaveCheckpointConfig.keep = None\n    utils.SaveCheckpointConfig.period = %SAVING_PERIOD\n    utils.SaveCheckpointConfig.save_dataset = False\n    \n#### Parameters for seqio.SentencePieceVocabulary:\n\n    seqio.SentencePieceVocabulary.extra_ids = 100\n    seqio.SentencePieceVocabulary.sentencepiece_model_file = \\\n        'gs://improved-t5/vocabs/tokenizer.model'\n    \n#### Parameters for network.T5Config:\n\n    network.T5Config.dropout_rate = %DROPOUT_RATE\n    network.T5Config.dtype = 'bfloat16'\n    network.T5Config.emb_dim = 1024\n    network.T5Config.head_dim = 64\n    network.T5Config.logits_via_embedding = False\n    network.T5Config.mlp_activations = ('gelu', 'linear')\n    network.T5Config.mlp_dim = 2816\n    network.T5Config.num_decoder_layers = 24\n    network.T5Config.num_encoder_layers = 24\n    network.T5Config.num_heads = 16\n    network.T5Config.remat_policy = 'minimal'\n    network.T5Config.scan_layers = True\n    network.T5Config.vocab_size = 32128\n    \n#### Parameters for train_script.train:\n\n    train_script.train.checkpoint_cfg = @utils.CheckpointConfig()\n    train_script.train.eval_period = %SAVING_PERIOD\n    train_script.train.eval_steps = 100\n    train_script.train.infer_eval_dataset_cfg = @infer_eval/utils.DatasetConfig()\n    train_script.train.inference_evaluator_cls = @seqio.Evaluator\n    train_script.train.model = %MODEL\n    train_script.train.model_dir = %MODEL_DIR\n    train_script.train.partitioner = @partitioning.PjitPartitioner()\n    train_script.train.random_seed = %RANDOM_SEED\n    train_script.train.run_eval_before_training = True\n    train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config\n    train_script.train.total_steps = %TRAIN_STEPS\n    train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()\n    train_script.train.train_eval_dataset_cfg = @train_eval/utils.DatasetConfig()\n    train_script.train.trainer_cls = @trainer.Trainer\n    train_script.train.use_hardware_rng = %USE_HARDWARE_RNG\n    \n#### Parameters for trainer.Trainer:\n\n    trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()\n    trainer.Trainer.num_microbatches = None\n    \n#### Parameters for network.Transformer:\n\n    network.Transformer.config = @network.T5Config()""",8.540316,8.695463,1.4543533e-05,0.0008151271,0.00081378873,249536.36,12.841709
large,1660000,32.08248,-0.95858264,192415.36,-0.9815872,2048.007,2048.007,192433.55,-0.98168,-0.9800166,1.0016973,0.9920211,1.0016973,1.28645,2579.0212,20480000.0,7940.997,62.03904,3.87744,-760074.1,7072.45,18.276037,-9.323333e-05,1.0442795255620871,"b""    from __gin__ import dynamic_registration\n    import __main__ as train_script\n    import data.pile.tasks\n    from models.scalable_t5 import network\n    import seqio\n    from t5x import adafactor\n    from t5x import gin_utils\n    from t5x import models\n    from t5x import partitioning\n    from t5x import trainer\n    from t5x import utils\n    \n#### Macros:\n\n    BATCH_SIZE = 2048\n    DROPOUT_RATE = 0.0\n    EVAL_BATCH_SIZE = 256\n    LABEL_SMOOTHING = 0.0\n    LOSS_NORMALIZING_FACTOR = None\n    MIXTURE_OR_TASK_MODULE = None\n    MIXTURE_OR_TASK_NAME = 'pile_r_denoiser'\n    MODEL = @models.EncoderDecoderModel()\n    MODEL_DIR = 'gs://improved-t5/ckpts/v2_large_mlm/'\n    OPTIMIZER = @adafactor.Adafactor()\n    RANDOM_SEED = None\n    SAVING_PERIOD = 10000\n    SHUFFLE_TRAIN_EXAMPLES = True\n    TASK_FEATURE_LENGTHS = {'inputs': 512, 'targets': 114}\n    TRAIN_STEPS = 2000000\n    USE_CACHED_TASKS = False\n    USE_HARDWARE_RNG = False\n    VOCABULARY = @seqio.SentencePieceVocabulary()\n    Z_LOSS = 0.0001\n    \n#### Parameters for adafactor.Adafactor:\n\n    adafactor.Adafactor.decay_rate = 0.8\n    adafactor.Adafactor.logical_factor_rules = \\\n        @adafactor.standard_logical_factor_rules()\n    adafactor.Adafactor.step_offset = 0\n    \n#### Parameters for utils.CheckpointConfig:\n\n    utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()\n    utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()\n    \n#### Parameters for utils.create_learning_rate_scheduler:\n\n    utils.create_learning_rate_scheduler.base_learning_rate = 1.0\n    utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'\n    utils.create_learning_rate_scheduler.warmup_steps = 10000\n    \n#### Parameters for infer_eval/utils.DatasetConfig:\n\n    infer_eval/utils.DatasetConfig.batch_size = %EVAL_BATCH_SIZE\n    infer_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    infer_eval/utils.DatasetConfig.seed = 0\n    infer_eval/utils.DatasetConfig.shuffle = False\n    infer_eval/utils.DatasetConfig.split = 'validation'\n    infer_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    \n#### Parameters for train/utils.DatasetConfig:\n\n    train/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train/utils.DatasetConfig.pack = True\n    train/utils.DatasetConfig.seed = None\n    train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES\n    train/utils.DatasetConfig.split = 'train'\n    train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for train_eval/utils.DatasetConfig:\n\n    train_eval/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train_eval/utils.DatasetConfig.pack = True\n    train_eval/utils.DatasetConfig.seed = 42\n    train_eval/utils.DatasetConfig.shuffle = False\n    train_eval/utils.DatasetConfig.split = 'validation'\n    train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for models.EncoderDecoderModel:\n\n    models.EncoderDecoderModel.input_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.label_smoothing = %LABEL_SMOOTHING\n    models.EncoderDecoderModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR\n    models.EncoderDecoderModel.module = @network.Transformer()\n    models.EncoderDecoderModel.optimizer_def = %OPTIMIZER\n    models.EncoderDecoderModel.output_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.z_loss = %Z_LOSS\n    \n#### Parameters for seqio.Evaluator:\n\n    seqio.Evaluator.logger_cls = \\\n        [@seqio.PyLoggingLogger, @seqio.TensorBoardLogger, @seqio.JSONLogger]\n    seqio.Evaluator.num_examples = 2048\n    \n#### Parameters for partitioning.PjitPartitioner:\n\n    partitioning.PjitPartitioner.logical_axis_rules = \\\n        @partitioning.standard_logical_axis_rules()\n    partitioning.PjitPartitioner.model_parallel_submesh = None\n    partitioning.PjitPartitioner.num_partitions = 1\n    \n#### Parameters for utils.RestoreCheckpointConfig:\n\n    utils.RestoreCheckpointConfig.path = []\n    \n#### Parameters for utils.SaveCheckpointConfig:\n\n    utils.SaveCheckpointConfig.dtype = 'float32'\n    utils.SaveCheckpointConfig.keep = None\n    utils.SaveCheckpointConfig.period = %SAVING_PERIOD\n    utils.SaveCheckpointConfig.save_dataset = False\n    \n#### Parameters for seqio.SentencePieceVocabulary:\n\n    seqio.SentencePieceVocabulary.extra_ids = 100\n    seqio.SentencePieceVocabulary.sentencepiece_model_file = \\\n        'gs://improved-t5/vocabs/tokenizer.model'\n    \n#### Parameters for network.T5Config:\n\n    network.T5Config.dropout_rate = %DROPOUT_RATE\n    network.T5Config.dtype = 'bfloat16'\n    network.T5Config.emb_dim = 1024\n    network.T5Config.head_dim = 64\n    network.T5Config.logits_via_embedding = False\n    network.T5Config.mlp_activations = ('gelu', 'linear')\n    network.T5Config.mlp_dim = 2816\n    network.T5Config.num_decoder_layers = 24\n    network.T5Config.num_encoder_layers = 24\n    network.T5Config.num_heads = 16\n    network.T5Config.remat_policy = 'minimal'\n    network.T5Config.scan_layers = True\n    network.T5Config.vocab_size = 32128\n    \n#### Parameters for train_script.train:\n\n    train_script.train.checkpoint_cfg = @utils.CheckpointConfig()\n    train_script.train.eval_period = %SAVING_PERIOD\n    train_script.train.eval_steps = 100\n    train_script.train.infer_eval_dataset_cfg = @infer_eval/utils.DatasetConfig()\n    train_script.train.inference_evaluator_cls = @seqio.Evaluator\n    train_script.train.model = %MODEL\n    train_script.train.model_dir = %MODEL_DIR\n    train_script.train.partitioner = @partitioning.PjitPartitioner()\n    train_script.train.random_seed = %RANDOM_SEED\n    train_script.train.run_eval_before_training = True\n    train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config\n    train_script.train.total_steps = %TRAIN_STEPS\n    train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()\n    train_script.train.train_eval_dataset_cfg = @train_eval/utils.DatasetConfig()\n    train_script.train.trainer_cls = @trainer.Trainer\n    train_script.train.use_hardware_rng = %USE_HARDWARE_RNG\n    \n#### Parameters for trainer.Trainer:\n\n    trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()\n    trainer.Trainer.num_microbatches = None\n    \n#### Parameters for network.Transformer:\n\n    network.Transformer.config = @network.T5Config()""",8.879068,0.8002782,1.335144e-05,0.00077732676,0.00077615073,13132.507,10.951503
large,1670000,,-0.95889723,192089.25,-0.9799236,2048.0063,2048.0063,192107.0,-0.98001415,-0.97835666,1.0016942,0.9920353,1.0016942,1.2859374,2579.0098,20480000.0,7941.0327,62.039318,3.8774574,-760077.5,7072.4824,18.199644,-9.284361e-05,1.0442983005462632,,,0.7710724,,0.00077498663,0.00077382347,15730.453,370.524
large,1680000,,-0.9580901,192937.14,-0.98424906,2048.0059,2048.0059,192955.16,-0.9843409,-0.98267484,1.0016954,0.9920295,1.0016954,1.2861483,2579.0103,20480000.0,7941.031,62.039303,3.8774564,-760077.4,7072.4805,18.228083,-9.298869e-05,1.0443011346378268,,,0.78424954,,0.000772665,0.00077151705,5730.9536,11.246182
large,1690000,,-0.95850706,192505.94,-0.9820493,2048.007,2048.007,192523.94,-0.9821411,-0.9804785,1.0016956,0.9920287,1.0016956,1.2861776,2579.0364,20480000.0,7940.951,62.03868,3.8774176,-760069.7,7072.4097,18.154661,-9.2614144e-05,1.0443062549364823,,,0.7990711,,0.00077035814,0.000769231,8329.2,12.56864
large,1700000,,-0.9587666,192221.6,-0.9805987,2048.008,2048.008,192239.38,-0.9806894,-0.9790245,1.0017005,0.9920057,1.0017005,1.2870041,2578.9863,20480000.0,7941.105,62.039883,3.8774927,-760084.44,7072.5464,18.125278,-9.2464245e-05,1.0442863083586553,,,0.78604627,,0.00076810434,0.0007669652,10928.819,11.179426
large,1710000,,-0.9586765,192290.75,-0.9809515,2048.0085,2048.0085,192308.6,-0.9810425,-0.97938585,1.0016916,0.99204767,1.0016916,1.285495,2579.0337,20480000.0,7940.9595,62.038746,3.8774216,-760070.5,7072.417,18.099205,-9.233123e-05,1.044268989298488,,,0.7874079,,0.00076584175,0.0007647193,13526.984,12.287857
large,1720000,,-0.9589826,191993.17,-0.9794335,2048.0063,2048.0063,192011.03,-0.97952455,-0.97787,1.001692,0.9920458,1.001692,1.2855619,2578.9844,20480000.0,7941.111,62.03993,3.8774955,-760085.0,7072.552,18.017948,-9.1916714e-05,1.0442641935246255,,,0.7954211,,0.0007636021,0.0007624931,16126.252,10.909969
large,1730000,,-0.95902354,191981.6,-0.97937435,2048.007,2048.007,192000.02,-0.97946835,-0.977811,1.0016949,0.9920322,1.0016949,1.2860539,2578.9832,20480000.0,7941.1143,62.039955,3.8774972,-760085.4,7072.5547,17.97496,-9.169742e-05,1.044268206904501,,,0.80138826,,0.00076139066,0.0007602861,18724.18,369.12076
large,1740000,,-0.9589531,192035.9,-0.9796514,2048.0073,2048.0073,192053.75,-0.9797425,-0.978096,1.0016834,0.9920862,1.0016834,1.284111,2579.0146,20480000.0,7941.017,62.039196,3.8774498,-760076.06,7072.4683,17.900671,-9.1318434e-05,1.0442315276169312,,,0.80130506,,0.00075918884,0.00075809826,21680.326,12.458603
large,1750000,,-0.9588961,192030.36,-0.97962314,2048.0088,2048.0088,192047.73,-0.97971183,-0.9780581,1.0016909,0.9920506,1.0016909,1.285387,2579.6218,20480000.0,7939.148,62.024593,3.876537,-759897.1,7070.8037,17.900724,-9.131871e-05,1.0442747855213816,,,0.8110256,,0.00075700623,0.00075592916,24280.404,11.526764
large,1760000,,-0.9591772,191778.44,-0.978338,2048.0073,2048.0073,191796.62,-0.9784308,-0.97677547,1.0016947,0.9920334,1.0016947,1.2860094,2578.987,20480000.0,7941.1025,62.039864,3.8774915,-760084.2,7072.5444,17.874998,-9.118747e-05,1.0441877091310383,,,0.7693515,,0.0007548546,0.0007537786,26878.926,10.9367485
large,1770000,,-0.95939684,191532.89,-0.9770854,2048.0056,2048.0056,191550.53,-0.9771754,-0.97554034,1.0016761,0.99211985,1.0016761,1.2828983,2579.017,20480000.0,7941.01,62.03914,3.8774462,-760075.4,7072.462,17.815895,-9.088596e-05,1.044211666097422,,,0.7888248,,0.00075270765,0.00075164624,29476.902,369.04572
large,1780000,,-0.95944065,191482.44,-0.976828,2048.0068,2048.0068,191499.56,-0.97691536,-0.9752674,1.0016898,0.9920563,1.0016898,1.2851855,2578.9775,20480000.0,7941.132,62.040092,3.8775058,-760087.0,7072.5703,17.779947,-9.070257e-05,1.0441996536960254,,,0.7850845,,0.00075058796,0.0007495319,32432.998,12.096992
large,1790000,,-0.959501,191364.77,-0.9762277,2048.008,2048.008,191382.62,-0.97631884,-0.9746784,1.001683,0.9920872,1.001683,1.2840735,2578.9915,20480000.0,7941.089,62.039757,3.8774848,-760082.9,7072.532,17.732122,-9.0458605e-05,1.044180047961841,,,0.802294,,0.00074848067,0.0007474353,35032.027,369.2091
large,1800000,,-0.95984596,191055.66,-0.97465086,2048.0076,2048.0076,191073.28,-0.9747407,-0.97309935,1.0016867,0.992071,1.0016867,1.2846613,2579.6885,20480000.0,7938.943,62.02299,3.876437,-759877.5,7070.621,17.877169,-9.119854e-05,1.044058582616784,,,0.7845223,,0.0007463927,0.00074535626,37988.953,12.765732
large,1810000,,-0.95981777,191142.86,-0.9750956,2048.0083,2048.0083,191160.36,-0.9751849,-0.9735343,1.0016955,0.99202937,1.0016955,1.2861532,2578.9897,20480000.0,7941.094,62.0398,3.8774874,-760083.4,7072.537,17.666847,-9.01256e-05,1.0440434193431547,,,0.7845688,,0.0007443258,0.00074329437,40588.695,369.08893
large,1820000,,-0.9617816,188900.84,-0.9636583,2048.0056,2048.0056,188918.31,-0.9637474,-0.96213275,1.0016782,0.9921112,1.0016782,1.2832177,2578.9849,20480000.0,7941.109,62.039913,3.8774946,-760084.8,7072.5503,17.546944,-8.951393e-05,1.0440347022830725,,,0.7957859,,0.0007422699,0.00074124953,43544.875,11.998886
large,1830000,,-0.96222895,188396.14,-0.9610836,2048.0068,2048.0068,188413.05,-0.96116984,-0.95956254,1.001675,0.9921254,1.001675,1.2827048,2579.0068,20480000.0,7941.0415,62.039387,3.8774617,-760078.4,7072.49,17.460344,-8.907216e-05,1.0440016860428698,,,0.79784155,,0.0007402374,0.00073922146,46143.883,12.403788
large,1840000,,-0.96197337,188731.66,-0.96279514,2048.0063,2048.0063,188749.25,-0.96288496,-0.9612756,1.0016742,0.9921296,1.0016742,1.2825556,2578.9907,20480000.0,7941.091,62.039772,3.8774858,-760083.1,7072.534,17.475868,-8.915135e-05,1.0439921096530755,,,0.7765167,,0.0007382143,0.00073721,48743.273,12.227374
large,1850000,,-0.9610417,189731.64,-0.9678965,2048.0059,2048.0059,189748.75,-0.9679838,-0.96636784,1.0016723,0.9921386,1.0016723,1.2822303,2579.0005,20480000.0,7941.061,62.03954,3.8774712,-760080.25,7072.508,17.53145,-8.943489e-05,1.0439558406857745,,,0.80803967,,0.00073621207,0.00073521485,51342.477,18.369171
large,1860000,,-0.96105295,189672.08,-0.9675926,2048.0066,2048.0066,189689.48,-0.9676814,-0.9660606,1.0016778,0.99211174,1.0016778,1.2831906,2579.0112,20480000.0,7941.028,62.03928,3.877455,-760077.06,7072.478,17.501904,-8.928416e-05,1.0439720952593028,,,0.7914324,,0.00073421677,0.00073323573,53947.86,11.9389925
large,1870000,,-0.9613075,189408.08,-0.96624583,2048.0068,2048.0068,189425.42,-0.9663343,-0.9647125,1.0016811,0.99209726,1.0016811,1.2837166,2579.0044,20480000.0,7941.049,62.039444,3.8774652,-760079.06,7072.4966,17.48932,-8.9219975e-05,1.0439813772606794,,,0.79213643,,0.00073224993,0.0007312726,56546.79,11.259841
large,1880000,,-0.961281,189430.0,-0.9663577,2048.0068,2048.0068,189447.3,-0.9664459,-0.9648245,1.0016805,0.99209994,1.0016805,1.2836205,2578.9822,20480000.0,7941.117,62.039978,3.8774986,-760085.6,7072.5576,17.449818,-8.9018446e-05,1.0440072762911723,,,0.8032279,,0.0007303058,0.0007293251,59144.992,11.900177
large,1890000,,-0.9616085,189087.86,-0.96461225,2048.0059,2048.0059,189104.9,-0.96469927,-0.9630843,1.0016769,0.99211675,1.0016769,1.2830156,2579.0176,20480000.0,7941.0083,62.039127,3.8774455,-760075.2,7072.4604,17.424255,-8.888805e-05,1.0439467229121404,,,0.7875047,,0.0007283683,0.00072739314,61743.914,10.943298
large,1900000,,-0.96155846,189124.89,-0.9648012,2048.0068,2048.0068,189142.1,-0.964889,-0.9632698,1.0016809,0.99209774,1.0016809,1.2836975,2578.9912,20480000.0,7941.0894,62.03976,3.877485,-760082.94,7072.5327,17.387733,-8.870174e-05,1.043936884809228,,,0.80581284,,0.000726443,0.0007254765,64341.82,11.28054
large,1910000,,-0.96143275,189272.17,-0.96555257,2048.007,2048.007,189289.12,-0.965639,-0.96402204,1.0016773,0.99211466,1.0016773,1.2830894,2578.9817,20480000.0,7941.119,62.039993,3.8774996,-760085.8,7072.559,17.35173,-8.851807e-05,1.0439119542181334,,,0.79068494,,0.0007245341,0.00072357483,66940.17,369.2448
large,1920000,,-0.9615628,189119.55,-0.96477395,2048.0076,2048.0076,189137.08,-0.9648634,-0.9632482,1.0016768,0.992117,1.0016768,1.2830058,2579.0105,20480000.0,7941.0303,62.0393,3.8774562,-760077.3,7072.48,17.354996,-8.8534725e-05,1.0439159121796362,,,0.79131556,,0.0007226429,0.00072168803,69896.516,12.207256
large,1930000,,-0.9616913,188958.08,-0.9639502,2048.0063,2048.0063,188975.23,-0.9640378,-0.96242577,1.0016749,0.99212545,1.0016749,1.2826998,2579.0056,20480000.0,7941.045,62.039413,3.8774633,-760078.75,7072.493,17.328968,-8.8401954e-05,1.0439409467084029,,,0.7912624,,0.00072075875,0.0007198159,72495.7,369.09708
large,1940000,,-0.9617756,188879.12,-0.96354747,2048.0063,2048.0063,188896.19,-0.9636345,-0.96202064,1.0016776,0.9921131,1.0016776,1.2831436,2578.9878,20480000.0,7941.1,62.039845,3.8774903,-760084.0,7072.542,17.30839,-8.8296976e-05,1.0439183470356737,,,0.7885344,,0.0007188838,0.0007179584,75451.78,11.996357
large,1950000,,-0.96185166,188758.28,-0.962931,2048.0068,2048.0068,188776.11,-0.96302193,-0.96140885,1.0016779,0.99211264,1.0016779,1.2831653,2579.0178,20480000.0,7941.008,62.039124,3.8774452,-760075.1,7072.46,17.25467,-8.802292e-05,1.0438866367268906,,,0.7786963,,0.00071702193,0.00071611506,78050.78,11.893865
large,1960000,,-0.96196425,188671.44,-0.96248794,2048.0059,2048.0059,188689.08,-0.96257794,-0.9609667,1.0016767,0.99211806,1.0016767,1.2829691,2579.0095,20480000.0,7941.033,62.03932,3.8774576,-760077.56,7072.483,17.217325,-8.783241e-05,1.0439769906344587,,,0.8096361,,0.0007151901,0.0007142859,80649.664,369.01294
large,1970000,,-0.9619758,188617.75,-0.96221405,2048.008,2048.008,188634.39,-0.962299,-0.96068656,1.0016783,0.9921097,1.0016783,1.283267,2578.986,20480000.0,7941.1055,62.039886,3.877493,-760084.5,7072.547,17.223902,-8.786597e-05,1.0439228747318752,,,0.7960532,,0.0007133783,0.0007124707,83605.664,11.01651
large,1980000,,-0.9622279,188427.72,-0.96124464,2048.0056,2048.0056,188444.97,-0.9613327,-0.95972735,1.0016726,0.99213666,1.0016726,1.2823013,2579.012,20480000.0,7941.0254,62.03926,3.8774538,-760076.8,7072.476,17.200354,-8.774583e-05,1.0439091985481825,,,0.79768443,,0.00071157695,0.0007106692,86203.67,17.960354
large,1990000,,-0.95509475,196581.8,-1.0028418,2048.0083,2048.0083,196599.25,-1.0029309,-1.0011829,1.0017459,0.991793,1.0017459,1.2946453,2578.9983,20480000.0,7941.0674,62.03959,3.8774743,-760080.8,7072.513,17.5935,-8.975143e-05,1.0439163429720375,,,0.78147626,,0.00070977944,0.0007088814,88808.65,369.04572
large,2000000,,-0.9540571,197783.4,-1.0089717,2048.007,2048.007,197801.56,-1.0090643,-1.0073001,1.0017514,0.9917665,1.0017514,1.2955883,2578.986,20480000.0,7941.1055,62.039886,3.877493,-760084.5,7072.547,17.666105,-9.012182e-05,1.0439123511981028,,,0.8080292,,0.0007079785,0.00070710696,91764.66,25.998947
large,1610000,32.51354,-0.9592629,191677.31,-0.9778222,2048.0063,2048.0063,191695.73,-0.9779161,-0.97627115,1.0016849,0.9920794,1.0016849,1.2843602,6970.9067,20480000.0,2937.9248,45.905075,1.4345336,-281204.06,5233.1787,18.391642,-9.382307e-05,1.0442899885455996,"b""    from __gin__ import dynamic_registration\n    import __main__ as train_script\n    import data.pile.tasks\n    from models.scalable_t5 import network\n    import seqio\n    from t5x import adafactor\n    from t5x import gin_utils\n    from t5x import models\n    from t5x import partitioning\n    from t5x import trainer\n    from t5x import utils\n    \n#### Macros:\n\n    BATCH_SIZE = 2048\n    DROPOUT_RATE = 0.0\n    EVAL_BATCH_SIZE = 256\n    LABEL_SMOOTHING = 0.0\n    LOSS_NORMALIZING_FACTOR = None\n    MIXTURE_OR_TASK_MODULE = None\n    MIXTURE_OR_TASK_NAME = 'pile_r_denoiser'\n    MODEL = @models.EncoderDecoderModel()\n    MODEL_DIR = 'gs://improved-t5/ckpts/v2_large_mlm/'\n    OPTIMIZER = @adafactor.Adafactor()\n    RANDOM_SEED = None\n    SAVING_PERIOD = 10000\n    SHUFFLE_TRAIN_EXAMPLES = True\n    TASK_FEATURE_LENGTHS = {'inputs': 512, 'targets': 114}\n    TRAIN_STEPS = 2000000\n    USE_CACHED_TASKS = False\n    USE_HARDWARE_RNG = False\n    VOCABULARY = @seqio.SentencePieceVocabulary()\n    Z_LOSS = 0.0001\n    \n#### Parameters for adafactor.Adafactor:\n\n    adafactor.Adafactor.decay_rate = 0.8\n    adafactor.Adafactor.logical_factor_rules = \\\n        @adafactor.standard_logical_factor_rules()\n    adafactor.Adafactor.step_offset = 0\n    \n#### Parameters for utils.CheckpointConfig:\n\n    utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()\n    utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()\n    \n#### Parameters for utils.create_learning_rate_scheduler:\n\n    utils.create_learning_rate_scheduler.base_learning_rate = 1.0\n    utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'\n    utils.create_learning_rate_scheduler.warmup_steps = 10000\n    \n#### Parameters for infer_eval/utils.DatasetConfig:\n\n    infer_eval/utils.DatasetConfig.batch_size = %EVAL_BATCH_SIZE\n    infer_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    infer_eval/utils.DatasetConfig.seed = 0\n    infer_eval/utils.DatasetConfig.shuffle = False\n    infer_eval/utils.DatasetConfig.split = 'validation'\n    infer_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    \n#### Parameters for train/utils.DatasetConfig:\n\n    train/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train/utils.DatasetConfig.pack = True\n    train/utils.DatasetConfig.seed = None\n    train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES\n    train/utils.DatasetConfig.split = 'train'\n    train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for train_eval/utils.DatasetConfig:\n\n    train_eval/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train_eval/utils.DatasetConfig.pack = True\n    train_eval/utils.DatasetConfig.seed = 42\n    train_eval/utils.DatasetConfig.shuffle = False\n    train_eval/utils.DatasetConfig.split = 'validation'\n    train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for models.EncoderDecoderModel:\n\n    models.EncoderDecoderModel.input_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.label_smoothing = %LABEL_SMOOTHING\n    models.EncoderDecoderModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR\n    models.EncoderDecoderModel.module = @network.Transformer()\n    models.EncoderDecoderModel.optimizer_def = %OPTIMIZER\n    models.EncoderDecoderModel.output_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.z_loss = %Z_LOSS\n    \n#### Parameters for seqio.Evaluator:\n\n    seqio.Evaluator.logger_cls = \\\n        [@seqio.PyLoggingLogger, @seqio.TensorBoardLogger, @seqio.JSONLogger]\n    seqio.Evaluator.num_examples = 2048\n    \n#### Parameters for partitioning.PjitPartitioner:\n\n    partitioning.PjitPartitioner.logical_axis_rules = \\\n        @partitioning.standard_logical_axis_rules()\n    partitioning.PjitPartitioner.model_parallel_submesh = None\n    partitioning.PjitPartitioner.num_partitions = 1\n    \n#### Parameters for utils.RestoreCheckpointConfig:\n\n    utils.RestoreCheckpointConfig.path = []\n    \n#### Parameters for utils.SaveCheckpointConfig:\n\n    utils.SaveCheckpointConfig.dtype = 'float32'\n    utils.SaveCheckpointConfig.keep = None\n    utils.SaveCheckpointConfig.period = %SAVING_PERIOD\n    utils.SaveCheckpointConfig.save_dataset = False\n    \n#### Parameters for seqio.SentencePieceVocabulary:\n\n    seqio.SentencePieceVocabulary.extra_ids = 100\n    seqio.SentencePieceVocabulary.sentencepiece_model_file = \\\n        'gs://improved-t5/vocabs/tokenizer.model'\n    \n#### Parameters for network.T5Config:\n\n    network.T5Config.dropout_rate = %DROPOUT_RATE\n    network.T5Config.dtype = 'bfloat16'\n    network.T5Config.emb_dim = 1024\n    network.T5Config.head_dim = 64\n    network.T5Config.logits_via_embedding = False\n    network.T5Config.mlp_activations = ('gelu', 'linear')\n    network.T5Config.mlp_dim = 2816\n    network.T5Config.num_decoder_layers = 24\n    network.T5Config.num_encoder_layers = 24\n    network.T5Config.num_heads = 16\n    network.T5Config.remat_policy = 'minimal'\n    network.T5Config.scan_layers = True\n    network.T5Config.vocab_size = 32128\n    \n#### Parameters for train_script.train:\n\n    train_script.train.checkpoint_cfg = @utils.CheckpointConfig()\n    train_script.train.eval_period = %SAVING_PERIOD\n    train_script.train.eval_steps = 100\n    train_script.train.infer_eval_dataset_cfg = @infer_eval/utils.DatasetConfig()\n    train_script.train.inference_evaluator_cls = @seqio.Evaluator\n    train_script.train.model = %MODEL\n    train_script.train.model_dir = %MODEL_DIR\n    train_script.train.partitioner = @partitioning.PjitPartitioner()\n    train_script.train.random_seed = %RANDOM_SEED\n    train_script.train.run_eval_before_training = True\n    train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config\n    train_script.train.total_steps = %TRAIN_STEPS\n    train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()\n    train_script.train.train_eval_dataset_cfg = @train_eval/utils.DatasetConfig()\n    train_script.train.trainer_cls = @trainer.Trainer\n    train_script.train.use_hardware_rng = %USE_HARDWARE_RNG\n    \n#### Parameters for trainer.Trainer:\n\n    trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()\n    trainer.Trainer.num_microbatches = None\n    \n#### Parameters for network.Transformer:\n\n    network.Transformer.config = @network.T5Config()""",9.066381,6.9977427,1.66893e-05,0.00078933634,0.0007881106,71988.555,9.64908
large,1620000,,-0.9582107,192909.61,-0.98410857,2048.0068,2048.0068,192928.1,-0.98420286,-0.98252994,1.0017027,0.99199593,1.0017027,1.2873548,2579.3313,20480000.0,7940.0425,62.03158,3.8769739,-759982.75,7071.6006,18.50717,-9.4412426e-05,1.0442947434735061,,,0.7947929,,0.00078689476,0.00078567443,2737.845,12.433354
large,1630000,,-0.9580027,193051.69,-0.9848334,2048.0068,2048.0068,193070.45,-0.98492914,-0.9832624,1.001695,0.99203086,1.001695,1.2860953,2579.0183,20480000.0,7941.006,62.03911,3.8774443,-760074.94,7072.4585,18.42137,-9.3974726e-05,1.0443372251952745,,,0.8042433,,0.00078447256,0.0007832607,5337.261,11.881163
large,1640000,,-0.9583876,192667.4,-0.982873,2048.008,2048.008,192685.81,-0.9829669,-0.98130846,1.00169,0.99205494,1.00169,1.2852336,2578.9888,20480000.0,7941.097,62.03982,3.8774889,-760083.7,7072.5396,18.389833,-9.381385e-05,1.0443381090396595,,,0.7980752,,0.0007820694,0.00078086904,7936.1216,11.017112
large,1650000,,-0.9586909,192337.69,-0.9811909,2048.007,2048.007,192355.9,-0.9812839,-0.9796254,1.001693,0.992041,1.001693,1.2857354,2578.9897,20480000.0,7941.094,62.0398,3.8774874,-760083.4,7072.537,18.30766,-9.339464e-05,1.0443375467929041,,,0.80542946,,0.0007796874,0.0007784992,10534.127,11.302819
large,10000,,-0.7456349,464330.9,-2.3687363,2048.0056,2048.0056,466820.88,-2.3814387,-2.3774176,1.0016913,0.99204826,1.0016913,1.285472,1761.6503,20480000.0,11625.463,45.411964,5.6764956,-1112733.5,5176.964,2489.788,-0.012701399,1.0739227101927353,,,0.7360065,,0.010000295,0.01,1908.1172,11.693929
large,20000,,-0.8767113,287489.44,-1.4665978,2048.0076,2048.0076,287975.03,-1.4690751,-1.4665972,1.0016896,0.9920565,1.0016896,1.2851734,1761.5953,20480000.0,11625.825,45.41338,5.6766725,-1112768.2,5177.1255,485.11285,-0.0024747534,1.063885861226616,,,0.71850896,,0.008284418,0.0070712445,3687.068,10.974796
large,30000,,-0.89766115,262171.66,-1.3374417,2048.0083,2048.0083,262408.44,-1.3386496,-1.3363907,1.0016903,0.9920534,1.0016903,1.2852862,1761.599,20480000.0,11625.802,45.41329,5.676661,-1112765.9,5177.1147,236.58995,-0.0012069393,1.0597512041238146,,,0.73869514,,0.00635681,0.0057735993,5465.2915,12.882885
large,40000,,-0.90837497,249546.98,-1.2730383,2048.0088,2048.0088,249717.61,-1.2739087,-1.2717628,1.0016873,0.99206805,1.0016873,1.2847657,1761.5902,20480000.0,11625.859,45.413513,5.676689,-1112771.4,5177.1406,171.22012,-0.00087346183,1.0573198494692888,,,0.75106883,,0.005359021,0.0050000623,7245.4336,15.348225
large,50000,,-0.9161761,240498.52,-1.2268784,2048.0083,2048.0083,240638.16,-1.2275908,-1.2255205,1.0016893,0.99205804,1.0016893,1.285122,1761.6254,20480000.0,11625.627,45.412605,5.6765757,-1112749.2,5177.037,139.4954,-0.0007116214,1.0556552102226666,,,0.72558665,,0.004721385,0.0044721807,9028.063,11.482043
large,60000,,-0.9222332,233617.69,-1.1917766,2048.0066,2048.0066,233738.42,-1.1923925,-1.1903796,1.001691,0.9920506,1.001691,1.285392,1761.5944,20480000.0,11625.832,45.413406,5.676676,-1112768.9,5177.1284,121.294685,-0.00061877235,1.0542530413792763,,,0.7227676,,0.0042684553,0.004082517,10806.784,17.747948
large,70000,,-0.9254359,229887.67,-1.1727483,2048.0066,2048.0066,229996.44,-1.1733032,-1.1713237,1.00169,0.9920551,1.00169,1.2852279,1761.5828,20480000.0,11625.908,45.413704,5.676713,-1112776.1,5177.162,108.568634,-0.0005538517,1.0534685618686683,,,0.70647335,,0.003925243,0.0037796716,12591.749,10.9138155
large,80000,,-0.92814976,226757.42,-1.1567796,2048.0073,2048.0073,226855.66,-1.1572808,-1.155328,1.0016903,0.992054,1.0016903,1.2852675,1761.641,20480000.0,11625.523,45.4122,5.676525,-1112739.4,5176.991,99.064835,-0.0005053691,1.0527860440126076,,,0.7216737,,0.0036535263,0.003535556,14369.899,16.611906
large,90000,,-0.92985535,224712.47,-1.1463475,2048.008,2048.008,224804.02,-1.1468146,-1.1448805,1.0016893,0.9920588,1.0016893,1.285098,1761.61,20480000.0,11625.729,45.413002,5.6766253,-1112758.9,5177.082,91.254974,-0.00046552788,1.0522883366746216,,,0.72538376,,0.0034314673,0.0033333518,16153.985,11.8757
large,100000,,-0.93206596,222235.62,-1.1337122,2048.0073,2048.0073,222320.8,-1.1341467,-1.1322373,1.0016863,0.9920721,1.0016863,1.2846189,1761.6372,20480000.0,11625.55,45.412304,5.676538,-1112741.8,5177.0024,84.82723,-0.0004327374,1.0516884534350133,,,0.73093724,,0.0032455528,0.0031622935,17933.105,11.787454
large,110000,22.111204,-0.93372756,220312.95,-1.1239039,2048.0063,2048.0063,220392.53,-1.1243098,-1.1224247,1.0016795,0.99210465,1.0016795,1.2834508,1761.6359,20480000.0,11625.559,45.41234,5.6765423,-1112742.6,5177.0063,79.396965,-0.00040503548,1.0514131832753033,"b""    from __gin__ import dynamic_registration\n    import __main__ as train_script\n    import data.pile.tasks\n    from models.scalable_t5 import network\n    import seqio\n    from t5x import adafactor\n    from t5x import gin_utils\n    from t5x import models\n    from t5x import partitioning\n    from t5x import trainer\n    from t5x import utils\n    \n#### Macros:\n\n    BATCH_SIZE = 2048\n    DROPOUT_RATE = 0.0\n    EVAL_BATCH_SIZE = 256\n    LABEL_SMOOTHING = 0.0\n    LOSS_NORMALIZING_FACTOR = None\n    MIXTURE_OR_TASK_MODULE = None\n    MIXTURE_OR_TASK_NAME = 'pile_r_denoiser'\n    MODEL = @models.EncoderDecoderModel()\n    MODEL_DIR = 'gs://improved-t5/ckpts/v2_large_mlm/'\n    OPTIMIZER = @adafactor.Adafactor()\n    RANDOM_SEED = None\n    SAVING_PERIOD = 10000\n    SHUFFLE_TRAIN_EXAMPLES = True\n    TASK_FEATURE_LENGTHS = {'inputs': 512, 'targets': 114}\n    TRAIN_STEPS = 1000000\n    USE_CACHED_TASKS = False\n    USE_HARDWARE_RNG = False\n    VOCABULARY = @seqio.SentencePieceVocabulary()\n    Z_LOSS = 0.0001\n    \n#### Parameters for adafactor.Adafactor:\n\n    adafactor.Adafactor.decay_rate = 0.8\n    adafactor.Adafactor.logical_factor_rules = \\\n        @adafactor.standard_logical_factor_rules()\n    adafactor.Adafactor.step_offset = 0\n    \n#### Parameters for utils.CheckpointConfig:\n\n    utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()\n    utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()\n    \n#### Parameters for utils.create_learning_rate_scheduler:\n\n    utils.create_learning_rate_scheduler.base_learning_rate = 1.0\n    utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'\n    utils.create_learning_rate_scheduler.warmup_steps = 10000\n    \n#### Parameters for infer_eval/utils.DatasetConfig:\n\n    infer_eval/utils.DatasetConfig.batch_size = %EVAL_BATCH_SIZE\n    infer_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    infer_eval/utils.DatasetConfig.seed = 0\n    infer_eval/utils.DatasetConfig.shuffle = False\n    infer_eval/utils.DatasetConfig.split = 'validation'\n    infer_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    \n#### Parameters for train/utils.DatasetConfig:\n\n    train/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train/utils.DatasetConfig.pack = True\n    train/utils.DatasetConfig.seed = None\n    train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES\n    train/utils.DatasetConfig.split = 'train'\n    train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for train_eval/utils.DatasetConfig:\n\n    train_eval/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train_eval/utils.DatasetConfig.pack = True\n    train_eval/utils.DatasetConfig.seed = 42\n    train_eval/utils.DatasetConfig.shuffle = False\n    train_eval/utils.DatasetConfig.split = 'validation'\n    train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for models.EncoderDecoderModel:\n\n    models.EncoderDecoderModel.input_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.label_smoothing = %LABEL_SMOOTHING\n    models.EncoderDecoderModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR\n    models.EncoderDecoderModel.module = @network.Transformer()\n    models.EncoderDecoderModel.optimizer_def = %OPTIMIZER\n    models.EncoderDecoderModel.output_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.z_loss = %Z_LOSS\n    \n#### Parameters for seqio.Evaluator:\n\n    seqio.Evaluator.logger_cls = \\\n        [@seqio.PyLoggingLogger, @seqio.TensorBoardLogger, @seqio.JSONLogger]\n    seqio.Evaluator.num_examples = 2048\n    \n#### Parameters for partitioning.PjitPartitioner:\n\n    partitioning.PjitPartitioner.logical_axis_rules = \\\n        @partitioning.standard_logical_axis_rules()\n    partitioning.PjitPartitioner.model_parallel_submesh = None\n    partitioning.PjitPartitioner.num_partitions = 1\n    \n#### Parameters for utils.RestoreCheckpointConfig:\n\n    utils.RestoreCheckpointConfig.path = []\n    \n#### Parameters for utils.SaveCheckpointConfig:\n\n    utils.SaveCheckpointConfig.dtype = 'float32'\n    utils.SaveCheckpointConfig.keep = None\n    utils.SaveCheckpointConfig.period = %SAVING_PERIOD\n    utils.SaveCheckpointConfig.save_dataset = False\n    \n#### Parameters for seqio.SentencePieceVocabulary:\n\n    seqio.SentencePieceVocabulary.extra_ids = 100\n    seqio.SentencePieceVocabulary.sentencepiece_model_file = \\\n        'gs://improved-t5/vocabs/tokenizer.model'\n    \n#### Parameters for network.T5Config:\n\n    network.T5Config.dropout_rate = %DROPOUT_RATE\n    network.T5Config.dtype = 'bfloat16'\n    network.T5Config.emb_dim = 1024\n    network.T5Config.head_dim = 64\n    network.T5Config.logits_via_embedding = False\n    network.T5Config.mlp_activations = ('gelu', 'linear')\n    network.T5Config.mlp_dim = 2816\n    network.T5Config.num_decoder_layers = 24\n    network.T5Config.num_encoder_layers = 24\n    network.T5Config.num_heads = 16\n    network.T5Config.remat_policy = 'minimal'\n    network.T5Config.scan_layers = True\n    network.T5Config.vocab_size = 32128\n    \n#### Parameters for train_script.train:\n\n    train_script.train.checkpoint_cfg = @utils.CheckpointConfig()\n    train_script.train.eval_period = %SAVING_PERIOD\n    train_script.train.eval_steps = 100\n    train_script.train.infer_eval_dataset_cfg = @infer_eval/utils.DatasetConfig()\n    train_script.train.inference_evaluator_cls = @seqio.Evaluator\n    train_script.train.model = %MODEL\n    train_script.train.model_dir = %MODEL_DIR\n    train_script.train.partitioner = @partitioning.PjitPartitioner()\n    train_script.train.random_seed = %RANDOM_SEED\n    train_script.train.run_eval_before_training = True\n    train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config\n    train_script.train.total_steps = %TRAIN_STEPS\n    train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()\n    train_script.train.train_eval_dataset_cfg = @train_eval/utils.DatasetConfig()\n    train_script.train.trainer_cls = @trainer.Trainer\n    train_script.train.use_hardware_rng = %USE_HARDWARE_RNG\n    \n#### Parameters for trainer.Trainer:\n\n    trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()\n    trainer.Trainer.num_microbatches = None\n    \n#### Parameters for network.Transformer:\n\n    network.Transformer.config = @network.T5Config()""",8.675981,5.1848326,6.4373016e-06,0.0030869492,0.0030151273,19712.143,11.203051
large,120000,29.025518,-0.93373036,220539.98,-1.125062,2048.006,2048.006,220616.38,-1.1254517,-1.1235404,1.0017012,0.99200267,1.0017012,1.287113,1762.2389,20480000.0,11621.58,45.396797,5.6745996,-1112361.9,5175.235,76.946594,-0.00039253515,1.0508781886071528,"b""    from __gin__ import dynamic_registration\n    import __main__ as train_script\n    import data.pile.tasks\n    from models.scalable_t5 import network\n    import seqio\n    from t5x import adafactor\n    from t5x import gin_utils\n    from t5x import models\n    from t5x import partitioning\n    from t5x import trainer\n    from t5x import utils\n    \n#### Macros:\n\n    BATCH_SIZE = 2048\n    DROPOUT_RATE = 0.0\n    EVAL_BATCH_SIZE = 256\n    LABEL_SMOOTHING = 0.0\n    LOSS_NORMALIZING_FACTOR = None\n    MIXTURE_OR_TASK_MODULE = None\n    MIXTURE_OR_TASK_NAME = 'pile_r_denoiser'\n    MODEL = @models.EncoderDecoderModel()\n    MODEL_DIR = 'gs://improved-t5/ckpts/v2_large_mlm/'\n    OPTIMIZER = @adafactor.Adafactor()\n    RANDOM_SEED = None\n    SAVING_PERIOD = 10000\n    SHUFFLE_TRAIN_EXAMPLES = True\n    TASK_FEATURE_LENGTHS = {'inputs': 512, 'targets': 114}\n    TRAIN_STEPS = 1000000\n    USE_CACHED_TASKS = False\n    USE_HARDWARE_RNG = False\n    VOCABULARY = @seqio.SentencePieceVocabulary()\n    Z_LOSS = 0.0001\n    \n#### Parameters for adafactor.Adafactor:\n\n    adafactor.Adafactor.decay_rate = 0.8\n    adafactor.Adafactor.logical_factor_rules = \\\n        @adafactor.standard_logical_factor_rules()\n    adafactor.Adafactor.step_offset = 0\n    \n#### Parameters for utils.CheckpointConfig:\n\n    utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()\n    utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()\n    \n#### Parameters for utils.create_learning_rate_scheduler:\n\n    utils.create_learning_rate_scheduler.base_learning_rate = 1.0\n    utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'\n    utils.create_learning_rate_scheduler.warmup_steps = 10000\n    \n#### Parameters for infer_eval/utils.DatasetConfig:\n\n    infer_eval/utils.DatasetConfig.batch_size = %EVAL_BATCH_SIZE\n    infer_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    infer_eval/utils.DatasetConfig.seed = 0\n    infer_eval/utils.DatasetConfig.shuffle = False\n    infer_eval/utils.DatasetConfig.split = 'validation'\n    infer_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    \n#### Parameters for train/utils.DatasetConfig:\n\n    train/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train/utils.DatasetConfig.pack = True\n    train/utils.DatasetConfig.seed = None\n    train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES\n    train/utils.DatasetConfig.split = 'train'\n    train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for train_eval/utils.DatasetConfig:\n\n    train_eval/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train_eval/utils.DatasetConfig.pack = True\n    train_eval/utils.DatasetConfig.seed = 42\n    train_eval/utils.DatasetConfig.shuffle = False\n    train_eval/utils.DatasetConfig.split = 'validation'\n    train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for models.EncoderDecoderModel:\n\n    models.EncoderDecoderModel.input_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.label_smoothing = %LABEL_SMOOTHING\n    models.EncoderDecoderModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR\n    models.EncoderDecoderModel.module = @network.Transformer()\n    models.EncoderDecoderModel.optimizer_def = %OPTIMIZER\n    models.EncoderDecoderModel.output_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.z_loss = %Z_LOSS\n    \n#### Parameters for seqio.Evaluator:\n\n    seqio.Evaluator.logger_cls = \\\n        [@seqio.PyLoggingLogger, @seqio.TensorBoardLogger, @seqio.JSONLogger]\n    seqio.Evaluator.num_examples = 2048\n    \n#### Parameters for partitioning.PjitPartitioner:\n\n    partitioning.PjitPartitioner.logical_axis_rules = \\\n        @partitioning.standard_logical_axis_rules()\n    partitioning.PjitPartitioner.model_parallel_submesh = None\n    partitioning.PjitPartitioner.num_partitions = 1\n    \n#### Parameters for utils.RestoreCheckpointConfig:\n\n    utils.RestoreCheckpointConfig.path = []\n    \n#### Parameters for utils.SaveCheckpointConfig:\n\n    utils.SaveCheckpointConfig.dtype = 'float32'\n    utils.SaveCheckpointConfig.keep = None\n    utils.SaveCheckpointConfig.period = %SAVING_PERIOD\n    utils.SaveCheckpointConfig.save_dataset = False\n    \n#### Parameters for seqio.SentencePieceVocabulary:\n\n    seqio.SentencePieceVocabulary.extra_ids = 100\n    seqio.SentencePieceVocabulary.sentencepiece_model_file = \\\n        'gs://improved-t5/vocabs/tokenizer.model'\n    \n#### Parameters for network.T5Config:\n\n    network.T5Config.dropout_rate = %DROPOUT_RATE\n    network.T5Config.dtype = 'bfloat16'\n    network.T5Config.emb_dim = 1024\n    network.T5Config.head_dim = 64\n    network.T5Config.logits_via_embedding = False\n    network.T5Config.mlp_activations = ('gelu', 'linear')\n    network.T5Config.mlp_dim = 2816\n    network.T5Config.num_decoder_layers = 24\n    network.T5Config.num_encoder_layers = 24\n    network.T5Config.num_heads = 16\n    network.T5Config.remat_policy = 'minimal'\n    network.T5Config.scan_layers = True\n    network.T5Config.vocab_size = 32128\n    \n#### Parameters for train_script.train:\n\n    train_script.train.checkpoint_cfg = @utils.CheckpointConfig()\n    train_script.train.eval_period = %SAVING_PERIOD\n    train_script.train.eval_steps = 100\n    train_script.train.infer_eval_dataset_cfg = @infer_eval/utils.DatasetConfig()\n    train_script.train.inference_evaluator_cls = @seqio.Evaluator\n    train_script.train.model = %MODEL\n    train_script.train.model_dir = %MODEL_DIR\n    train_script.train.partitioner = @partitioning.PjitPartitioner()\n    train_script.train.random_seed = %RANDOM_SEED\n    train_script.train.run_eval_before_training = True\n    train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config\n    train_script.train.total_steps = %TRAIN_STEPS\n    train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()\n    train_script.train.train_eval_dataset_cfg = @train_eval/utils.DatasetConfig()\n    train_script.train.trainer_cls = @trainer.Trainer\n    train_script.train.use_hardware_rng = %USE_HARDWARE_RNG\n    \n#### Parameters for trainer.Trainer:\n\n    trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()\n    trainer.Trainer.num_microbatches = None\n    \n#### Parameters for network.Transformer:\n\n    network.Transformer.config = @network.T5Config()""",5.898055,7.3206015,1.168251e-05,0.0029495438,0.0028867633,1906.333,10.315814
large,130000,,-0.93730813,216346.67,-1.1036704,2048.0066,2048.0066,216406.98,-1.103978,-1.1021174,1.0016881,0.9920635,1.0016881,1.2849247,4870.884,20480000.0,4204.5757,65.696495,2.0530155,-402441.78,7489.4004,59.656776,-0.0003043329,1.0503857952929987,,,0.7406728,,0.0028289973,0.002773512,5015.982,9.9238205
large,140000,,-0.9376992,215810.72,-1.1009362,2048.0068,2048.0068,215872.12,-1.1012495,-1.0993919,1.0016896,0.9920568,1.0016896,1.2851663,4863.87,20480000.0,4210.6387,65.79123,2.055976,-403022.12,7500.2,62.165432,-0.00031713056,1.0501784140369954,,,0.7651632,,0.0027221239,0.002672622,9903.783,10.421305
large,150000,,-0.9385385,214841.17,-1.0959901,2048.0066,2048.0066,214904.47,-1.0963131,-1.0944672,1.0016866,0.99207103,1.0016866,1.2846535,4864.2856,20480000.0,4210.2793,65.785614,2.0558004,-402987.72,7499.56,62.66457,-0.00031967685,1.050010923088225,,,0.7451408,,0.0026265252,0.0025819975,14792.517,14.254774
large,160000,,-0.9394166,213863.8,-1.0910041,2048.006,2048.006,213923.95,-1.0913111,-1.0894784,1.0016822,0.992092,1.0016822,1.2839022,4864.247,20480000.0,4210.3125,65.78613,2.0558167,-402990.9,7499.619,61.33117,-0.00031287465,1.049864387579811,,,0.7359617,,0.0025403376,0.0025000079,19685.037,12.117996
large,1520000,,-0.9584241,192652.02,-0.98279446,2048.0076,2048.0076,192671.0,-0.9828913,-0.98122793,1.0016952,0.9920312,1.0016952,1.2860886,6971.8276,20480000.0,2937.5369,45.899014,1.4343442,-281166.94,5232.4873,18.759153,-9.569789e-05,1.0444409778192385,,,0.8259685,,0.0008124503,0.0008111074,7142.9175,11.004472
large,1530000,,-0.95817995,192817.8,-0.9836402,2048.0076,2048.0076,192836.56,-0.9837359,-0.9820738,1.0016924,0.9920444,1.0016924,1.2856189,6970.8164,20480000.0,2937.963,45.90567,1.4345522,-281207.72,5233.2466,18.720743,-9.550195e-05,1.0443764589618936,,,0.8402934,,0.00080978975,0.0008084524,14149.458,368.84778
large,1540000,,-0.95847464,192517.17,-0.9821066,2048.007,2048.007,192536.05,-0.9822029,-0.98054457,1.0016912,0.99204963,1.0016912,1.2854261,6970.8213,20480000.0,2937.961,45.90564,1.4345512,-281207.53,5233.2427,18.68347,-9.53118e-05,1.044407373479728,,,0.821815,,0.00080713385,0.00080582325,21513.9,369.06653
large,1550000,,-0.9586496,192340.61,-0.9812059,2048.0068,2048.0068,192359.45,-0.981302,-0.9796427,1.0016938,0.9920371,1.0016938,1.2858733,6970.906,20480000.0,2937.925,45.90508,1.4345337,-281204.1,5233.179,18.671413,-9.5250296e-05,1.0444245646102968,,,0.837353,,0.0008045286,0.0008032196,28879.05,9.321275
large,1560000,,-0.9587107,192277.27,-0.98088276,2048.0056,2048.0056,192295.52,-0.9809758,-0.9793198,1.001691,0.9920505,1.001691,1.2853919,6970.8154,20480000.0,2937.9634,45.905678,1.4345524,-281207.75,5233.247,18.621643,-9.49964e-05,1.0443951843195916,,,0.83879733,,0.0008019294,0.000800641,35883.965,369.09628
large,1570000,,-0.9589295,192019.3,-0.97956675,2048.0068,2048.0068,192037.86,-0.9796614,-0.97800404,1.0016947,0.99203324,1.0016947,1.2860129,6970.817,20480000.0,2937.9626,45.905666,1.4345521,-281207.7,5233.246,18.572569,-9.474605e-05,1.0444253028780448,,,0.83301735,,0.0007993572,0.00079808716,43248.63,9.148456
large,1580000,,-0.9587577,192185.47,-0.98041445,2048.0066,2048.0066,192204.08,-0.98050934,-0.9788488,1.0016965,0.992025,1.0016965,1.286309,6970.9033,20480000.0,2937.9263,45.905098,1.4345343,-281204.22,5233.181,18.54982,-9.463e-05,1.0443799006688217,,,0.8275156,,0.0007968276,0.0007955575,50253.44,9.128114
large,1590000,,-0.95898026,191970.42,-0.97931737,2048.0076,2048.0076,191989.38,-0.97941405,-0.97776085,1.0016907,0.9920512,1.0016907,1.2853671,6970.819,20480000.0,2937.962,45.905655,1.4345517,-281207.6,5233.2446,18.506254,-9.4407755e-05,1.0443969817737921,,,0.8279722,,0.0007943035,0.00079305185,57258.203,369.69836
large,1600000,,-0.9590242,191933.11,-0.979127,2048.0068,2048.0068,191951.56,-0.97922117,-0.97756577,1.0016934,0.9920387,1.0016934,1.285814,6970.89,20480000.0,2937.9316,45.90518,1.4345369,-281204.72,5233.191,18.439034,-9.406484e-05,1.0443513800482611,,,0.84652734,,0.00079179695,0.00079056964,64623.535,369.27994
large,1000000,27.619028,-0.9561026,195077.0,-0.9951653,2048.008,2048.008,195098.14,-0.9952731,-0.99359494,1.001689,0.9920602,1.001689,1.2850466,4864.4424,20480000.0,4210.1436,65.78349,2.0557342,-402974.72,7499.3184,21.751732,-0.00011096422,1.0451699678286173,"b""    from __gin__ import dynamic_registration\n    import __main__ as train_script\n    import data.pile.tasks\n    from models.scalable_t5 import network\n    import seqio\n    from t5x import adafactor\n    from t5x import gin_utils\n    from t5x import models\n    from t5x import partitioning\n    from t5x import trainer\n    from t5x import utils\n    \n#### Macros:\n\n    BATCH_SIZE = 2048\n    DROPOUT_RATE = 0.0\n    EVAL_BATCH_SIZE = 256\n    LABEL_SMOOTHING = 0.0\n    LOSS_NORMALIZING_FACTOR = None\n    MIXTURE_OR_TASK_MODULE = None\n    MIXTURE_OR_TASK_NAME = 'pile_r_denoiser'\n    MODEL = @models.EncoderDecoderModel()\n    MODEL_DIR = 'gs://improved-t5/ckpts/v2_large_mlm/'\n    OPTIMIZER = @adafactor.Adafactor()\n    RANDOM_SEED = None\n    SAVING_PERIOD = 10000\n    SHUFFLE_TRAIN_EXAMPLES = True\n    TASK_FEATURE_LENGTHS = {'inputs': 512, 'targets': 114}\n    TRAIN_STEPS = 2000000\n    USE_CACHED_TASKS = False\n    USE_HARDWARE_RNG = False\n    VOCABULARY = @seqio.SentencePieceVocabulary()\n    Z_LOSS = 0.0001\n    \n#### Parameters for adafactor.Adafactor:\n\n    adafactor.Adafactor.decay_rate = 0.8\n    adafactor.Adafactor.logical_factor_rules = \\\n        @adafactor.standard_logical_factor_rules()\n    adafactor.Adafactor.step_offset = 0\n    \n#### Parameters for utils.CheckpointConfig:\n\n    utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()\n    utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()\n    \n#### Parameters for utils.create_learning_rate_scheduler:\n\n    utils.create_learning_rate_scheduler.base_learning_rate = 1.0\n    utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'\n    utils.create_learning_rate_scheduler.warmup_steps = 10000\n    \n#### Parameters for infer_eval/utils.DatasetConfig:\n\n    infer_eval/utils.DatasetConfig.batch_size = %EVAL_BATCH_SIZE\n    infer_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    infer_eval/utils.DatasetConfig.seed = 0\n    infer_eval/utils.DatasetConfig.shuffle = False\n    infer_eval/utils.DatasetConfig.split = 'validation'\n    infer_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    \n#### Parameters for train/utils.DatasetConfig:\n\n    train/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train/utils.DatasetConfig.pack = True\n    train/utils.DatasetConfig.seed = None\n    train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES\n    train/utils.DatasetConfig.split = 'train'\n    train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for train_eval/utils.DatasetConfig:\n\n    train_eval/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train_eval/utils.DatasetConfig.pack = True\n    train_eval/utils.DatasetConfig.seed = 42\n    train_eval/utils.DatasetConfig.shuffle = False\n    train_eval/utils.DatasetConfig.split = 'validation'\n    train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for models.EncoderDecoderModel:\n\n    models.EncoderDecoderModel.input_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.label_smoothing = %LABEL_SMOOTHING\n    models.EncoderDecoderModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR\n    models.EncoderDecoderModel.module = @network.Transformer()\n    models.EncoderDecoderModel.optimizer_def = %OPTIMIZER\n    models.EncoderDecoderModel.output_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.z_loss = %Z_LOSS\n    \n#### Parameters for seqio.Evaluator:\n\n    seqio.Evaluator.logger_cls = \\\n        [@seqio.PyLoggingLogger, @seqio.TensorBoardLogger, @seqio.JSONLogger]\n    seqio.Evaluator.num_examples = 2048\n    \n#### Parameters for partitioning.PjitPartitioner:\n\n    partitioning.PjitPartitioner.logical_axis_rules = \\\n        @partitioning.standard_logical_axis_rules()\n    partitioning.PjitPartitioner.model_parallel_submesh = None\n    partitioning.PjitPartitioner.num_partitions = 1\n    \n#### Parameters for utils.RestoreCheckpointConfig:\n\n    utils.RestoreCheckpointConfig.path = []\n    \n#### Parameters for utils.SaveCheckpointConfig:\n\n    utils.SaveCheckpointConfig.dtype = 'float32'\n    utils.SaveCheckpointConfig.keep = None\n    utils.SaveCheckpointConfig.period = %SAVING_PERIOD\n    utils.SaveCheckpointConfig.save_dataset = False\n    \n#### Parameters for seqio.SentencePieceVocabulary:\n\n    seqio.SentencePieceVocabulary.extra_ids = 100\n    seqio.SentencePieceVocabulary.sentencepiece_model_file = \\\n        'gs://improved-t5/vocabs/tokenizer.model'\n    \n#### Parameters for network.T5Config:\n\n    network.T5Config.dropout_rate = %DROPOUT_RATE\n    network.T5Config.dtype = 'bfloat16'\n    network.T5Config.emb_dim = 1024\n    network.T5Config.head_dim = 64\n    network.T5Config.logits_via_embedding = False\n    network.T5Config.mlp_activations = ('gelu', 'linear')\n    network.T5Config.mlp_dim = 2816\n    network.T5Config.num_decoder_layers = 24\n    network.T5Config.num_encoder_layers = 24\n    network.T5Config.num_heads = 16\n    network.T5Config.remat_policy = 'minimal'\n    network.T5Config.scan_layers = True\n    network.T5Config.vocab_size = 32128\n    \n#### Parameters for train_script.train:\n\n    train_script.train.checkpoint_cfg = @utils.CheckpointConfig()\n    train_script.train.eval_period = %SAVING_PERIOD\n    train_script.train.eval_steps = 100\n    train_script.train.infer_eval_dataset_cfg = @infer_eval/utils.DatasetConfig()\n    train_script.train.inference_evaluator_cls = @seqio.Evaluator\n    train_script.train.model = %MODEL\n    train_script.train.model_dir = %MODEL_DIR\n    train_script.train.partitioner = @partitioning.PjitPartitioner()\n    train_script.train.random_seed = %RANDOM_SEED\n    train_script.train.run_eval_before_training = True\n    train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config\n    train_script.train.total_steps = %TRAIN_STEPS\n    train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()\n    train_script.train.train_eval_dataset_cfg = @train_eval/utils.DatasetConfig()\n    train_script.train.trainer_cls = @trainer.Trainer\n    train_script.train.use_hardware_rng = %USE_HARDWARE_RNG\n    \n#### Parameters for trainer.Trainer:\n\n    trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()\n    trainer.Trainer.num_microbatches = None\n    \n#### Parameters for network.Transformer:\n\n    network.Transformer.config = @network.T5Config()""",7.5064497,0.7071836,1.1444092e-05,0.0010025135,0.0010000005,430394.72,9.848256
large,1010000,,-0.95602465,195246.08,-0.9960278,2048.007,2048.007,195268.55,-0.99614245,-0.9944627,1.0016891,0.9920595,1.0016891,1.2850705,4875.342,20480000.0,4200.7314,65.63643,2.0511384,-402073.84,7482.5522,21.73927,-0.000110900655,1.0451294778290399,,,0.7316687,,0.0009975114,0.0009950376,5020.37,9.276073
large,1020000,,-0.9557081,195494.7,-0.99729615,2048.008,2048.008,195516.56,-0.9974077,-0.9957245,1.0016904,0.99205285,1.0016904,1.2853068,4865.188,20480000.0,4209.498,65.77341,2.055419,-402912.97,7498.1685,21.666832,-0.00011053112,1.0451648011822918,,,0.7357807,,0.000992588,0.000990148,9908.86,8.542822
large,1030000,,-0.9560219,195188.22,-0.99573267,2048.006,2048.006,195209.14,-0.9958394,-0.99416035,1.001689,0.99206,1.001689,1.2850525,4865.246,20480000.0,4209.448,65.77263,2.0553946,-402908.16,7498.0796,21.53648,-0.000109866145,1.045129285892363,,,0.70487857,,0.0009877331,0.0009853297,14796.655,10.470716
large,1040000,,-0.9562758,194958.72,-0.99456185,2048.007,2048.007,194980.39,-0.9946724,-0.99300605,1.0016781,0.992111,1.0016781,1.2832208,4865.317,20480000.0,4209.3867,65.77167,2.0553646,-402902.28,7497.97,21.414564,-0.0001092442,1.0451077252891556,,,0.69113445,,0.0009829515,0.0009805812,19686.469,10.486617
large,1050000,,-0.95629215,194861.55,-0.9940662,2048.0059,2048.0059,194883.7,-0.9941792,-0.99250066,1.0016912,0.9920491,1.0016912,1.2854416,4865.095,20480000.0,4209.5786,65.774666,2.0554583,-402920.66,7498.312,21.364624,-0.000108989436,1.045150038702346,,,0.7042327,,0.0009782402,0.0009759005,24576.03,11.704806
large,1060000,,-0.9565106,194687.66,-0.99317914,2048.0068,2048.0068,194708.61,-0.99328595,-0.9916143,1.0016857,0.99207485,1.0016857,1.28452,4865.246,20480000.0,4209.448,65.77263,2.0553946,-402908.16,7498.0796,21.242636,-0.00010836712,1.0450907816565476,,,0.7096231,,0.0009735984,0.0009712863,29467.0,11.4313965
large,1070000,,-0.9566476,194474.17,-0.99209,2048.0059,2048.0059,194495.8,-0.9922003,-0.99052554,1.0016907,0.9920516,1.0016907,1.2853557,4865.2383,20480000.0,4209.4546,65.77273,2.0553977,-402908.78,7498.0913,21.177778,-0.00010803626,1.0450891971928602,,,0.7469356,,0.0009689927,0.000966737,34357.652,13.472142
large,1080000,,-0.95652366,194605.2,-0.99275845,2048.0078,2048.0078,194626.06,-0.9928649,-0.991195,1.0016847,0.9920799,1.0016847,1.2843374,4865.2275,20480000.0,4209.464,65.77287,2.0554023,-402909.7,7498.1074,21.07104,-0.000107491745,1.0450647053101971,,,0.72019506,,0.0009644823,0.0009622509,39250.383,10.284838
large,1090000,,-0.9564607,194660.98,-0.993043,2048.0088,2048.0088,194681.72,-0.9931488,-0.9914783,1.0016849,0.99207896,1.0016849,1.2843702,4865.2856,20480000.0,4209.414,65.772095,2.055378,-402904.88,7498.0186,20.999825,-0.00010712845,1.0450787050478136,,,0.69345164,,0.00096004637,0.0009578268,44139.973,22.529428
large,1100000,,-0.9568618,194222.69,-0.9908071,2048.0066,2048.0066,194243.33,-0.99091244,-0.98924285,1.0016878,0.99206567,1.0016878,1.2848479,4865.2334,20480000.0,4209.459,65.7728,2.0554,-402909.2,7498.0986,20.89366,-0.00010658686,1.0450275222995353,,,0.7530396,,0.0009556196,0.000953463,49041.715,13.079448
large,1110000,,-0.95696783,194133.95,-0.9903544,2048.0056,2048.0056,194154.06,-0.99045706,-0.9887995,1.0016763,0.992119,1.0016763,1.2829316,4865.153,20480000.0,4209.529,65.77389,2.055434,-402915.88,7498.2227,20.822191,-0.00010622227,1.0450107381587612,,,0.7554567,,0.00095131237,0.00094915845,53934.004,9.25229
large,1120000,,-0.95713246,193952.39,-0.9894282,2048.0068,2048.0068,193973.31,-0.989535,-0.98787045,1.0016849,0.9920792,1.0016849,1.284365,4865.2773,20480000.0,4209.421,65.7722,2.0553813,-402905.56,7498.0312,20.747477,-0.00010584112,1.044994885965942,,,0.6885972,,0.0009470065,0.00094491156,58822.566,11.103744
large,1130000,,-0.95712173,193957.22,-0.98945284,2048.008,2048.008,193977.52,-0.9895564,-0.9879007,1.001676,0.9921207,1.001676,1.2828726,4865.2095,20480000.0,4209.4795,65.77312,2.05541,-402911.2,7498.1357,20.631273,-0.00010524832,1.0449671815408619,,,0.6984389,,0.00094282895,0.0009407213,63712.867,10.88138
large,1140000,,-0.95723647,193814.45,-0.9887245,2048.0076,2048.0076,193834.5,-0.9888268,-0.98716617,1.0016823,0.99209154,1.0016823,1.2839196,4865.196,20480000.0,4209.4917,65.77331,2.0554159,-402912.34,7498.1567,20.565712,-0.00010491387,1.0449732980248077,,,0.697907,,0.00093864615,0.0009365862,68602.945,10.379129
large,1150000,,-0.9573348,193765.81,-0.98847634,2048.0068,2048.0068,193786.25,-0.98858064,-0.9869228,1.0016798,0.9921033,1.0016798,1.2834995,4865.2437,20480000.0,4209.45,65.77266,2.0553956,-402908.38,7498.083,20.488728,-0.00010452114,1.0449718069666185,,,0.70395136,,0.0009345193,0.00093250524,73492.58,9.875484
large,1160000,,-0.95680827,194321.66,-0.99131197,2048.0076,2048.0076,194342.25,-0.99141705,-0.98974437,1.00169,0.99205524,1.00169,1.285225,4865.288,20480000.0,4209.4116,65.77206,2.0553768,-402904.7,7498.0146,20.461538,-0.00010438244,1.044950521823608,,,0.7349272,,0.00093050546,0.0009284771,78381.73,10.698606
large,1170000,,-0.9567878,194275.1,-0.99107444,2048.008,2048.008,194295.19,-0.99117696,-0.98950994,1.0016847,0.9920802,1.0016847,1.2843275,4865.0864,20480000.0,4209.586,65.77478,2.055462,-402921.38,7498.325,20.366932,-0.000103899816,1.0449588150566795,,,0.7255006,,0.00092649757,0.0009245007,83271.67,10.696767
large,1180000,,-0.9570061,194059.31,-0.98997366,2048.0076,2048.0076,194079.33,-0.99007577,-0.9884102,1.0016851,0.9920772,1.0016851,1.2844307,4865.243,20480000.0,4209.4507,65.77267,2.0553958,-402908.4,7498.0835,20.320498,-0.00010366293,1.044901555502674,,,0.743937,,0.00092254323,0.00092057505,88161.66,9.617165
large,1190000,,-0.95722586,193874.86,-0.9890327,2048.008,2048.008,193895.58,-0.98913836,-0.98747617,1.0016832,0.99208724,1.0016832,1.2840778,4865.2417,20480000.0,4209.4517,65.77268,2.0553963,-402908.5,7498.086,20.258802,-0.000103348204,1.044888040491402,,,0.71273685,,0.0009186377,0.0009166989,93050.516,10.652051
large,1200000,,-0.9574419,193635.97,-0.98781395,2048.0068,2048.0068,193656.44,-0.98791844,-0.98625576,1.0016859,0.99207467,1.0016859,1.2845261,4865.2183,20480000.0,4209.472,65.773,2.0554063,-402910.44,7498.122,20.175694,-0.00010292423,1.0447814404131588,,,0.70824456,,0.0009147812,0.0009128713,97940.37,9.507149
large,1210000,,-0.9580606,192988.55,-0.98451126,2048.0068,2048.0068,193008.44,-0.98461276,-0.9829633,1.001678,0.9921108,1.001678,1.283225,4865.29,20480000.0,4209.41,65.77203,2.055376,-402904.5,7498.0117,19.899603,-0.000101515776,1.0447238550843918,,,0.7147188,,0.0009109817,0.00090909126,102829.13,8.798088
large,1220000,,-0.95821095,192796.12,-0.9835296,2048.0078,2048.0078,192815.84,-0.98363024,-0.98197454,1.0016861,0.9920732,1.0016861,1.2845771,4865.228,20480000.0,4209.4634,65.772865,2.055402,-402909.66,7498.107,19.860956,-0.00010131863,1.0446969249035467,,,0.7121184,,0.00090723217,0.0009053578,107717.164,9.693582
large,1230000,,-0.95803946,192982.22,-0.98447895,2048.0076,2048.0076,193002.2,-0.9845809,-0.9829323,1.0016772,0.99211514,1.0016772,1.2830709,4865.1455,20480000.0,4209.535,65.77399,2.055437,-402916.5,7498.2344,19.836714,-0.00010119496,1.0446722618778967,,,0.7080047,,0.0009035372,0.0009016701,112605.984,9.853797
large,1240000,,-0.95814264,192862.55,-0.9838685,2048.007,2048.007,192882.61,-0.9839708,-0.9823139,1.0016868,0.99207026,1.0016868,1.2846844,4865.2783,20480000.0,4209.42,65.77219,2.0553808,-402905.5,7498.0293,19.80645,-0.00010104057,1.0446978853480855,,,0.7282798,,0.00089985144,0.0008980269,117495.13,10.236881
large,1250000,,-0.9583909,192548.55,-0.98226666,2048.0076,2048.0076,192568.36,-0.9823677,-0.9807144,1.0016859,0.99207467,1.0016859,1.2845243,4865.199,20480000.0,4209.489,65.77326,2.0554144,-402912.03,7498.152,19.754047,-0.00010077325,1.044677774244239,,,0.72886395,,0.0008962217,0.0008944276,122384.58,11.472729
large,1260000,,-0.95849204,192455.31,-0.981791,2048.0068,2048.0068,192474.56,-0.9818892,-0.9802444,1.0016779,0.992112,1.0016779,1.2831868,4865.1904,20480000.0,4209.496,65.77338,2.055418,-402912.75,7498.165,19.703342,-0.00010051458,1.0446978950623589,,,0.75494146,,0.0008926151,0.00089087116,127275.24,10.540332
large,1270000,,-0.95859873,192389.6,-0.98145574,2048.0076,2048.0076,192409.25,-0.981556,-0.97991085,1.0016788,0.99210733,1.0016788,1.2833525,4865.2563,20480000.0,4209.439,65.772484,2.0553901,-402907.3,7498.0635,19.666912,-0.00010032873,1.0446837114073513,,,0.74926853,,0.00088913785,0.0008873568,132165.06,10.909595
large,1280000,,-0.95851505,192450.67,-0.98176736,2048.0068,2048.0068,192470.16,-0.9818667,-0.98022354,1.0016763,0.9921189,1.0016763,1.2829362,4865.2812,20480000.0,4209.4175,65.77215,2.0553796,-402905.25,7498.0254,19.617456,-0.000100076446,1.0446444661408358,,,0.7318406,,0.00088561507,0.00088388385,137055.28,11.11847
large,1290000,,-0.958292,192731.69,-0.9832009,2048.009,2048.009,192751.25,-0.98330075,-0.98165214,1.0016794,0.99210453,1.0016794,1.2834516,4865.092,20480000.0,4209.5815,65.77471,2.0554597,-402920.9,7498.317,19.571983,-9.984447e-05,1.0446278341949071,,,0.73567295,,0.00088214036,0.0008804512,141945.52,13.792179
large,1300000,,-0.95796186,193205.45,-0.9856178,2048.0063,2048.0063,193225.06,-0.9857178,-0.9840606,1.0016841,0.9920829,1.0016841,1.2842302,4865.2544,20480000.0,4209.441,65.772514,2.055391,-402907.47,7498.067,19.588917,-9.993085e-05,1.0446577001797552,,,0.73966694,,0.0008787652,0.00087705837,146838.58,11.176372
large,1310000,,-0.9578915,193193.28,-0.9855557,2048.0068,2048.0068,193213.03,-0.9856564,-0.9840019,1.0016814,0.9920947,1.0016814,1.2838011,4865.246,20480000.0,4209.448,65.77263,2.0553946,-402908.16,7498.0796,19.569363,-9.98311e-05,1.0446159024907704,,,0.7286639,,0.0008753525,0.0008737044,151729.02,11.602468
large,1320000,,-0.95777404,193234.84,-0.9857677,2048.0078,2048.0078,193254.25,-0.9858667,-0.9842038,1.0016896,0.9920575,1.0016896,1.2851464,4865.225,20480000.0,4209.4663,65.77291,2.0554035,-402909.9,7498.112,19.489014,-9.94212e-05,1.0445470424214878,,,0.7415402,,0.00087205716,0.0008703886,156619.84,10.301942
large,1330000,,-0.9580919,192937.8,-0.9842524,2048.007,2048.007,192956.89,-0.9843498,-0.9826906,1.0016885,0.992062,1.0016885,1.2849804,4865.2876,20480000.0,4209.412,65.772064,2.055377,-402904.72,7498.015,19.46266,-9.928676e-05,1.0446011723950563,,,0.69012237,,0.0008687168,0.00086711027,161509.45,10.820231
large,1340000,,-0.9581946,192853.45,-0.98382205,2048.0068,2048.0068,192872.23,-0.9839179,-0.98226094,1.0016869,0.9920701,1.0016869,1.2846934,4865.2256,20480000.0,4209.466,65.7729,2.0554032,-402909.84,7498.111,19.417414,-9.905594e-05,1.0445879082336258,,,0.71639466,,0.00086548785,0.0008638687,166399.5,9.896227
large,1350000,,-0.95827717,192769.42,-0.9833934,2048.008,2048.008,192788.77,-0.98349214,-0.9818365,1.0016862,0.99207294,1.0016862,1.2845885,4865.1484,20480000.0,4209.532,65.77394,2.0554357,-402916.25,7498.2295,19.348917,-9.8706514e-05,1.044569158852524,,,0.7629862,,0.000862281,0.00086066325,171288.53,15.73683
large,1360000,,-0.9585332,192441.73,-0.98172176,2048.0063,2048.0063,192460.77,-0.98181885,-0.98016703,1.0016853,0.9920777,1.0016853,1.2844176,4865.2803,20480000.0,4209.4185,65.77216,2.05538,-402905.3,7498.0264,19.289099,-9.840135e-05,1.0445438759960803,,,0.73679924,,0.00085906,0.00085749326,176183.58,10.517267
large,1370000,,-0.95851505,192438.81,-0.9817068,2048.0046,2048.0046,192458.42,-0.9818068,-0.9801522,1.0016881,0.99206436,1.0016881,1.2848997,4865.2026,20480000.0,4209.486,65.77322,2.055413,-402911.75,7498.1465,19.24098,-9.8155884e-05,1.0445817621827787,,,0.71661234,,0.0008559233,0.000854358,181073.31,9.668314
large,1380000,,-0.9581475,192807.05,-0.98358536,2048.007,2048.007,192826.19,-0.983683,-0.98202014,1.0016932,0.9920401,1.0016932,1.2857697,4865.1963,20480000.0,4209.491,65.7733,2.0554156,-402912.28,7498.156,19.208475,-9.799006e-05,1.0445552276202823,,,0.7560022,,0.0008528227,0.00085125683,185962.17,10.495001
large,1390000,,-0.95849043,192458.77,-0.98180866,2048.0059,2048.0059,192477.81,-0.98190576,-0.98024714,1.001692,0.99204594,1.001692,1.2855618,4865.245,20480000.0,4209.4487,65.77264,2.055395,-402908.22,7498.0806,19.165861,-9.7772674e-05,1.0445766542559722,,,0.7362261,,0.0008497159,0.00084818923,190851.95,10.972019
large,1400000,,-0.958636,192295.44,-0.9809754,2048.0056,2048.0056,192314.6,-0.98107314,-0.9794213,1.0016866,0.99207133,1.0016866,1.2846463,4865.2827,20480000.0,4209.416,65.772125,2.055379,-402905.12,7498.0225,19.12145,-9.7546115e-05,1.0445596972789348,,,0.75619054,,0.00084665354,0.0008451545,195742.2,11.002655
large,1410000,,-0.9588401,192105.77,-0.9800078,2048.007,2048.007,192124.92,-0.9801056,-0.9784598,1.001682,0.9920922,1.001682,1.2838944,4865.0894,20480000.0,4209.5835,65.77474,2.0554607,-402921.16,7498.321,19.065054,-9.725842e-05,1.0445504583775225,,,0.72734976,,0.00084364426,0.0008421522,200632.34,12.154915
large,1420000,,-0.95884734,192080.2,-0.9798774,2048.0063,2048.0063,192098.25,-0.9799695,-0.9783179,1.0016882,0.9920639,1.0016882,1.2849144,4865.249,20480000.0,4209.4453,65.77258,2.0553932,-402907.9,7498.0747,19.01222,-9.6988886e-05,1.0445499357262227,,,0.73923254,,0.0008406639,0.00083918165,205523.78,10.867577
large,1430000,,-0.958885,192029.08,-0.97961664,2048.0073,2048.0073,192048.11,-0.97971374,-0.9780681,1.0016825,0.9920901,1.0016825,1.2839712,4865.247,20480000.0,4209.4473,65.77261,2.0553942,-402908.1,7498.0776,18.963774,-9.674175e-05,1.0445110567403773,,,0.71674085,,0.0008377118,0.00083624234,210413.94,10.322149
large,1440000,,-0.959166,191757.44,-0.97823083,2048.008,2048.008,191776.6,-0.9783286,-0.9766895,1.0016782,0.99210995,1.0016782,1.2832556,4865.23,20480000.0,4209.462,65.77284,2.0554013,-402909.47,7498.104,18.930212,-9.657053e-05,1.0444655399474918,,,0.7385454,,0.0008347885,0.0008333336,215303.47,9.893663
large,1450000,,-0.95878017,192211.62,-0.9805479,2048.0076,2048.0076,192230.53,-0.98064435,-0.9789945,1.0016853,0.9920776,1.0016853,1.2844217,4865.2793,20480000.0,4209.4194,65.77218,2.0553806,-402905.4,7498.028,18.89817,-9.640708e-05,1.0444939101049115,,,0.7357526,,0.00083189586,0.0008304551,220192.62,13.673455
large,1460000,,-0.95850116,192451.36,-0.9817709,2048.0063,2048.0063,192470.44,-0.98186815,-0.9802141,1.0016874,0.992067,1.0016874,1.2847999,4865.2324,20480000.0,4209.46,65.77281,2.0554004,-402909.28,7498.1,18.906813,-9.645116e-05,1.0444961286572907,,,0.73856735,,0.0008290311,0.0008276061,225085.56,10.076275
large,1470000,,-0.95860016,192328.14,-0.9811422,2048.0068,2048.0068,192346.58,-0.98123634,-0.9795864,1.0016843,0.9920817,1.0016843,1.2842716,4865.1416,20480000.0,4209.5386,65.77404,2.0554388,-402916.8,7498.24,18.844797,-9.61348e-05,1.04450306383065,,,0.74293184,,0.0008261957,0.0008247863,229974.8,12.876269
large,1480000,,-0.9588927,192085.83,-0.97990614,2048.0068,2048.0068,192105.1,-0.98000443,-0.9783581,1.0016828,0.9920893,1.0016828,1.2840015,4865.2725,20480000.0,4209.4253,65.77227,2.0553834,-402905.97,7498.0386,18.81042,-9.595942e-05,1.0444490492061407,,,0.7241833,,0.00082338724,0.00082199514,234866.95,9.840263
large,1490000,,-0.9588725,192098.66,-0.9799716,2048.0063,2048.0063,192117.58,-0.9800681,-0.9784223,1.0016822,0.9920923,1.0016822,1.2838937,4865.213,20480000.0,4209.4766,65.77307,2.0554085,-402910.9,7498.1304,18.794567,-9.587855e-05,1.044415840722407,,,0.7388439,,0.0008206108,0.0008192322,239756.02,11.709503
large,1500000,,-0.9590656,191847.25,-0.9786891,2048.0083,2048.0083,191866.64,-0.97878796,-0.9771428,1.0016836,0.9920844,1.0016836,1.2841728,4865.189,20480000.0,4209.497,65.77339,2.0554185,-402912.88,7498.167,18.70864,-9.54402e-05,1.044496237157979,,,0.68983173,,0.00081786537,0.00081649685,244646.94,10.203625
large,170000,,-0.9399743,213131.23,-1.087267,2048.0068,2048.0068,213191.39,-1.087574,-1.0857372,1.0016917,0.9920469,1.0016917,1.2855221,4864.1514,20480000.0,4210.395,65.78742,2.055857,-402998.8,7499.7666,59.427414,-0.00030316284,1.0496364818974147,,,0.71937966,,0.0024621193,0.0024253633,24575.318,15.662793
large,180000,,-0.94071436,212356.0,-1.0833123,2048.0066,2048.0066,212413.33,-1.0836047,-1.0817809,1.0016859,0.9920743,1.0016859,1.2845389,4863.7266,20480000.0,4210.7627,65.79317,2.0560365,-403034.0,7500.4214,57.163338,-0.0002916129,1.04948817904456,,,0.713593,,0.0023907027,0.0023570291,29468.72,12.5363035
large,190000,,-0.94128615,211648.53,-1.0797032,2048.005,2048.005,211703.69,-1.0799847,-1.0781616,1.0016909,0.9920515,1.0016909,1.2853587,4864.9326,20480000.0,4209.719,65.77686,2.055527,-402934.1,7498.562,54.902596,-0.00028007993,1.0493748507671394,,,0.70292044,,0.0023251679,0.0022941635,34360.152,12.402022
large,200000,,-0.9415333,211307.22,-1.077962,2048.0063,2048.0063,211360.3,-1.0782328,-1.0764172,1.0016867,0.99207026,1.0016867,1.2846825,4865.617,20480000.0,4209.127,65.76761,2.0552378,-402877.4,7497.507,53.06949,-0.00027072855,1.0491850079913894,,,0.6978564,,0.002264747,0.0022360736,39252.117,11.248102
large,210000,,-0.9419395,210862.67,-1.0756942,2048.0059,2048.0059,210913.4,-1.075953,-1.0741408,1.0016872,0.99206823,1.0016872,1.2847558,4865.1597,20480000.0,4209.523,65.773796,2.0554311,-402915.3,7498.2124,51.386044,-0.00026214062,1.0490462260475524,,,0.73351526,,0.0022087973,0.002182184,44142.465,10.459005
large,220000,,-0.9427258,209955.45,-1.0710661,2048.0063,2048.0063,210004.95,-1.0713186,-1.0695144,1.001687,0.9920687,1.001687,1.284738,4865.007,20480000.0,4209.6553,65.77586,2.0554957,-402927.97,7498.448,49.822777,-0.0002541658,1.0488952775169447,,,0.6880894,,0.0021567962,0.002132012,49031.934,8.877553
large,230000,,-0.94328016,209349.97,-1.0679773,2048.0063,2048.0063,209398.1,-1.0682229,-1.0664343,1.0016772,0.9921149,1.0016772,1.2830787,4864.5483,20480000.0,4210.052,65.78206,2.0556893,-402965.94,7499.1543,48.30452,-0.00024642053,1.0488293882879214,,,0.75068426,,0.0021083185,0.0020851486,53919.312,12.127901
large,240000,,-0.9438154,208775.42,-1.0650463,2048.007,2048.007,208822.05,-1.0652841,-1.0634927,1.0016845,0.992081,1.0016845,1.2842999,4865.601,20480000.0,4209.141,65.76783,2.0552447,-402878.78,7497.532,46.705727,-0.00023826447,1.0486475280450402,,,0.7157817,,0.0020629603,0.0020412458,58811.05,9.075813
large,250000,,-0.94420916,208323.36,-1.0627402,2048.0068,2048.0068,208369.19,-1.062974,-1.0611962,1.0016752,0.9921243,1.0016752,1.2827433,4865.839,20480000.0,4208.935,65.76461,2.055144,-402859.06,7497.1655,45.441757,-0.00023181645,1.0485916191924454,,,0.7157314,,0.00202042,0.002000004,63699.945,9.505747
large,260000,,-0.944847,207584.52,-1.058971,2048.0066,2048.0066,207628.86,-1.0591973,-1.0574161,1.0016845,0.992081,1.0016845,1.2842997,4865.8145,20480000.0,4208.9565,65.764946,2.0551546,-402861.1,7497.2036,44.40079,-0.00022650606,1.0484419886976333,,,0.7152126,,0.001980391,0.001961165,68589.266,9.504051
large,270000,,-0.9453007,207131.67,-1.0566609,2048.0056,2048.0056,207175.44,-1.0568842,-1.05511,1.0016816,0.9920952,1.0016816,1.2837919,4865.853,20480000.0,4208.923,65.76442,2.055138,-402857.9,7497.144,43.482414,-0.00022182106,1.048309946487608,,,0.7268865,,0.0019426537,0.0019245044,73478.6,12.333115
large,280000,,-0.9451159,207313.6,-1.057589,2048.0056,2048.0056,207356.14,-1.057806,-1.0560181,1.001693,0.99204105,1.001693,1.2857345,4865.8296,20480000.0,4208.9434,65.76474,2.0551481,-402859.84,7497.18,42.51358,-0.00021687866,1.0481941494536322,,,0.7017553,,0.0019070015,0.0018898257,78370.766,10.290688
large,290000,,-0.9453335,206982.9,-1.055902,2048.0068,2048.0068,207024.02,-1.0561117,-1.0543339,1.0016862,0.9920733,1.0016862,1.2845768,4865.7803,20480000.0,4208.986,65.7654,2.0551689,-402863.9,7497.256,41.515396,-0.00021178652,1.0480747315585002,,,0.7122686,,0.001873255,0.0018569566,83260.8,9.852192
large,300000,,-0.9458407,206442.1,-1.053143,2048.0063,2048.0063,206482.2,-1.0533477,-1.0515753,1.0016855,0.99207544,1.0016855,1.284494,4865.857,20480000.0,4208.9194,65.764366,2.0551364,-402857.6,7497.138,40.520924,-0.00020671332,1.0479681354693415,,,0.71842813,,0.0018412152,0.001825745,88150.48,13.334953
large,310000,,-0.94644475,205859.9,-1.0501732,2048.0093,2048.0093,205899.9,-1.0503771,-1.0486133,1.0016822,0.99209267,1.0016822,1.2838836,4865.832,20480000.0,4208.941,65.7647,2.055147,-402859.62,7497.1763,39.797405,-0.00020302237,1.0479401820988772,,,0.6962795,,0.0018107702,0.0017960559,93043.61,10.847332
large,320000,,-0.9467795,205420.42,-1.0479312,2048.0076,2048.0076,205459.56,-1.0481308,-1.0463673,1.0016854,0.9920768,1.0016854,1.2844499,4865.7554,20480000.0,4209.0073,65.76574,2.0551794,-402865.97,7497.294,39.11428,-0.00019953748,1.0478262676290262,,,0.6902199,,0.001781797,0.0017677698,97934.17,9.783404
large,330000,,-0.9472074,204939.72,-1.0454788,2048.006,2048.006,204978.02,-1.0456742,-1.043919,1.0016814,0.992095,1.0016814,1.2837931,4865.6147,20480000.0,4209.129,65.76764,2.0552387,-402877.6,7497.5107,38.35024,-0.0001956398,1.047770110996911,,,0.7520151,,0.0017541745,0.0017407791,102823.52,14.8366585
large,340000,,-0.9475723,204539.66,-1.0434381,2048.005,2048.005,204577.45,-1.0436308,-1.0418726,1.0016875,0.9920664,1.0016875,1.284822,4865.5864,20480000.0,4209.154,65.76803,2.055251,-402879.97,7497.5547,37.748894,-0.0001925721,1.0475149639326116,,,0.6871216,,0.0017277872,0.0017149884,107717.96,12.25248
large,350000,,-0.947688,204398.38,-1.0427173,2048.0073,2048.0073,204435.17,-1.042905,-1.0411589,1.0016772,0.9921151,1.0016772,1.2830714,4866.2017,20480000.0,4208.6216,65.75971,2.054991,-402829.03,7496.607,37.11838,-0.0001893556,1.0475286893444338,,,0.7199607,,0.001702562,0.0016903109,112610.51,11.487502
large,360000,,-0.94839025,203657.38,-1.0389371,2048.007,2048.007,203694.36,-1.0391258,-1.0373768,1.0016861,0.9920735,1.0016861,1.2845684,4864.0303,20480000.0,4210.5005,65.78907,2.0559084,-403008.88,7499.9536,36.75456,-0.00018749961,1.0472795544381353,,,0.70879173,,0.001678393,0.001666669,117499.99,9.432852
large,370000,,-0.9492811,202662.77,-1.0338632,2048.0076,2048.0076,202698.23,-1.0340443,-1.0323049,1.0016849,0.9920792,1.0016849,1.2843639,4863.3174,20480000.0,4211.117,65.798706,2.0562096,-403067.94,7501.0527,35.770855,-0.00018248135,1.0471457134379483,,,0.6948738,,0.0016552461,0.001643992,122386.7,9.622206
large,380000,,-0.94988203,202032.86,-1.0306499,2048.007,2048.007,202068.12,-1.0308298,-1.0291028,1.0016781,0.9921109,1.0016781,1.2832247,4863.172,20480000.0,4211.2437,65.80068,2.0562713,-403080.0,7501.2773,35.089947,-0.00017900775,1.0470615266794177,,,0.7193413,,0.0016330457,0.0016222164,127273.445,9.039756
large,390000,,-0.9502024,201720.64,-1.0290571,2048.0051,2048.0051,201755.83,-1.0292366,-1.0275086,1.0016818,0.9920941,1.0016818,1.28383,4863.2188,20480000.0,4211.2026,65.80004,2.0562513,-403076.12,7501.205,34.905117,-0.00017806487,1.0469414434559747,,,0.6987226,,0.001611688,0.0016012837,132159.67,16.113213
large,400000,,-0.9505996,201260.14,-1.0267079,2048.0066,2048.0066,201295.19,-1.0268867,-1.0251677,1.0016768,0.992117,1.0016768,1.2830042,4864.382,20480000.0,4210.196,65.78431,2.0557597,-402979.72,7499.411,34.44176,-0.00017570109,1.0468115660993693,,,0.7298739,,0.0015911432,0.0015811408,137054.14,11.814676
large,410000,,-0.9505901,201324.56,-1.0270365,2048.0076,2048.0076,201358.4,-1.0272092,-1.025486,1.0016804,0.9920999,1.0016804,1.2836189,4863.8833,20480000.0,4210.6274,65.79105,2.0559704,-403021.03,7500.1797,34.00924,-0.00017349463,1.0468007950968765,,,0.7485099,,0.0015713765,0.0015617395,141943.83,10.759569
large,420000,,-0.950383,201644.64,-1.0286695,2048.0078,2048.0078,201678.27,-1.028841,-1.027113,1.0016824,0.9920909,1.0016824,1.2839428,4864.0103,20480000.0,4210.5176,65.78934,2.0559168,-403010.53,7499.9844,33.66714,-0.00017174946,1.0467730436275688,,,0.7155907,,0.0015523254,0.0015430354,146832.61,10.7632
large,430000,,-0.95022607,201714.16,-1.029024,2048.0063,2048.0063,201747.4,-1.0291936,-1.0274625,1.0016849,0.9920782,1.0016849,1.2843941,4863.992,20480000.0,4210.533,65.78958,2.0559244,-403012.03,7500.012,33.31367,-0.00016994627,1.04678616584032,,,0.7377343,,0.0015339577,0.0015249875,151721.34,9.55129
large,440000,,-0.95026463,201586.42,-1.0283724,2048.0068,2048.0068,201619.16,-1.0285394,-1.0268054,1.0016887,0.99206156,1.0016887,1.2850002,4863.937,20480000.0,4210.5806,65.79032,2.0559475,-403016.6,7500.097,32.82442,-0.00016745042,1.0467327828276376,,,0.74082303,,0.0015162186,0.0015075585,156608.84,10.070799
large,450000,,-0.95069253,201148.28,-1.0261374,2048.008,2048.008,201180.33,-1.0263008,-1.024572,1.0016873,0.9920676,1.0016873,1.2847791,4864.0176,20480000.0,4210.511,65.78924,2.0559137,-403009.9,7499.973,32.367973,-0.00016512189,1.046667418547271,,,0.7375839,,0.0014990865,0.0014907136,161496.94,14.24783
large,460000,,-0.95089805,200937.45,-1.0250617,2048.0088,2048.0088,200969.3,-1.0252242,-1.0235002,1.0016843,0.9920815,1.0016843,1.2842795,4863.9775,20480000.0,4210.546,65.78978,2.0559306,-403013.22,7500.0347,31.979393,-0.00016313959,1.0466071577302567,,,0.72040176,,0.0014825218,0.0014744212,166389.14,10.2901
large,470000,,-0.95107716,200726.22,-1.0239842,2048.0078,2048.0078,200758.7,-1.0241499,-1.0224282,1.001684,0.99208355,1.001684,1.284209,4863.8755,20480000.0,4210.6343,65.79116,2.0559738,-403021.7,7500.1924,31.650654,-0.00016146255,1.0465794919606277,,,0.7112744,,0.0014665002,0.0014586514,171277.28,13.440466
large,480000,,-0.9514455,200297.0,-1.0217946,2048.0068,2048.0068,200327.84,-1.0219519,-1.0202326,1.0016853,0.99207747,1.0016853,1.2844256,4864.0103,20480000.0,4210.517,65.78933,2.0559165,-403010.5,7499.984,31.179756,-0.00015906032,1.0465371064570157,,,0.71750045,,0.0014509773,0.0014433772,176168.7,10.713051
large,490000,,-0.9514866,200202.45,-1.0213122,2048.0066,2048.0066,200233.05,-1.0214683,-1.019747,1.0016879,0.9920655,1.0016879,1.284859,4863.9927,20480000.0,4210.5327,65.78957,2.0559242,-403011.97,7500.011,30.807043,-0.00015715897,1.0465353693473247,,,0.73971844,,0.0014359363,0.0014285729,181057.39,9.777354
large,500000,,-0.95127755,200398.62,-1.022313,2048.007,2048.007,200429.12,-1.0224686,-1.0207399,1.0016935,0.9920389,1.0016935,1.2858119,4863.9478,20480000.0,4210.5713,65.79018,2.055943,-403015.7,7500.08,30.456678,-0.00015537161,1.0465270658657477,,,0.7222173,,0.0014213494,0.001414215,185945.11,12.739551
large,510000,,-0.9516643,199986.88,-1.0202125,2048.0073,2048.0073,200016.72,-1.0203648,-1.0186405,1.0016927,0.9920434,1.0016927,1.2856529,4864.018,20480000.0,4210.5107,65.78923,2.0559134,-403009.88,7499.972,30.152798,-0.0001538214,1.046470861924128,,,0.723444,,0.0014072084,0.0014002814,190835.83,10.857158
large,520000,,-0.95177937,199853.2,-1.0195307,2048.0073,2048.0073,199882.77,-1.0196813,-1.0179629,1.0016881,0.99206394,1.0016881,1.2849118,4863.982,20480000.0,4210.542,65.78972,2.0559287,-403012.84,7500.028,29.841675,-0.00015223424,1.0464482359544696,,,0.7328429,,0.0013934822,0.0013867519,195724.64,10.997709
large,530000,,-0.9520228,199603.27,-1.0182555,2048.007,2048.007,199632.11,-1.0184027,-1.0166924,1.0016822,0.9920914,1.0016822,1.2839216,4863.877,20480000.0,4210.633,65.79114,2.055973,-403021.56,7500.1895,29.484072,-0.00015040996,1.0464561265611294,,,0.69938254,,0.0013801508,0.001373607,200613.53,11.607269
large,540000,,-0.95211416,199477.64,-1.0176147,2048.0063,2048.0063,199507.02,-1.0177644,-1.016049,1.0016884,0.99206287,1.0016884,1.28495,4864.0127,20480000.0,4210.5156,65.78931,2.0559158,-403010.34,7499.9805,29.179996,-0.00014885876,1.0464246703880526,,,0.72903466,,0.0013671798,0.001360829,205503.12,10.721286
large,550000,,-0.95220536,199380.1,-1.017117,2048.0078,2048.0078,199408.98,-1.0172645,-1.0155579,1.0016805,0.99209964,1.0016805,1.2836267,4863.9897,20480000.0,4210.535,65.78961,2.0559254,-403012.2,7500.0156,28.920202,-0.00014753344,1.0463523551294984,,,0.7176001,,0.001354586,0.001348401,210391.81,10.295914
large,560000,,-0.95244795,199103.19,-1.0157045,2048.0056,2048.0056,199132.03,-1.0158516,-1.0141447,1.0016831,0.99208754,1.0016831,1.2840631,4863.946,20480000.0,4210.573,65.79021,2.055944,-403015.84,7500.0835,28.616222,-0.00014598272,1.046361915703095,,,0.72003627,,0.0013423256,0.0013363075,215280.08,11.246511
large,570000,,-0.9521402,199501.38,-1.0177357,2048.0078,2048.0078,199529.64,-1.01788,-1.0161697,1.0016831,0.9920878,1.0016831,1.2840567,4864.0117,20480000.0,4210.516,65.789314,2.055916,-403010.38,7499.982,28.349562,-0.00014462238,1.0463182409723915,,,0.7230463,,0.0013303938,0.0013245335,220169.3,9.832163
large,580000,,-0.95193875,199644.17,-1.0184642,2048.0066,2048.0066,199672.14,-1.0186069,-1.0168924,1.001686,0.9920737,1.001686,1.2845601,4863.983,20480000.0,4210.541,65.7897,2.0559282,-403012.78,7500.026,28.123285,-0.00014346805,1.0463060527920596,,,0.74214315,,0.0013187655,0.0013130654,225057.11,10.656272
large,590000,,-0.9520722,199495.58,-1.0177062,2048.0066,2048.0066,199523.55,-1.0178488,-1.0161431,1.0016787,0.9921078,1.0016787,1.2833335,4863.8745,20480000.0,4210.635,65.79117,2.055974,-403021.78,7500.1934,27.827963,-0.00014196149,1.0462834888137855,,,0.71478677,,0.0013074477,0.0013018901,229945.64,10.71758
large,600000,,-0.95242256,199149.78,-1.0159421,2048.006,2048.006,199177.6,-1.0160841,-1.0143764,1.0016835,0.9920856,1.0016835,1.2841316,4864.02,20480000.0,4210.509,65.7892,2.0559125,-403009.7,7499.9688,27.6103,-0.0001408511,1.0462714944311886,,,0.7606065,,0.0012964226,0.0012909955,234834.34,10.396042
large,610000,,-0.9523211,199261.47,-1.0165119,2048.0066,2048.0066,199288.73,-1.0166509,-1.0149385,1.0016873,0.9920676,1.0016873,1.2847801,4863.995,20480000.0,4210.5303,65.789536,2.055923,-403011.75,7500.0073,27.474821,-0.00014015997,1.0462640712463944,,,0.72541404,,0.0012856664,0.0012803698,239722.77,10.661845
large,620000,,-0.95300233,198508.47,-1.0126705,2048.0078,2048.0078,198535.7,-1.0128095,-1.0111096,1.0016812,0.99209565,1.0016812,1.2837684,4863.9443,20480000.0,4210.5747,65.79023,2.0559447,-403016.0,7500.086,26.958988,-0.0001375285,1.0460921417219369,,,0.7430506,,0.0012751668,0.0012700022,244611.38,9.833347
large,630000,,-0.953293,198190.81,-1.01105,2048.0063,2048.0063,198217.64,-1.011187,-1.0094885,1.0016824,0.9920902,1.0016824,1.283965,4864.013,20480000.0,4210.515,65.7893,2.0559156,-403010.28,7499.98,26.737743,-0.00013639985,1.0460305674893697,,,0.7369497,,0.0012649208,0.0012598827,249499.23,9.671937
large,640000,,-0.95342064,198044.55,-1.0103039,2048.0066,2048.0066,198071.2,-1.0104399,-1.0087448,1.0016804,0.9921,1.0016804,1.2836151,4863.9824,20480000.0,4210.5415,65.78971,2.0559285,-403012.8,7500.027,26.567444,-0.00013553108,1.0460596464599867,,,0.713233,,0.0012549129,0.0012500009,254386.88,9.907664
large,650000,,-0.95337015,198081.89,-1.0104945,2048.0088,2048.0088,198108.14,-1.0106283,-1.008925,1.0016884,0.99206334,1.0016884,1.2849352,4863.8716,20480000.0,4210.6377,65.791214,2.0559754,-403022.03,7500.198,26.408224,-0.00013471884,1.0459815853943524,,,0.74395394,,0.0012451629,0.0012403483,259274.64,11.617893
large,660000,,-0.9535849,197834.23,-1.009231,2048.0063,2048.0063,197860.22,-1.0093635,-1.0076756,1.001675,0.99212515,1.001675,1.2827121,4864.003,20480000.0,4210.5234,65.78943,2.0559196,-403011.1,7499.995,26.25303,-0.00013392714,1.0459771134575184,,,0.7236359,,0.0012356176,0.0012309158,264164.28,10.376928
large,670000,,-0.95383906,197593.0,-1.0080004,2048.0083,2048.0083,197618.98,-1.0081329,-1.0064489,1.0016732,0.99213356,1.0016732,1.2824086,4863.986,20480000.0,4210.5386,65.789665,2.055927,-403012.53,7500.0215,26.055859,-0.00013292128,1.0459656348856283,,,0.6884515,,0.0012262804,0.0012216953,269052.62,11.369381
large,680000,,-0.953923,197484.53,-1.007447,2048.0068,2048.0068,197510.47,-1.0075793,-1.0058929,1.0016766,0.99211776,1.0016766,1.2829764,4863.9443,20480000.0,4210.5747,65.79023,2.0559447,-403016.0,7500.086,25.890718,-0.00013207884,1.0459492479815042,,,0.731889,,0.0012171788,0.001212679,273941.88,9.573698
large,690000,,-0.95399356,197425.08,-1.0071437,2048.0088,2048.0088,197450.94,-1.0072756,-1.0055908,1.0016755,0.99212337,1.0016755,1.2827773,4864.0054,20480000.0,4210.5215,65.7894,2.0559187,-403010.9,7499.9917,25.68639,-0.00013103648,1.0459106997198169,,,0.7169175,,0.0012082481,0.0012038593,278829.44,24.77444
large,700000,,-0.9538606,197526.3,-1.0076602,2048.0066,2048.0066,197551.97,-1.007791,-1.006091,1.0016898,0.99205613,1.0016898,1.2851927,4863.984,20480000.0,4210.54,65.78969,2.0559278,-403012.7,7500.0244,25.542774,-0.00013030383,1.0459348316992925,,,0.7054846,,0.0011995289,0.0011952295,283732.2,11.130489
large,710000,,-0.9535184,197975.42,-1.0099512,2048.0078,2048.0078,198001.11,-1.0100824,-1.0083758,1.0016924,0.99204445,1.0016924,1.2856154,4864.244,20480000.0,4210.315,65.78617,2.0558178,-402991.16,7499.6235,25.39849,-0.0001295678,1.0458623986110709,,,0.749109,,0.0011909944,0.0011867824,288621.5,10.131172
large,720000,,-0.95377856,197747.92,-1.0087906,2048.0083,2048.0083,197772.97,-1.0089184,-1.0072262,1.00168,0.9921013,1.00168,1.2835661,4864.0176,20480000.0,4210.511,65.78924,2.0559137,-403009.9,7499.9727,25.246403,-0.00012879193,1.0458784221450739,,,0.7118678,,0.0011826245,0.0011785121,293509.7,10.014578
large,730000,,-0.9533217,198131.5,-1.0107476,2048.008,2048.008,198156.64,-1.0108757,-1.0091728,1.0016874,0.9920669,1.0016874,1.284803,4863.9863,20480000.0,4210.538,65.78966,2.0559268,-403012.5,7500.021,25.081047,-0.00012794838,1.045883223889849,,,0.72425795,,0.0011744511,0.0011704123,298397.66,10.651914
large,740000,,-0.953461,197956.67,-1.0098556,2048.0056,2048.0056,197981.7,-1.0099833,-1.0082808,1.0016886,0.9920619,1.0016886,1.2849845,4863.949,20480000.0,4210.5703,65.79016,2.0559425,-403015.56,7500.078,24.927185,-0.00012716347,1.045843265192776,,,0.6989918,,0.0011664189,0.0011624771,303286.25,10.394623
large,750000,,-0.95384943,197609.81,-1.0080862,2048.006,2048.006,197634.52,-1.0082122,-1.0065192,1.001682,0.99209195,1.001682,1.2839013,4864.0166,20480000.0,4210.512,65.78925,2.0559142,-403010.0,7499.9746,24.760632,-0.00012631381,1.0457772146006474,,,0.7182472,,0.0011585751,0.0011547012,308174.62,10.522992
large,760000,,-0.95395434,197454.36,-1.0072931,2048.007,2048.007,197479.1,-1.0074192,-1.0057236,1.001686,0.99207324,1.001686,1.2845733,4864.223,20480000.0,4210.333,65.78645,2.0558267,-402992.88,7499.656,24.594635,-0.00012546699,1.0457800529553187,,,0.7419517,,0.0011508791,0.0011470794,313063.34,10.555599
large,770000,,-0.9541407,197255.95,-1.0062809,2048.008,2048.008,197279.42,-1.0064007,-1.0047033,1.0016894,0.99205774,1.0016894,1.2851349,4863.886,20480000.0,4210.625,65.791016,2.0559692,-403020.8,7500.1753,24.430166,-0.00012462797,1.045729671746714,,,0.71649265,,0.0011433268,0.0011396065,317951.8,11.045454
large,780000,,-0.95437044,196994.4,-1.0049467,2048.0078,2048.0078,197018.38,-1.0050689,-1.0033767,1.0016866,0.992072,1.0016866,1.284627,4864.1973,20480000.0,4210.3555,65.786804,2.0558376,-402995.03,7499.6963,24.27775,-0.00012385045,1.0457745350196312,,,0.7638619,,0.0011359173,0.0011322778,322841.06,9.992152
large,790000,,-0.95427966,197042.17,-1.0051904,2048.0066,2048.0066,197066.06,-1.0053122,-1.0036129,1.0016932,0.9920401,1.0016932,1.2857703,4864.1904,20480000.0,4210.362,65.7869,2.0558407,-402995.6,7499.7065,24.131151,-0.00012310258,1.045693007933873,,,0.7090447,,0.0011286756,0.0011250885,327729.28,10.215113
large,800000,,-0.9540696,197268.69,-1.0063459,2048.0063,2048.0063,197293.23,-1.0064712,-1.0047729,1.0016903,0.99205375,1.0016903,1.2852756,4864.053,20480000.0,4210.4805,65.78876,2.0558987,-403006.97,7499.918,24.01762,-0.00012252342,1.0457257036844763,,,0.7243166,,0.001121544,0.0011180347,332617.53,10.014725
large,810000,,-0.95452565,196758.0,-1.0037407,2048.007,2048.007,196782.05,-1.0038633,-1.0021727,1.001687,0.99206936,1.001687,1.2847176,4864.1216,20480000.0,4210.4214,65.787834,2.0558698,-403001.3,7499.813,23.862919,-0.00012173423,1.0457152530824563,,,0.7111287,,0.0011145591,0.0011111118,337505.8,10.877033
large,820000,,-0.95472443,196570.6,-1.0027847,2048.0078,2048.0078,196594.78,-1.002908,-1.001228,1.001678,0.9921107,1.001678,1.2832277,4864.0923,20480000.0,4210.4463,65.78822,2.055882,-403003.72,7499.8574,23.720985,-0.00012101017,1.0456453188250234,,,0.69907,,0.001107703,0.001104316,342394.75,10.845673
large,830000,,-0.95485353,196420.78,-1.0020205,2048.0073,2048.0073,196443.58,-1.0021367,-1.0004487,1.0016873,0.99206823,1.0016873,1.2847586,4863.9873,20480000.0,4210.537,65.78964,2.0559263,-403012.4,7500.0195,23.55371,-0.00012015684,1.045571661924047,,,0.7072623,,0.0011009688,0.0010976433,347283.56,11.140193
large,840000,,-0.95517343,196118.84,-1.0004802,2048.007,2048.007,196142.62,-1.0006014,-0.9989272,1.0016761,0.9921205,1.0016761,1.2828784,4864.1226,20480000.0,4210.4204,65.78782,2.0558693,-403001.22,7499.8115,23.332695,-0.00011902935,1.0455087446093514,,,0.6927483,,0.0010943569,0.0010910901,352172.78,10.548103
large,850000,,-0.95555276,195665.55,-0.9981677,2048.007,2048.007,195688.23,-0.9982834,-0.9966125,1.0016766,0.9921179,1.0016766,1.282972,4864.1094,20480000.0,4210.4316,65.787994,2.0558748,-403002.3,7499.831,23.220587,-0.00011845743,1.0454732149694665,,,0.72457194,,0.0010878694,0.001084653,357061.38,10.184683
large,860000,,-0.95548546,195781.42,-0.99875873,2048.0078,2048.0078,195804.62,-0.9988771,-0.997201,1.0016809,0.9920986,1.0016809,1.2836679,4864.284,20480000.0,4210.2803,65.78563,2.055801,-402987.84,7499.562,23.103455,-0.0001178599,1.045437073966455,,,0.74410415,,0.0010814735,0.0010783284,361949.84,10.882612
large,870000,,-0.955291,195991.89,-0.99983245,2048.0073,2048.0073,196015.34,-0.99995214,-0.9982619,1.0016931,0.99204063,1.0016931,1.2857509,4864.115,20480000.0,4210.427,65.78792,2.0558724,-403001.84,7499.8228,23.017937,-0.00011742364,1.0454155829980654,,,0.69762874,,0.0010752105,0.0010721132,366838.88,10.355066
large,880000,,-0.9551322,196145.38,-1.0006155,2048.0073,2048.0073,196168.61,-1.000734,-0.9990544,1.0016812,0.9920964,1.0016812,1.2837459,4864.0967,20480000.0,4210.443,65.78817,2.0558803,-403003.38,7499.851,22.911165,-0.00011687895,1.0454202371509125,,,0.6935501,,0.0010690391,0.0010660042,371727.28,9.750011
large,890000,,-0.9553346,195921.03,-0.99947095,2048.0076,2048.0076,195943.5,-0.9995856,-0.99790585,1.0016832,0.99208593,1.0016832,1.2841161,4863.9927,20480000.0,4210.5327,65.78957,2.0559242,-403011.97,7500.011,22.800644,-0.00011631514,1.045388131753481,,,0.72871447,,0.0010630098,0.0010599985,376614.97,11.1268215
large,900000,,-0.9556024,195690.72,-0.99829614,2048.0068,2048.0068,195713.66,-0.9984131,-0.9967341,1.0016845,0.9920806,1.0016845,1.2843097,4864.131,20480000.0,4210.413,65.787704,2.0558658,-403000.53,7499.7983,22.698814,-0.000115795665,1.0453959716146388,,,0.71308184,,0.0010570436,0.0010540931,381504.22,10.377804
large,910000,,-0.9556873,195569.19,-0.99767613,2048.007,2048.007,195591.77,-0.9977913,-0.9961122,1.0016856,0.99207556,1.0016856,1.2844945,4864.095,20480000.0,4210.444,65.788185,2.0558808,-403003.47,7499.853,22.615465,-0.00011537047,1.0453796649796483,,,0.7464161,,0.0010511758,0.0010482854,386392.66,10.339919
large,920000,,-0.9558966,195301.56,-0.9963108,2048.0059,2048.0059,195324.1,-0.9964258,-0.9947496,1.001685,0.9920783,1.001685,1.2843956,4864.2617,20480000.0,4210.3,65.785934,2.0558105,-402989.66,7499.596,22.459883,-0.00011457678,1.0453754565653943,,,0.7240107,,0.0010454139,0.0010425727,391281.28,10.5272875
large,930000,,-0.95601463,195194.83,-0.99576634,2048.007,2048.007,195217.08,-0.9958798,-0.99421144,1.0016781,0.99211085,1.0016781,1.2832265,4864.4717,20480000.0,4210.1177,65.78309,2.0557215,-402972.28,7499.2725,22.403818,-0.00011429078,1.0453091660846314,,,0.69738364,,0.0010397567,0.0010369522,396170.28,10.269809
large,940000,,-0.95611864,195077.3,-0.9951668,2048.0068,2048.0068,195099.64,-0.99528074,-0.9936116,1.0016799,0.99210215,1.0016799,1.2835361,4864.437,20480000.0,4210.148,65.78356,2.0557363,-402975.2,7499.326,22.293242,-0.00011372667,1.045297615431751,,,0.7381103,,0.001034198,0.0010314218,401058.94,12.231586
large,950000,,-0.9557366,195450.03,-0.9970682,2048.0066,2048.0066,195471.78,-0.9971792,-0.9955087,1.0016781,0.9921109,1.0016781,1.2832233,4864.3447,20480000.0,4210.228,65.78481,2.0557754,-402982.8,7499.4688,22.236454,-0.00011343698,1.0453093235624016,,,0.7083366,,0.0010286918,0.0010259789,405949.5,10.503241
large,960000,,-0.95628715,194853.38,-0.99402446,2048.0078,2048.0078,194875.88,-0.99413925,-0.9924769,1.001675,0.992126,1.001675,1.2826854,4864.4688,20480000.0,4210.1206,65.783134,2.055723,-402972.56,7499.2773,22.11912,-0.00011283841,1.045248100567799,,,0.74713564,,0.0010232817,0.0010206213,410838.44,10.326242
large,970000,,-0.95627165,194843.02,-0.9939716,2048.007,2048.007,194864.64,-0.9940819,-0.99240875,1.001686,0.9920742,1.001686,1.2845448,4864.4443,20480000.0,4210.1416,65.78346,2.0557332,-402974.56,7499.315,22.02353,-0.000112350775,1.0452618235843185,,,0.7296953,,0.0010179855,0.0010153466,415727.3,10.546967
large,980000,,-0.9565282,194624.0,-0.9928543,2048.0088,2048.0088,194646.53,-0.9929693,-0.99131113,1.0016726,0.9921363,1.0016726,1.282312,4864.4062,20480000.0,4210.175,65.78398,2.0557494,-402977.72,7499.374,21.895218,-0.00011169621,1.0452237125768682,,,0.7010722,,0.0010127369,0.001010153,420616.25,11.205097
large,990000,,-0.9562183,194959.42,-0.9945654,2048.0066,2048.0066,194980.92,-0.9946751,-0.99300414,1.0016828,0.9920894,1.0016828,1.2839973,4864.467,20480000.0,4210.122,65.78316,2.0557237,-402972.7,7499.2803,21.818157,-0.00011130309,1.0452008530608887,,,0.69550204,,0.0010075895,0.0010050383,425505.9,10.438713
base,0,27.27255,2.8265275e-05,2547856.0,10.912897,2048.0,2048.0,2550615.8,10.924718,10.939886,0.9986135,0.99859595,0.9986135,0.9985992,102.868835,204800.0,1990.8848,15.553787,0.9721117,226960.88,1773.1318,2758.8806,0.011816751,1.8357067157117084,"b""    from __gin__ import dynamic_registration\n    import __main__ as train_script\n    import data.pile.tasks\n    from models.scalable_t5 import network\n    import seqio\n    from t5x import adafactor\n    from t5x import gin_utils\n    from t5x import models\n    from t5x import partitioning\n    from t5x import trainer\n    from t5x import utils\n    \n#### Macros:\n\n    BATCH_SIZE = 2048\n    DROPOUT_RATE = 0.0\n    EVAL_BATCH_SIZE = 128\n    LABEL_SMOOTHING = 0.0\n    LOSS_NORMALIZING_FACTOR = None\n    MIXTURE_OR_TASK_MODULE = None\n    MIXTURE_OR_TASK_NAME = 'pile_r_denoiser'\n    MODEL = @models.EncoderDecoderModel()\n    MODEL_DIR = 'gs://improved-t5/ckpts/v2_base_mlm/'\n    OPTIMIZER = @adafactor.Adafactor()\n    RANDOM_SEED = None\n    SAVING_PERIOD = 10000\n    SHUFFLE_TRAIN_EXAMPLES = True\n    TASK_FEATURE_LENGTHS = {'inputs': 512, 'targets': 114}\n    TRAIN_STEPS = 1000000\n    USE_CACHED_TASKS = False\n    USE_HARDWARE_RNG = False\n    VOCABULARY = @seqio.SentencePieceVocabulary()\n    Z_LOSS = 0.0001\n    \n#### Parameters for adafactor.Adafactor:\n\n    adafactor.Adafactor.decay_rate = 0.8\n    adafactor.Adafactor.logical_factor_rules = \\\n        @adafactor.standard_logical_factor_rules()\n    adafactor.Adafactor.step_offset = 0\n    \n#### Parameters for utils.CheckpointConfig:\n\n    utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()\n    utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()\n    \n#### Parameters for utils.create_learning_rate_scheduler:\n\n    utils.create_learning_rate_scheduler.base_learning_rate = 1.0\n    utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'\n    utils.create_learning_rate_scheduler.warmup_steps = 10000\n    \n#### Parameters for infer_eval/utils.DatasetConfig:\n\n    infer_eval/utils.DatasetConfig.batch_size = %EVAL_BATCH_SIZE\n    infer_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    infer_eval/utils.DatasetConfig.seed = 0\n    infer_eval/utils.DatasetConfig.shuffle = False\n    infer_eval/utils.DatasetConfig.split = 'validation'\n    infer_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    \n#### Parameters for train/utils.DatasetConfig:\n\n    train/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train/utils.DatasetConfig.pack = True\n    train/utils.DatasetConfig.seed = None\n    train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES\n    train/utils.DatasetConfig.split = 'train'\n    train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for train_eval/utils.DatasetConfig:\n\n    train_eval/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train_eval/utils.DatasetConfig.pack = True\n    train_eval/utils.DatasetConfig.seed = 42\n    train_eval/utils.DatasetConfig.shuffle = False\n    train_eval/utils.DatasetConfig.split = 'validation'\n    train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for models.EncoderDecoderModel:\n\n    models.EncoderDecoderModel.input_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.label_smoothing = %LABEL_SMOOTHING\n    models.EncoderDecoderModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR\n    models.EncoderDecoderModel.module = @network.Transformer()\n    models.EncoderDecoderModel.optimizer_def = %OPTIMIZER\n    models.EncoderDecoderModel.output_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.z_loss = %Z_LOSS\n    \n#### Parameters for seqio.Evaluator:\n\n    seqio.Evaluator.logger_cls = \\\n        [@seqio.PyLoggingLogger, @seqio.TensorBoardLogger, @seqio.JSONLogger]\n    seqio.Evaluator.num_examples = 2048\n    \n#### Parameters for partitioning.PjitPartitioner:\n\n    partitioning.PjitPartitioner.logical_axis_rules = \\\n        @partitioning.standard_logical_axis_rules()\n    partitioning.PjitPartitioner.model_parallel_submesh = None\n    partitioning.PjitPartitioner.num_partitions = 1\n    \n#### Parameters for utils.RestoreCheckpointConfig:\n\n    utils.RestoreCheckpointConfig.path = []\n    \n#### Parameters for utils.SaveCheckpointConfig:\n\n    utils.SaveCheckpointConfig.dtype = 'float32'\n    utils.SaveCheckpointConfig.keep = None\n    utils.SaveCheckpointConfig.period = %SAVING_PERIOD\n    utils.SaveCheckpointConfig.save_dataset = False\n    \n#### Parameters for seqio.SentencePieceVocabulary:\n\n    seqio.SentencePieceVocabulary.extra_ids = 100\n    seqio.SentencePieceVocabulary.sentencepiece_model_file = \\\n        'gs://improved-t5/vocabs/tokenizer.model'\n    \n#### Parameters for network.T5Config:\n\n    network.T5Config.dropout_rate = %DROPOUT_RATE\n    network.T5Config.dtype = 'bfloat16'\n    network.T5Config.emb_dim = 768\n    network.T5Config.head_dim = 64\n    network.T5Config.logits_via_embedding = False\n    network.T5Config.mlp_activations = ('gelu', 'linear')\n    network.T5Config.mlp_dim = 2048\n    network.T5Config.num_decoder_layers = 12\n    network.T5Config.num_encoder_layers = 12\n    network.T5Config.num_heads = 12\n    network.T5Config.remat_policy = 'minimal'\n    network.T5Config.scan_layers = True\n    network.T5Config.vocab_size = 32128\n    \n#### Parameters for train_script.train:\n\n    train_script.train.checkpoint_cfg = @utils.CheckpointConfig()\n    train_script.train.eval_period = %SAVING_PERIOD\n    train_script.train.eval_steps = 100\n    train_script.train.infer_eval_dataset_cfg = @infer_eval/utils.DatasetConfig()\n    train_script.train.inference_evaluator_cls = @seqio.Evaluator\n    train_script.train.model = %MODEL\n    train_script.train.model_dir = %MODEL_DIR\n    train_script.train.partitioner = @partitioning.PjitPartitioner()\n    train_script.train.random_seed = %RANDOM_SEED\n    train_script.train.run_eval_before_training = True\n    train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config\n    train_script.train.total_steps = %TRAIN_STEPS\n    train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()\n    train_script.train.train_eval_dataset_cfg = @train_eval/utils.DatasetConfig()\n    train_script.train.trainer_cls = @trainer.Trainer\n    train_script.train.use_hardware_rng = %USE_HARDWARE_RNG\n    \n#### Parameters for trainer.Trainer:\n\n    trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()\n    trainer.Trainer.num_microbatches = None\n    \n#### Parameters for network.Transformer:\n\n    network.Transformer.config = @network.T5Config()""",4.824829,5.1685705,2.4318695e-05,,,,
base,10000,,-0.74585724,464945.34,-2.371871,2048.0063,2048.0063,467504.44,-2.3849258,-2.3808968,1.0016923,0.99204415,1.0016923,1.2856205,945.6887,20480000.0,21656.176,169.18887,10.574305,-2072825.1,19287.531,2560.6663,-0.013062975,1.0811766372641582,,,0.7040994,,0.010000295,0.01,1102.2042,7.8400793
base,20000,,-0.85525477,314959.22,-1.6067321,2048.0056,2048.0056,315454.1,-1.6092565,-1.6065288,1.0016979,0.99201775,1.0016979,1.28657,945.6435,20480000.0,21657.21,169.19696,10.57481,-2072924.2,19288.453,494.9748,-0.002525063,1.0710428106002168,,,0.7290075,,0.008284418,0.0070712445,2059.3196,8.528565
base,30000,,-0.87457335,291057.2,-1.4847983,2048.0054,2048.0054,291306.38,-1.4860694,-1.4835589,1.0016923,0.99204427,1.0016923,1.2856164,945.602,20480000.0,21658.162,169.20439,10.575274,-2073015.2,19289.3,249.38875,-0.0012722311,1.0672965547300772,,,0.7106538,,0.00635681,0.0057735993,3017.0942,8.551078
base,40000,,-0.8844654,279047.22,-1.4235307,2048.0078,2048.0078,279228.72,-1.4244566,-1.4220569,1.0016875,0.9920666,1.0016875,1.2848145,945.6358,20480000.0,21657.387,169.19833,10.574896,-2072941.0,19288.61,181.58728,-0.0009263488,1.0650425922780764,,,0.71142864,,0.005359021,0.0050000623,3974.9172,8.056658
base,50000,,-0.89044464,271781.16,-1.3864636,2048.0054,2048.0054,271929.66,-1.3872211,-1.3848864,1.0016859,0.99207455,1.0016859,1.2845304,945.629,20480000.0,21657.543,169.19955,10.574972,-2072956.0,19288.748,148.34785,-0.0007567813,1.0633852586787522,,,0.7228224,,0.004721385,0.0044721807,4932.2056,9.14432
base,60000,,-0.89514446,266231.72,-1.3581538,2048.0066,2048.0066,266359.0,-1.358803,-1.3565103,1.0016901,0.9920547,1.0016901,1.2852439,945.66864,20480000.0,21656.635,169.19246,10.574529,-2072869.0,19287.94,127.0276,-0.00064801826,1.062398173805143,,,0.7378082,,0.0042684553,0.004082517,5890.6387,8.074138
base,70000,,-0.8984802,262254.1,-1.3378624,2048.0076,2048.0076,262366.94,-1.3384379,-1.3361826,1.0016879,0.99206495,1.0016879,1.2848727,945.634,20480000.0,21657.428,169.19865,10.574916,-2072945.0,19288.646,113.62721,-0.00057965756,1.061401473610509,,,0.74141264,,0.003925243,0.0037796716,6847.9556,8.6177025
base,80000,,-0.901839,258280.3,-1.3175904,2048.0078,2048.0078,258383.47,-1.3181167,-1.3158855,1.0016955,0.9920294,1.0016955,1.2861538,945.6421,20480000.0,21657.242,169.1972,10.574825,-2072927.2,19288.482,102.938576,-0.00052513054,1.0604682273865784,,,0.6782627,,0.0036535263,0.003535556,7805.8535,8.879397
base,90000,,-0.9040499,255658.9,-1.3042176,2048.0063,2048.0063,255753.58,-1.3047006,-1.3025016,1.0016882,0.992063,1.0016882,1.2849438,945.6325,20480000.0,21657.46,169.19891,10.574932,-2072948.2,19288.676,95.01368,-0.00048470253,1.0599567121474838,,,0.7406168,,0.0034314673,0.0033333518,8763.935,7.779356
base,100000,,-0.90610933,253328.92,-1.2923315,2048.0083,2048.0083,253417.83,-1.292785,-1.2906171,1.0016798,0.9921027,1.0016798,1.2835162,945.6283,20480000.0,21657.559,169.19968,10.57498,-2072957.5,19288.762,88.79962,-0.00045300217,1.059545788198562,,,0.7001486,,0.0032455528,0.0031622935,9720.994,8.73894
base,110000,,-0.9077503,251323.88,-1.282103,2048.0073,2048.0073,251407.2,-1.282528,-1.2803775,1.0016795,0.99210393,1.0016795,1.2834742,945.6117,20480000.0,21657.938,169.20264,10.575165,-2072993.9,19289.102,83.39475,-0.00042542975,1.0591305478886976,,,0.70351005,,0.0030869492,0.0030151273,10678.932,7.9928775
base,120000,,-0.90921474,249638.61,-1.2735057,2048.0054,2048.0054,249718.38,-1.2739125,-1.2717724,1.0016829,0.9920887,1.0016829,1.2840216,945.6083,20480000.0,21658.018,169.20326,10.575204,-2073001.4,19289.172,78.820076,-0.0004020925,1.0586852693392341,,,0.7000911,,0.0029495438,0.0028867633,11636.12,7.869754
base,130000,,-0.91057754,248020.2,-1.2652495,2048.006,2048.006,248095.31,-1.2656327,-1.2635022,1.0016862,0.992073,1.0016862,1.2845862,945.6166,20480000.0,21657.828,169.20178,10.575111,-2072983.2,19289.002,74.76111,-0.00038138608,1.0582188303027853,,,0.7019818,,0.0028289973,0.002773512,12593.19,7.9371696
base,140000,,-0.9117926,246643.4,-1.2582259,2048.0073,2048.0073,246714.14,-1.2585868,-1.2564658,1.0016881,0.9920644,1.0016881,1.2848952,945.64307,20480000.0,21657.22,169.19704,10.574815,-2072925.2,19288.463,71.24746,-0.00036346156,1.058080571194735,,,0.7031615,,0.0027221239,0.002672622,13550.35,7.325352
base,150000,,-0.91090757,247952.08,-1.264902,2048.007,2048.007,248021.28,-1.265255,-1.263111,1.0016974,0.99202013,1.0016974,1.2864838,945.6105,20480000.0,21657.967,169.20287,10.575179,-2072996.6,19289.127,68.64528,-0.00035018683,1.0577875522516986,,,0.7128413,,0.0026265252,0.0025819975,14506.889,8.12524
base,160000,,-0.9103022,248579.64,-1.2681035,2048.007,2048.007,248646.53,-1.2684447,-1.266288,1.0017031,0.99199367,1.0017031,1.2874362,945.6256,20480000.0,21657.62,169.20015,10.575009,-2072963.4,19288.818,66.43949,-0.00033893422,1.057872120010352,,,0.6854832,,0.0025403376,0.0025000079,15464.213,8.887309
base,170000,,-0.9117283,246931.02,-1.2596933,2048.0085,2048.0085,246995.1,-1.2600201,-1.257889,1.0016941,0.99203473,1.0016941,1.2859528,945.60474,20480000.0,21658.098,169.20389,10.575243,-2073009.1,19289.244,64.1375,-0.00032719082,1.057409716521007,,,0.6932595,,0.0024621193,0.0024253633,16422.287,7.610077
base,180000,,-0.9133572,244987.22,-1.2497771,2048.008,2048.008,245049.55,-1.2500951,-1.247979,1.0016956,0.9920283,1.0016956,1.2861868,945.63947,20480000.0,21657.303,169.19768,10.574855,-2072933.0,19288.535,61.621788,-0.00031435722,1.0571428743960465,,,0.7295575,,0.0023907027,0.0023570291,17379.111,8.413698
base,190000,,-0.914187,243983.89,-1.2446587,2048.0083,2048.0083,244043.86,-1.2449646,-1.2428608,1.0016928,0.9920418,1.0016928,1.2857028,945.6162,20480000.0,21657.836,169.20184,10.575115,-2072984.0,19289.01,59.637287,-0.0003042335,1.0568428916768897,,,0.731323,,0.0023251679,0.0022941635,18336.742,8.041594
base,200000,,-0.915079,242875.55,-1.2390046,2048.007,2048.007,242933.33,-1.2392994,-1.2372037,1.0016938,0.9920368,1.0016938,1.2858827,945.61676,20480000.0,21657.822,169.20174,10.575109,-2072982.8,19288.998,57.96966,-0.00029572626,1.056811530641415,,,0.6983752,,0.002264747,0.0022360736,19294.02,8.046625
base,210000,,-0.91591537,241978.78,-1.2344298,2048.0063,2048.0063,242035.73,-1.2347205,-1.2326262,1.001699,0.9920131,1.001699,1.2867377,945.6262,20480000.0,21657.605,169.20004,10.575003,-2072962.0,19288.805,56.442757,-0.0002879369,1.0565580705546727,,,0.71500444,,0.0022087973,0.002182184,20251.246,8.317656
base,220000,,-0.91646504,241262.48,-1.2307758,2048.0078,2048.0078,241318.42,-1.2310611,-1.2289736,1.0016986,0.99201435,1.0016986,1.2866901,945.88513,20480000.0,21651.678,169.15373,10.572108,-2072394.8,19283.525,54.9414,-0.0002802779,1.0563670178029754,,,0.7336731,,0.0021567962,0.002132012,21209.074,7.420272
base,230000,,-0.9171929,240443.58,-1.2265981,2048.006,2048.006,240497.81,-1.2268748,-1.2247949,1.0016983,0.99201643,1.0016983,1.2866161,945.6282,20480000.0,21657.56,169.19969,10.574981,-2072957.8,19288.766,53.575005,-0.00027330738,1.056313117784901,,,0.706172,,0.0021083185,0.0020851486,22165.73,7.738592
base,240000,,-0.91772395,239847.33,-1.2235564,2048.008,2048.008,239899.8,-1.2238241,-1.221758,1.0016911,0.9920491,1.0016911,1.2854404,945.6119,20480000.0,21657.934,169.2026,10.575163,-2072993.5,19289.098,52.366405,-0.00026714185,1.0561740983188543,,,0.7018516,,0.0020629603,0.0020412458,23122.672,7.424656
base,250000,,-0.91813856,239348.4,-1.2210113,2048.0054,2048.0054,239399.5,-1.221272,-1.2192184,1.0016843,0.99208206,1.0016843,1.2842629,945.60364,20480000.0,21658.123,169.20409,10.575255,-2073011.5,19289.266,51.244743,-0.0002614198,1.0560404022166736,,,0.7058456,,0.00202042,0.002000004,24079.307,8.300971
base,260000,,-0.9184155,238998.5,-1.2192262,2048.008,2048.008,239049.55,-1.2194867,-1.2174366,1.001684,0.9920835,1.001684,1.2842093,945.64197,20480000.0,21657.246,169.19724,10.574827,-2072927.6,19288.484,50.255905,-0.00025637535,1.0559031781514805,,,0.7295277,,0.001980391,0.001961165,25036.832,7.866247
base,270000,,-0.9190594,238239.28,-1.2153533,2048.0073,2048.0073,238289.2,-1.2156079,-1.2135638,1.0016843,0.9920812,1.0016843,1.2842864,946.20276,20480000.0,21644.41,169.09695,10.56856,-2071699.0,19277.053,48.855686,-0.00024923225,1.0557124426111268,,,0.70551085,,0.0019426537,0.0019245044,25994.52,7.4521537
base,280000,,-0.9195696,237649.1,-1.2123425,2048.0093,2048.0093,237697.08,-1.2125872,-1.2105405,1.0016906,0.9920522,1.0016906,1.2853346,945.6079,20480000.0,21658.025,169.20332,10.575208,-2073002.2,19289.178,48.028145,-0.00024501063,1.055623558013164,,,0.71672153,,0.0019070015,0.0018898257,26951.164,8.051797
base,290000,,-0.92002153,237097.7,-1.2095295,2048.0076,2048.0076,237144.83,-1.20977,-1.2077341,1.0016856,0.9920753,1.0016856,1.2845018,945.6124,20480000.0,21657.922,169.20251,10.575157,-2072992.4,19289.086,47.16097,-0.00024058683,1.0555120022032785,,,0.7101581,,0.001873255,0.0018569566,27908.42,7.763206
base,300000,,-0.9204738,236610.94,-1.2070464,2048.0066,2048.0066,236658.1,-1.207287,-1.2052575,1.0016838,0.9920836,1.0016838,1.2842015,945.6305,20480000.0,21657.508,169.19928,10.574955,-2072952.8,19288.719,46.416576,-0.00023678939,1.0554471900580769,,,0.7521684,,0.0018412152,0.001825745,28865.4,7.5366173
base,310000,,-0.9211602,235805.48,-1.2029375,2048.0066,2048.0066,235851.67,-1.203173,-1.2011472,1.0016867,0.99207056,1.0016867,1.2846721,945.6087,20480000.0,21658.006,169.20317,10.575198,-2073000.4,19289.162,45.624695,-0.00023274968,1.0552914190580942,,,0.70918775,,0.0018107702,0.0017960559,29822.191,7.301401
base,320000,,-0.92170656,235265.45,-1.2001826,2048.0066,2048.0066,235310.75,-1.2004136,-1.1983925,1.0016865,0.9920715,1.0016865,1.2846369,945.6189,20480000.0,21657.773,169.20135,10.575085,-2072978.1,19288.955,44.953636,-0.00022932634,1.0552126235754988,,,0.72063017,,0.001781797,0.0017677698,30778.709,12.368521
base,330000,,-0.92030376,236861.64,-1.2083253,2048.0078,2048.0078,236905.6,-1.2085495,-1.2065078,1.0016923,0.99204457,1.0016923,1.2856092,945.61597,20480000.0,21657.842,169.20189,10.575118,-2072984.6,19289.016,44.410645,-0.00022655634,1.05515089760998,,,0.7283449,,0.0017541745,0.0017407791,31740.297,7.4767413
base,340000,,-0.92235655,234359.45,-1.1955606,2048.006,2048.006,234402.81,-1.1957818,-1.1937783,1.0016783,0.99210954,1.0016783,1.283272,945.64087,20480000.0,21657.27,169.19742,10.574839,-2072929.9,19288.506,43.62457,-0.00022254625,1.054981148184232,,,0.7522118,,0.0017277872,0.0017149884,32697.004,7.4726696
base,350000,,-0.92290807,233683.25,-1.1921111,2048.0056,2048.0056,233726.16,-1.19233,-1.1903374,1.0016739,0.99213076,1.0016739,1.2825129,945.6113,20480000.0,21657.947,169.20271,10.57517,-2072994.8,19289.11,43.134884,-0.00022004818,1.055034210376153,,,0.72599196,,0.001702562,0.0016903109,33653.715,7.2136555
base,360000,,-0.92273533,234002.8,-1.1937412,2048.0085,2048.0085,234045.48,-1.193959,-1.1919472,1.0016878,0.99206495,1.0016878,1.2848718,945.61475,20480000.0,21657.87,169.2021,10.575131,-2072987.2,19289.04,42.29574,-0.00021576736,1.0549058700950822,,,0.6833434,,0.001678393,0.001666669,34610.15,7.360161
base,370000,,-0.92267656,234012.83,-1.1937923,2048.0066,2048.0066,234054.58,-1.1940054,-1.1920063,1.0016769,0.9921159,1.0016769,1.2830424,945.6097,20480000.0,21657.984,169.203,10.575188,-2072998.4,19289.143,41.582752,-0.00021213014,1.054795354470853,,,0.72004867,,0.0016552461,0.001643992,35566.668,8.936439
base,380000,,-0.9232335,233336.9,-1.1903442,2048.0063,2048.0063,233377.03,-1.1905489,-1.1885389,1.0016911,0.9920501,1.0016911,1.2854097,945.63904,20480000.0,21657.312,169.19775,10.57486,-2072934.0,19288.545,41.015396,-0.00020923583,1.054795558191889,,,0.7088325,,0.0016330457,0.0016222164,36524.84,7.812208
base,390000,,-0.9237928,232728.31,-1.1872396,2048.0056,2048.0056,232768.88,-1.1874465,-1.1854488,1.0016851,0.9920777,1.0016851,1.2844166,945.62006,20480000.0,21657.748,169.20116,10.575072,-2072975.6,19288.932,40.361984,-0.00020590251,1.0546800600128863,,,0.6981263,,0.001611688,0.0016012837,37481.875,7.2150145
base,400000,,-0.9238839,232603.98,-1.1866053,2048.0078,2048.0078,232643.78,-1.1868083,-1.1848066,1.0016896,0.99205756,1.0016896,1.2851425,945.62164,20480000.0,21657.71,169.20087,10.575054,-2072972.1,19288.898,39.864094,-0.00020336258,1.0545199698862204,,,0.70696473,,0.0015911432,0.0015811408,38438.277,7.780776
base,410000,,-0.9244297,231977.52,-1.1834095,2048.007,2048.007,232016.44,-1.1836079,-1.1816145,1.001687,0.9920686,1.001687,1.2847434,945.6097,20480000.0,21657.984,169.203,10.575188,-2072998.2,19289.143,39.228863,-0.00020012201,1.054476014442765,,,0.70174074,,0.0015713765,0.0015617395,39395.258,7.6368876
base,420000,,-0.9246611,231733.86,-1.1821665,2048.007,2048.007,231772.75,-1.1823648,-1.1803838,1.0016783,0.99210954,1.0016783,1.2832713,945.65106,20480000.0,21657.037,169.1956,10.574725,-2072907.6,19288.299,38.77112,-0.00019778688,1.0544309884206546,,,0.6978853,,0.0015523254,0.0015430354,40352.113,7.7444124
base,430000,,-0.92490464,231481.73,-1.1808802,2048.0078,2048.0078,231520.39,-1.1810775,-1.179099,1.001678,0.9921115,1.001678,1.2832035,945.6194,20480000.0,21657.762,169.20126,10.575079,-2072977.0,19288.945,39.211227,-0.00020003205,1.0542351995724386,,,0.7164502,,0.0015339577,0.0015249875,41309.047,7.5594645
base,440000,,-0.925249,231066.5,-1.178762,2048.0068,2048.0068,231103.61,-1.1789514,-1.1769708,1.0016826,0.99208987,1.0016826,1.283983,945.6258,20480000.0,21657.615,169.20012,10.575007,-2072963.0,19288.814,37.51851,-0.00019139682,1.0541928159197393,,,0.67922044,,0.0015162186,0.0015075585,42265.836,7.5830164
base,450000,,-0.9257059,230557.06,-1.1761631,2048.0073,2048.0073,230593.8,-1.1763505,-1.1743804,1.0016776,0.99211293,1.0016776,1.2831494,945.62103,20480000.0,21657.725,169.20097,10.575061,-2072973.4,19288.91,37.19969,-0.00018977039,1.0541607648018225,,,0.69081545,,0.0014990865,0.0014907136,43222.61,7.3215656
base,460000,,-0.92608196,230091.27,-1.1737869,2048.0066,2048.0066,230127.84,-1.1739736,-1.1720147,1.0016713,0.99214333,1.0016713,1.2820625,945.64453,20480000.0,21657.188,169.19678,10.574799,-2072922.0,19288.432,36.771427,-0.00018758565,1.05408157325952,,,0.7049732,,0.0014825218,0.0014744212,44179.14,7.527527
base,470000,,-0.9262673,229880.22,-1.1727103,2048.0095,2048.0095,229916.8,-1.1728969,-1.1709275,1.0016819,0.9920932,1.0016819,1.2838612,945.61755,20480000.0,21657.805,169.2016,10.5751,-2072981.1,19288.982,36.51801,-0.00018629288,1.0540561090772573,,,0.7166548,,0.0014665002,0.0014586514,45135.875,7.8149166
base,480000,,-0.92650974,229597.11,-1.1712661,2048.007,2048.007,229632.89,-1.1714486,-1.1694816,1.0016819,0.9920932,1.0016819,1.2838609,945.62103,20480000.0,21657.725,169.20097,10.575061,-2072973.5,19288.912,36.129536,-0.00018431111,1.0540320507980263,,,0.7045357,,0.0014509773,0.0014433772,46093.16,7.3562317
base,490000,,-0.9266088,229467.06,-1.1706026,2048.0073,2048.0073,229502.64,-1.1707841,-1.1688184,1.0016819,0.99209344,1.0016819,1.2838527,945.60864,20480000.0,21658.008,169.20319,10.575199,-2073000.6,19289.164,35.78109,-0.00018253355,1.0538946355845078,,,0.72210836,,0.0014359363,0.0014285729,47049.734,7.279832
base,500000,,-0.92556304,230925.72,-1.1780438,2048.0068,2048.0068,230961.97,-1.1782287,-1.1762426,1.0016886,0.9920616,1.0016886,1.2849926,945.6443,20480000.0,21657.191,169.19681,10.5748005,-2072922.5,19288.438,35.521957,-0.00018121161,1.0539538412917,,,0.69280815,,0.0014213494,0.001414215,48006.266,7.2923994
base,510000,,-0.9198904,237940.61,-1.2138295,2048.0088,2048.0088,237976.45,-1.2140124,-1.2119126,1.0017326,0.99185425,1.0017326,1.2924329,945.61597,20480000.0,21657.842,169.20189,10.575118,-2072984.6,19289.014,35.788433,-0.00018257102,1.053938957089438,,,0.6969578,,0.0014072084,0.0014002814,48962.74,7.789212
base,520000,,-0.9210742,236369.52,-1.2058147,2048.0073,2048.0073,236404.98,-1.2059958,-1.2039087,1.0017335,0.9918499,1.0017335,1.2925935,945.6203,20480000.0,21657.742,169.20111,10.575069,-2072975.1,19288.926,35.2864,-0.00018000994,1.0537937741378145,,,0.73626494,,0.0013934822,0.0013867519,49919.7,7.483389
base,530000,,-0.92405593,232731.3,-1.1872548,2048.0051,2048.0051,232765.62,-1.1874299,-1.1854031,1.0017098,0.9919616,1.0017098,1.2885826,945.85516,20480000.0,21652.363,169.15909,10.572443,-2072460.2,19284.137,34.640858,-0.00017671677,1.0537087491611576,,,0.7171595,,0.0013801508,0.001373607,50876.945,7.448003
base,540000,,-0.9268787,229175.19,-1.1691136,2048.008,2048.008,229209.34,-1.1692879,-1.1673268,1.00168,0.99210197,1.00168,1.2835449,945.64685,20480000.0,21657.135,169.19637,10.574773,-2072916.9,19288.385,34.052486,-0.00017371526,1.0537369159357646,,,0.6997349,,0.0013671798,0.001360829,51833.62,7.302556
base,550000,,-0.92731744,228618.11,-1.1662718,2048.0083,2048.0083,228651.98,-1.1664445,-1.164497,1.0016724,0.9921368,1.0016724,1.2822894,945.6174,20480000.0,21657.809,169.20163,10.575102,-2072981.5,19288.986,33.749413,-0.00017216915,1.05366106959332,,,0.71697974,,0.001354586,0.001348401,52790.09,7.2371807
base,560000,,-0.92781496,228123.83,-1.1637503,2048.0076,2048.0076,228157.52,-1.1639221,-1.1619772,1.0016738,0.9921304,1.0016738,1.2825192,945.6197,20480000.0,21657.756,169.20122,10.575076,-2072976.4,19288.938,33.51268,-0.0001709615,1.0536036596825988,,,0.7055423,,0.0013423256,0.0013363075,53746.547,7.3041506
base,570000,,-0.9278259,228109.48,-1.1636771,2048.0066,2048.0066,228142.64,-1.1638463,-1.1619045,1.0016712,0.9921424,1.0016712,1.2820889,945.6261,20480000.0,21657.61,169.20007,10.575005,-2072962.4,19288.807,33.224537,-0.00016949156,1.0535313462545892,,,0.71238995,,0.0013303938,0.0013245335,54703.05,7.2704062
base,580000,,-0.9283231,227530.23,-1.1607221,2048.007,2048.007,227563.52,-1.1608919,-1.158955,1.0016712,0.992143,1.0016712,1.2820711,945.64264,20480000.0,21657.229,169.1971,10.574819,-2072926.0,19288.47,33.001255,-0.00016835252,1.0534807719541384,,,0.6995044,,0.0013187655,0.0013130654,55659.543,7.4311886
base,590000,,-0.9284218,227436.72,-1.1602451,2048.007,2048.007,227469.48,-1.1604122,-1.1584713,1.0016754,0.99212325,1.0016754,1.2827798,945.6201,20480000.0,21657.746,169.20114,10.575071,-2072975.5,19288.93,32.76065,-0.0001671251,1.0535590683386773,,,0.7212131,,0.0013074477,0.0013018901,56616.16,7.662223
base,600000,,-0.92850757,227326.31,-1.1596818,2048.0066,2048.0066,227358.8,-1.1598475,-1.1579003,1.0016817,0.9920947,1.0016817,1.2838084,945.61304,20480000.0,21657.908,169.20241,10.5751505,-2072991.0,19289.074,32.54313,-0.00016601542,1.053480677328345,,,0.71035457,,0.0012964226,0.0012909955,57573.03,7.2663946
base,610000,,-0.92863,227177.52,-1.1589228,2048.0063,2048.0063,227209.47,-1.1590858,-1.1571445,1.0016776,0.9921131,1.0016776,1.2831445,945.6095,20480000.0,21657.99,169.20305,10.575191,-2072998.8,19289.146,32.296474,-0.00016475715,1.0533407827434498,,,0.7287321,,0.0012856664,0.0012803698,58529.49,7.8352747
base,620000,,-0.9288481,226915.2,-1.1575845,2048.0066,2048.0066,226946.97,-1.1577467,-1.1558062,1.001679,0.992107,1.001679,1.2833635,945.6437,20480000.0,21657.205,169.19691,10.574807,-2072923.8,19288.45,32.06559,-0.00016357932,1.0533270839871454,,,0.6956041,,0.0012751668,0.0012700022,59486.586,7.6146936
base,630000,,-0.9291175,226631.19,-1.1561357,2048.0073,2048.0073,226663.5,-1.1563005,-1.1543714,1.0016712,0.99214375,1.0016712,1.2820481,945.61615,20480000.0,21657.838,169.20186,10.575116,-2072984.2,19289.012,31.809065,-0.00016227068,1.0532612169497317,,,0.705997,,0.0012649208,0.0012598827,60443.426,7.120384
base,640000,,-0.9293559,226338.05,-1.1546403,2048.007,2048.007,226369.89,-1.1548027,-1.1528698,1.0016767,0.9921187,1.0016767,1.2829491,945.6178,20480000.0,21657.799,169.20155,10.575097,-2072980.6,19288.977,31.783413,-0.00016213981,1.0532470153727784,,,0.7090306,,0.0012549129,0.0012500009,61399.754,7.894651
base,650000,,-0.92942953,226260.25,-1.1542434,2048.0066,2048.0066,226292.25,-1.1544067,-1.152482,1.00167,0.99214894,1.00167,1.281858,945.6072,20480000.0,21658.041,169.20345,10.575215,-2073003.8,19289.193,31.67798,-0.00016160196,1.0531736557724816,,,0.7096424,,0.0012451629,0.0012403483,62356.844,8.261317
base,660000,,-0.9297827,225852.23,-1.152162,2048.006,2048.006,225883.44,-1.1523211,-1.1503918,1.0016772,0.9921157,1.0016772,1.2830546,945.6436,20480000.0,21657.207,169.19693,10.574808,-2072924.0,19288.451,31.40602,-0.00016021458,1.0531911157438572,,,0.74335814,,0.0012356176,0.0012309158,63314.324,8.052685
base,670000,,-0.9299728,225684.7,-1.1513073,2048.0073,2048.0073,225714.84,-1.151461,-1.1495466,1.0016655,0.99217045,1.0016655,1.281087,945.8492,20480000.0,21652.502,169.16017,10.572511,-2072473.5,19284.26,31.261576,-0.00015947771,1.0531086191282661,,,0.72938585,,0.0012262804,0.0012216953,64271.87,7.418726
base,680000,,-0.92337286,233898.55,-1.1932094,2048.0088,2048.0088,233929.9,-1.1933694,-1.1912987,1.0017382,0.9918284,1.0017382,1.2933667,945.6143,20480000.0,21657.879,169.20218,10.575136,-2072988.1,19289.047,31.591213,-0.00016115932,1.0531448703242758,,,0.7219188,,0.0012171788,0.001212679,65228.516,7.1678686
base,690000,,-0.92145956,236188.27,-1.2048901,2048.0078,2048.0078,236220.39,-1.205054,-1.2029309,1.001765,0.9917019,1.001765,1.2979039,945.6168,20480000.0,21657.82,169.20172,10.575108,-2072982.6,19288.996,31.472622,-0.00016055435,1.0531402524221223,,,0.75085735,,0.0012082481,0.0012038593,66185.15,7.451486
base,700000,,-0.9227961,234576.53,-1.196668,2048.0066,2048.0066,234608.36,-1.1968304,-1.194738,1.0017513,0.9917676,1.0017513,1.2955543,945.6319,20480000.0,21657.477,169.19904,10.57494,-2072949.6,19288.69,31.19991,-0.00015916313,1.053137553716646,,,0.74723315,,0.0011995289,0.0011952295,67141.86,7.554159
base,710000,,-0.9270308,229286.81,-1.1696831,2048.0063,2048.0063,229316.86,-1.1698364,-1.1678388,1.0017105,0.9919595,1.0017105,1.2886655,945.6122,20480000.0,21657.928,169.20256,10.57516,-2072992.9,19289.092,30.707296,-0.00015665012,1.0530905581818366,,,0.7303498,,0.0011909944,0.0011867824,68098.66,7.2629657
base,720000,,-0.92775303,228334.0,-1.1648225,2048.0073,2048.0073,228363.83,-1.1649746,-1.1630039,1.0016944,0.9920342,1.0016944,1.2859789,945.62134,20480000.0,21657.719,169.20093,10.575058,-2072972.8,19288.904,30.437012,-0.0001552713,1.0529368160792991,,,0.7265167,,0.0011826245,0.0011785121,69055.125,7.540506
base,730000,,-0.9281025,227930.8,-1.1627655,2048.0059,2048.0059,227961.14,-1.1629204,-1.1609521,1.0016954,0.9920296,1.0016954,1.286141,945.60846,20480000.0,21658.012,169.20322,10.575201,-2073001.0,19289.168,30.238087,-0.0001542565,1.0529366501468704,,,0.71599865,,0.0011744511,0.0011704123,70011.88,7.2694263
base,740000,,-0.9284903,227502.34,-1.1605798,2048.0063,2048.0063,227532.39,-1.1607331,-1.158771,1.0016932,0.9920396,1.0016932,1.2857828,946.01294,20480000.0,21648.752,169.13087,10.57068,-2072114.6,19280.92,30.08306,-0.00015346563,1.0529118467760095,,,0.70376825,,0.0011664189,0.0011624771,70968.77,7.2108364
base,750000,,-0.9284481,227503.97,-1.1605881,2048.0068,2048.0068,227533.8,-1.1607403,-1.1587726,1.001698,0.9920173,1.001698,1.2865846,945.62366,20480000.0,21657.664,169.2005,10.575031,-2072967.6,19288.857,29.944862,-0.00015276064,1.0527965557485222,,,0.7151215,,0.0011585751,0.0011547012,71925.19,7.4679174
base,760000,,-0.92894346,226995.44,-1.1579938,2048.007,2048.007,227025.84,-1.158149,-1.1561891,1.0016952,0.9920311,1.0016952,1.2860916,945.61365,20480000.0,21657.895,169.2023,10.575144,-2072989.6,19289.062,29.813332,-0.00015208965,1.0527984665325223,,,0.696666,,0.0011508791,0.0011470794,72881.85,7.878392
base,770000,,-0.9289832,226941.64,-1.1577195,2048.008,2048.008,226971.31,-1.1578708,-1.1559203,1.0016875,0.99206626,1.0016875,1.2848233,945.60944,20480000.0,21657.99,169.20305,10.575191,-2072998.9,19289.146,29.691044,-0.0001514658,1.0528152888458044,,,0.71849847,,0.0011433268,0.0011396065,73838.92,7.5395956
base,780000,,-0.9291962,226682.78,-1.1563989,2048.006,2048.006,226712.3,-1.1565495,-1.1545972,1.0016909,0.9920505,1.0016909,1.2853904,945.63763,20480000.0,21657.344,169.198,10.574875,-2072937.0,19288.572,29.5394,-0.00015069221,1.0528187169278438,,,0.70871925,,0.0011359173,0.0011322778,74795.695,7.446752
base,790000,,-0.9291421,226701.31,-1.1564934,2048.0066,2048.0066,226731.03,-1.156645,-1.1546922,1.0016912,0.99204856,1.0016912,1.2854594,945.60565,20480000.0,21658.078,169.20374,10.575233,-2073007.2,19289.225,29.49251,-0.00015045301,1.0527450444239086,,,0.7105341,,0.0011286756,0.0011250885,75752.33,7.2218113
base,800000,,-0.9296043,226210.61,-1.1539901,2048.0093,2048.0093,226239.72,-1.1541387,-1.1521945,1.0016873,0.99206746,1.0016873,1.2847836,945.61383,20480000.0,21657.89,169.20227,10.575142,-2072989.2,19289.059,29.448093,-0.00015022642,1.0527041828454364,,,0.7335756,,0.001121544,0.0011180347,76708.76,8.071437
base,810000,,-0.92971295,226054.69,-1.1531947,2048.0078,2048.0078,226083.78,-1.1533432,-1.1513978,1.0016896,0.9920571,1.0016896,1.2851583,945.6228,20480000.0,21657.684,169.20065,10.575041,-2072969.5,19288.875,29.189629,-0.00014890789,1.0526863598066771,,,0.6973407,,0.0011145591,0.0011111118,77666.06,7.9080954
base,820000,,-0.9304216,225338.6,-1.1495417,2048.007,2048.007,225367.1,-1.149687,-1.147751,1.0016868,0.9920697,1.0016868,1.2847013,945.632,20480000.0,21657.473,169.199,10.574938,-2072949.4,19288.688,28.942915,-0.0001476493,1.0524918226089588,,,0.665637,,0.001107703,0.001104316,78623.164,7.346814
base,830000,,-0.93078035,224912.56,-1.1473683,2048.0078,2048.0078,224941.36,-1.1475152,-1.1455796,1.0016897,0.9920564,1.0016897,1.2851804,945.6211,20480000.0,21657.723,169.20096,10.57506,-2072973.4,19288.91,28.857607,-0.00014721412,1.052509571354163,,,0.73201466,,0.0011009688,0.0010976433,79579.664,7.296636
base,840000,,-0.9308608,224808.27,-1.1468363,2048.0085,2048.0085,224837.02,-1.1469829,-1.1450475,1.0016903,0.99205333,1.0016903,1.2852879,945.6197,20480000.0,21657.756,169.20122,10.575076,-2072976.4,19288.938,28.752771,-0.0001466793,1.0524898176002784,,,0.69533324,,0.0010943569,0.0010910901,80536.18,7.5179825
base,850000,,-0.9303177,225579.5,-1.1507705,2048.006,2048.006,225607.86,-1.1509153,-1.1489652,1.0016972,0.99202126,1.0016972,1.2864431,945.8618,20480000.0,21652.213,169.15791,10.57237,-2072445.8,19284.002,28.68954,-0.00014635675,1.052477008435513,,,0.72513294,,0.0010878694,0.001084653,81493.14,7.614873
base,860000,,-0.93235403,223013.98,-1.1376829,2048.006,2048.006,223042.36,-1.1378276,-1.1359245,1.0016755,0.9921234,1.0016755,1.2827743,945.6422,20480000.0,21657.238,169.19717,10.574823,-2072927.0,19288.479,28.438808,-0.00014507766,1.0524590198118762,,,0.7162132,,0.0010814735,0.0010783284,82450.01,7.961613
base,870000,,-0.933026,222276.77,-1.1339221,2048.0068,2048.0068,222305.72,-1.1340698,-1.1321762,1.0016725,0.992137,1.0016725,1.2822862,945.6177,20480000.0,21657.8,169.20157,10.575098,-2072980.9,19288.98,28.297987,-0.00014435928,1.0524230093480655,,,0.7100606,,0.0010752105,0.0010721132,83407.19,7.4018545
base,880000,,-0.9327132,222694.9,-1.1360552,2048.0059,2048.0059,222723.61,-1.1362016,-1.134305,1.001672,0.9921399,1.001672,1.2821856,945.6157,20480000.0,21657.846,169.20192,10.57512,-2072985.0,19289.02,28.242838,-0.00014407793,1.0523357677668959,,,0.7183676,,0.0010690391,0.0010660042,84363.82,8.126962
base,890000,,-0.931327,224330.1,-1.1443969,2048.0078,2048.0078,224357.86,-1.1445385,-1.1426145,1.001684,0.9920836,1.001684,1.284205,945.60004,20480000.0,21658.205,169.20473,10.575295,-2073019.4,19289.34,28.207085,-0.00014389554,1.0524024181499958,,,0.69151187,,0.0010630098,0.0010599985,85321.16,7.775385
base,900000,,-0.9313307,224240.16,-1.1439381,2048.007,2048.007,224267.9,-1.1440797,-1.1421504,1.0016892,0.9920589,1.0016892,1.2850916,945.8781,20480000.0,21651.838,169.15498,10.572186,-2072410.0,19283.668,28.067019,-0.00014318101,1.052347210531172,,,0.72262836,,0.0010570436,0.0010540931,86278.38,7.341652
base,910000,,-0.93170947,223854.47,-1.1419705,2048.005,2048.005,223881.78,-1.1421099,-1.1401875,1.001686,0.99207455,1.001686,1.2845325,945.6185,20480000.0,21657.781,169.20142,10.5750885,-2072979.0,19288.963,28.00865,-0.00014288325,1.0523666984619844,,,0.7064154,,0.0010511758,0.0010482854,87234.93,7.415898
base,920000,,-0.9316849,223854.44,-1.1419704,2048.0083,2048.0083,223881.78,-1.1421099,-1.1401889,1.0016848,0.9920802,1.0016848,1.2843294,945.6088,20480000.0,21658.004,169.20316,10.575197,-2073000.1,19289.16,27.912987,-0.00014239523,1.0523292952092738,,,1.0113308,,0.0010454139,0.0010425727,88191.54,9.395992
base,930000,,-0.9320155,223499.58,-1.1401601,2048.0059,2048.0059,223527.53,-1.1403027,-1.1383888,1.0016813,0.99209595,1.0016813,1.2837611,945.6148,20480000.0,21657.867,169.20209,10.57513,-2072987.1,19289.037,27.842588,-0.0001420361,1.052237280206,,,0.6968272,,0.0010397567,0.0010369522,89150.44,7.447859
base,940000,,-0.93202585,223429.53,-1.1398028,2048.0068,2048.0068,223456.33,-1.1399395,-1.138019,1.0016875,0.99206626,1.0016875,1.2848259,945.6362,20480000.0,21657.377,169.19826,10.574891,-2072940.1,19288.602,27.820782,-0.00014192486,1.0522343570593768,,,0.7261567,,0.001034198,0.0010314218,90107.086,7.451163
base,950000,,-0.9321781,223301.89,-1.1391516,2048.0066,2048.0066,223330.38,-1.139297,-1.1373824,1.0016832,0.99208647,1.0016832,1.2841014,946.0683,20480000.0,21647.486,169.12099,10.570062,-2071993.4,19279.793,27.691122,-0.00014126342,1.0522468361313833,,,0.71067667,,0.0010286918,0.0010259789,91064.63,7.1746316
base,960000,,-0.9321297,223339.9,-1.1393455,2048.0054,2048.0054,223367.19,-1.1394848,-1.1375718,1.0016817,0.9920942,1.0016817,1.2838224,945.6093,20480000.0,21657.992,169.20306,10.5751915,-2072999.1,19289.15,27.637762,-0.00014099121,1.0522497631246166,,,0.72332954,,0.0010232817,0.0010206213,92021.02,7.316154
base,970000,,-0.9321823,223239.89,-1.1388353,2048.0076,2048.0076,223266.97,-1.1389735,-1.1370585,1.0016842,0.99208254,1.0016842,1.2842427,945.61584,20480000.0,21657.844,169.2019,10.575119,-2072984.9,19289.018,27.533094,-0.00014045724,1.05213185415145,,,0.7151625,,0.0010179855,0.0010153466,92977.55,7.771955
base,980000,,-0.9325065,222903.25,-1.137118,2048.0076,2048.0076,222930.97,-1.1372594,-1.135343,1.001688,0.9920649,1.001688,1.2848778,945.643,20480000.0,21657.22,169.19704,10.574815,-2072925.2,19288.463,27.40646,-0.00013981124,1.0522009844462277,,,0.6779642,,0.0010127369,0.001010153,93934.54,7.493313
base,990000,,-0.932651,222732.0,-1.1362444,2048.0066,2048.0066,222758.86,-1.1363814,-1.1344842,1.0016724,0.9921375,1.0016724,1.2822673,945.6198,20480000.0,21657.754,169.2012,10.575075,-2072976.2,19288.938,27.303041,-0.00013928366,1.0522025277283034,,,0.6999605,,0.0010075895,0.0010050383,94891.23,7.3239565
base,1000000,29.341541,-0.93265647,222704.92,-1.1361063,2048.0083,2048.0083,222732.55,-1.1362472,-1.1343402,1.0016811,0.99209714,1.0016811,1.283721,945.61957,20480000.0,21657.76,169.20125,10.575078,-2072976.8,19288.941,27.23539,-0.00013893854,1.0521580125168541,"b""    from __gin__ import dynamic_registration\n    import __main__ as train_script\n    import data.pile.tasks\n    from models.scalable_t5 import network\n    import seqio\n    from t5x import adafactor\n    from t5x import gin_utils\n    from t5x import models\n    from t5x import partitioning\n    from t5x import trainer\n    from t5x import utils\n    \n#### Macros:\n\n    BATCH_SIZE = 2048\n    DROPOUT_RATE = 0.0\n    EVAL_BATCH_SIZE = 256\n    LABEL_SMOOTHING = 0.0\n    LOSS_NORMALIZING_FACTOR = None\n    MIXTURE_OR_TASK_MODULE = None\n    MIXTURE_OR_TASK_NAME = 'pile_r_denoiser'\n    MODEL = @models.EncoderDecoderModel()\n    MODEL_DIR = 'gs://improved-t5/ckpts/v2_base_mlm/'\n    OPTIMIZER = @adafactor.Adafactor()\n    RANDOM_SEED = None\n    SAVING_PERIOD = 10000\n    SHUFFLE_TRAIN_EXAMPLES = True\n    TASK_FEATURE_LENGTHS = {'inputs': 512, 'targets': 114}\n    TRAIN_STEPS = 2000000\n    USE_CACHED_TASKS = False\n    USE_HARDWARE_RNG = False\n    VOCABULARY = @seqio.SentencePieceVocabulary()\n    Z_LOSS = 0.0001\n    \n#### Parameters for adafactor.Adafactor:\n\n    adafactor.Adafactor.decay_rate = 0.8\n    adafactor.Adafactor.logical_factor_rules = \\\n        @adafactor.standard_logical_factor_rules()\n    adafactor.Adafactor.step_offset = 0\n    \n#### Parameters for utils.CheckpointConfig:\n\n    utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()\n    utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()\n    \n#### Parameters for utils.create_learning_rate_scheduler:\n\n    utils.create_learning_rate_scheduler.base_learning_rate = 1.0\n    utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'\n    utils.create_learning_rate_scheduler.warmup_steps = 10000\n    \n#### Parameters for infer_eval/utils.DatasetConfig:\n\n    infer_eval/utils.DatasetConfig.batch_size = %EVAL_BATCH_SIZE\n    infer_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    infer_eval/utils.DatasetConfig.seed = 0\n    infer_eval/utils.DatasetConfig.shuffle = False\n    infer_eval/utils.DatasetConfig.split = 'validation'\n    infer_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    \n#### Parameters for train/utils.DatasetConfig:\n\n    train/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train/utils.DatasetConfig.pack = True\n    train/utils.DatasetConfig.seed = None\n    train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES\n    train/utils.DatasetConfig.split = 'train'\n    train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for train_eval/utils.DatasetConfig:\n\n    train_eval/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train_eval/utils.DatasetConfig.pack = True\n    train_eval/utils.DatasetConfig.seed = 42\n    train_eval/utils.DatasetConfig.shuffle = False\n    train_eval/utils.DatasetConfig.split = 'validation'\n    train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for models.EncoderDecoderModel:\n\n    models.EncoderDecoderModel.input_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.label_smoothing = %LABEL_SMOOTHING\n    models.EncoderDecoderModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR\n    models.EncoderDecoderModel.module = @network.Transformer()\n    models.EncoderDecoderModel.optimizer_def = %OPTIMIZER\n    models.EncoderDecoderModel.output_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.z_loss = %Z_LOSS\n    \n#### Parameters for seqio.Evaluator:\n\n    seqio.Evaluator.logger_cls = \\\n        [@seqio.PyLoggingLogger, @seqio.TensorBoardLogger, @seqio.JSONLogger]\n    seqio.Evaluator.num_examples = 2048\n    \n#### Parameters for partitioning.PjitPartitioner:\n\n    partitioning.PjitPartitioner.logical_axis_rules = \\\n        @partitioning.standard_logical_axis_rules()\n    partitioning.PjitPartitioner.model_parallel_submesh = None\n    partitioning.PjitPartitioner.num_partitions = 1\n    \n#### Parameters for utils.RestoreCheckpointConfig:\n\n    utils.RestoreCheckpointConfig.path = []\n    \n#### Parameters for utils.SaveCheckpointConfig:\n\n    utils.SaveCheckpointConfig.dtype = 'float32'\n    utils.SaveCheckpointConfig.keep = None\n    utils.SaveCheckpointConfig.period = %SAVING_PERIOD\n    utils.SaveCheckpointConfig.save_dataset = False\n    \n#### Parameters for seqio.SentencePieceVocabulary:\n\n    seqio.SentencePieceVocabulary.extra_ids = 100\n    seqio.SentencePieceVocabulary.sentencepiece_model_file = \\\n        'gs://improved-t5/vocabs/tokenizer.model'\n    \n#### Parameters for network.T5Config:\n\n    network.T5Config.dropout_rate = %DROPOUT_RATE\n    network.T5Config.dtype = 'bfloat16'\n    network.T5Config.emb_dim = 768\n    network.T5Config.head_dim = 64\n    network.T5Config.logits_via_embedding = False\n    network.T5Config.mlp_activations = ('gelu', 'linear')\n    network.T5Config.mlp_dim = 2048\n    network.T5Config.num_decoder_layers = 12\n    network.T5Config.num_encoder_layers = 12\n    network.T5Config.num_heads = 12\n    network.T5Config.remat_policy = 'minimal'\n    network.T5Config.scan_layers = True\n    network.T5Config.vocab_size = 32128\n    \n#### Parameters for train_script.train:\n\n    train_script.train.checkpoint_cfg = @utils.CheckpointConfig()\n    train_script.train.eval_period = %SAVING_PERIOD\n    train_script.train.eval_steps = 100\n    train_script.train.infer_eval_dataset_cfg = @infer_eval/utils.DatasetConfig()\n    train_script.train.inference_evaluator_cls = @seqio.Evaluator\n    train_script.train.model = %MODEL\n    train_script.train.model_dir = %MODEL_DIR\n    train_script.train.partitioner = @partitioning.PjitPartitioner()\n    train_script.train.random_seed = %RANDOM_SEED\n    train_script.train.run_eval_before_training = True\n    train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config\n    train_script.train.total_steps = %TRAIN_STEPS\n    train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()\n    train_script.train.train_eval_dataset_cfg = @train_eval/utils.DatasetConfig()\n    train_script.train.trainer_cls = @trainer.Trainer\n    train_script.train.use_hardware_rng = %USE_HARDWARE_RNG\n    \n#### Parameters for trainer.Trainer:\n\n    trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()\n    trainer.Trainer.num_microbatches = None\n    \n#### Parameters for network.Transformer:\n\n    network.Transformer.config = @network.T5Config()""",4.883385,0.7407248,1.5258789e-05,0.0010025135,0.0010000005,95848.234,7.455415
base,1010000,,-0.93155295,224134.31,-1.1433982,2048.0068,2048.0068,224162.05,-1.1435397,-1.1416076,1.0016924,0.9920439,1.0016924,1.2856314,3077.6887,20480000.0,6654.344,207.94826,3.2491915,-636921.9,23706.102,27.244696,-0.00013898603,1.052194571200038,,,0.79521203,,0.0009975114,0.0009950376,3243.6624,9.255008
base,1020000,,-0.93124336,224366.0,-1.1445801,2048.007,2048.007,224393.52,-1.1447204,-1.1427859,1.0016929,0.99204236,1.0016929,1.2856904,3072.7302,20480000.0,6665.0825,208.28383,3.2544348,-637949.7,23744.357,27.17168,-0.00013861354,1.052167518945343,,,0.78867435,,0.000992588,0.000990148,6334.735,7.9013267
base,1030000,,-0.93155605,224018.12,-1.1428055,2048.0083,2048.0083,224046.45,-1.1429499,-1.1410244,1.0016876,0.99206585,1.0016876,1.284841,3073.0566,20480000.0,6664.3745,208.2617,3.254089,-637881.94,23741.834,27.087505,-0.00013818413,1.0521348026436033,,,0.76058054,,0.0009877331,0.0009853297,9424.805,7.9324245
base,1040000,,-0.93183285,223769.08,-1.141535,2048.0063,2048.0063,223795.7,-1.1416708,-1.1397464,1.0016885,0.99206215,1.0016885,1.2849734,3072.8604,20480000.0,6664.8,208.275,3.2542968,-637922.7,23743.35,27.029415,-0.00013788778,1.0522384665944209,,,0.7848244,,0.0009829515,0.0009805812,12514.648,7.834324
base,1050000,,-0.9319227,223619.47,-1.1407717,2048.0073,2048.0073,223646.12,-1.1409078,-1.138983,1.0016898,0.99205583,1.0016898,1.2852008,3072.881,20480000.0,6664.755,208.27359,3.2542748,-637918.3,23743.19,26.913055,-0.00013729419,1.0521139091686174,,,0.7816162,,0.0009782402,0.0009759005,15604.473,7.85116
base,1060000,,-0.9322073,223298.94,-1.1391366,2048.0076,2048.0076,223326.12,-1.1392753,-1.1373512,1.0016918,0.99204624,1.0016918,1.2855432,3073.1243,20480000.0,6664.228,208.25713,3.2540176,-637867.9,23741.312,26.988556,-0.00013767935,1.052067658816473,,,0.79006433,,0.0009735984,0.0009712863,18694.564,7.905303
base,1070000,,-0.93199176,223488.66,-1.1401044,2048.0059,2048.0059,223515.08,-1.1402392,-1.1383064,1.001698,0.9920179,1.001698,1.2865641,3072.9163,20480000.0,6664.679,208.27122,3.254238,-637911.06,23742.92,26.934456,-0.00013740336,1.0520619179617232,,,0.78647566,,0.0009689927,0.000966737,21784.518,8.138274
base,1080000,,-0.9322549,223238.86,-1.1388301,2048.0063,2048.0063,223265.78,-1.1389675,-1.1370518,1.0016848,0.9920799,1.0016848,1.2843387,3072.9316,20480000.0,6664.6455,208.27017,3.2542214,-637907.9,23742.8,26.748777,-0.00013645615,1.0520563231888205,,,0.78611374,,0.0009644823,0.0009622509,24874.682,7.789437
base,1090000,,-0.93236154,223098.92,-1.1381162,2048.007,2048.007,223124.81,-1.1382483,-1.1363227,1.0016946,0.9920336,1.0016946,1.2859988,3073.1475,20480000.0,6664.1772,208.25554,3.2539928,-637863.06,23741.13,26.702349,-0.0001362193,1.0520661482055456,,,0.7908902,,0.00096004637,0.0009578268,27964.738,7.82201
base,1100000,,-0.9326267,222839.75,-1.1367941,2048.008,2048.008,222867.42,-1.1369352,-1.1350337,1.0016752,0.9921238,1.0016752,1.2827599,3072.8945,20480000.0,6664.726,208.27269,3.2542608,-637915.6,23743.086,26.617258,-0.00013578522,1.0520056835698004,,,0.7884631,,0.0009556196,0.000953463,31054.5,7.991209
base,1110000,,-0.9326883,222738.97,-1.13628,2048.0076,2048.0076,222765.12,-1.1364135,-1.1345115,1.0016764,0.99211806,1.0016764,1.2829648,3072.894,20480000.0,6664.7266,208.2727,3.254261,-637915.6,23743.09,26.551327,-0.00013544886,1.0520088144300124,,,0.79486704,,0.00095131237,0.00094915845,34144.496,8.050493
base,1120000,,-0.9327357,222669.16,-1.1359239,2048.007,2048.007,222695.52,-1.1360583,-1.1341418,1.0016898,0.9920558,1.0016898,1.285203,3073.0886,20480000.0,6664.3047,208.25952,3.254055,-637875.25,23741.586,26.43078,-0.00013483391,1.0519893259476096,,,0.78555655,,0.0009470065,0.00094491156,37234.75,8.347588
base,1130000,,-0.932804,222556.69,-1.1353501,2048.0068,2048.0068,222582.86,-1.1354836,-1.1335696,1.0016884,0.9920633,1.0016884,1.2849387,3072.8708,20480000.0,6664.7773,208.27429,3.2542858,-637920.5,23743.27,26.422049,-0.00013478937,1.0519918628777885,,,0.7767873,,0.00094282895,0.0009407213,40325.016,8.011095
base,1140000,,-0.9329643,222370.78,-1.1344017,2048.0068,2048.0068,222396.61,-1.1345335,-1.1326231,1.0016867,0.9920708,1.0016867,1.2846667,3072.8435,20480000.0,6664.8364,208.27614,3.2543147,-637926.1,23743.48,26.337893,-0.00013436006,1.0519685819795788,,,0.7700083,,0.00093864615,0.0009365862,43414.957,8.494627
base,1150000,,-0.93275255,222695.8,-1.1360598,2048.0068,2048.0068,222722.5,-1.136196,-1.1342776,1.0016912,0.9920493,1.0016912,1.2854387,3073.0916,20480000.0,6664.299,208.25934,3.2540522,-637874.7,23741.564,26.315958,-0.00013424816,1.051922830093193,,,0.7829652,,0.0009345193,0.00093250524,46505.63,8.020438
base,1160000,,-0.9327829,222659.19,-1.135873,2048.0083,2048.0083,222686.08,-1.1360102,-1.1340947,1.001689,0.99206024,1.001689,1.2850446,3073.051,20480000.0,6664.387,208.2621,3.2540953,-637883.1,23741.879,26.246443,-0.00013389354,1.0519958640165206,,,0.8188646,,0.00093050546,0.0009284771,49595.797,8.022173
base,1170000,,-0.9326289,222738.69,-1.1362785,2048.0073,2048.0073,222764.88,-1.1364121,-1.1344974,1.0016876,0.9920658,1.0016876,1.2848444,3073.0022,20480000.0,6664.4927,208.2654,3.2541468,-637893.25,23742.254,26.22029,-0.00013376011,1.0519424541278708,,,0.7814338,,0.00092649757,0.0009245007,52685.957,7.8698454
base,1180000,,-0.93289995,222436.38,-1.1347363,2048.0063,2048.0063,222463.17,-1.134873,-1.1329606,1.0016881,0.99206406,1.0016881,1.2849058,3073.3418,20480000.0,6663.756,208.24237,3.253787,-637822.7,23739.63,26.146303,-0.00013338268,1.0519175037512527,,,0.8127754,,0.00092254323,0.00092057505,55776.223,8.099537
base,1190000,,-0.9330228,222369.62,-1.1343958,2048.0059,2048.0059,222395.31,-1.1345268,-1.1326143,1.0016887,0.9920612,1.0016887,1.2850083,3073.101,20480000.0,6664.278,208.25868,3.254042,-637872.7,23741.49,26.087719,-0.00013308383,1.0519034083952161,,,0.805305,,0.0009186377,0.0009166989,58866.504,7.945513
base,1200000,,-0.9331098,222236.47,-1.1337165,2048.0068,2048.0068,222262.47,-1.1338491,-1.1319361,1.00169,0.9920547,1.00169,1.2852426,3072.9788,20480000.0,6664.543,208.26697,3.2541714,-637898.06,23742.436,26.014683,-0.00013271124,1.0518909719841054,,,0.79788566,,0.0009147812,0.0009128713,61956.56,7.926218
base,1210000,,-0.93346596,221891.7,-1.1319578,2048.0076,2048.0076,221917.67,-1.1320902,-1.1301888,1.0016823,0.99209154,1.0016823,1.2839211,3073.1562,20480000.0,6664.158,208.25494,3.2539835,-637861.25,23741.062,25.911194,-0.0001321833,1.0518773107240653,,,0.7843759,,0.0009109817,0.00090909126,65046.73,7.108296
base,1220000,,-0.932452,223057.12,-1.137903,2048.008,2048.008,223083.44,-1.1380372,-1.136107,1.001699,0.99201286,1.001699,1.2867454,3072.9912,20480000.0,6664.516,208.26613,3.2541583,-637895.5,23742.34,25.9197,-0.00013222669,1.0518652209993722,,,0.8292055,,0.00090723217,0.0009053578,68135.88,7.6611156
base,1230000,,-0.93272007,222778.92,-1.1364838,2048.0085,2048.0085,222804.39,-1.1366137,-1.1346986,1.0016878,0.99206525,1.0016878,1.2848622,3072.9036,20480000.0,6664.7065,208.27208,3.2542512,-637913.7,23743.018,25.888056,-0.00013206525,1.0518783501455347,,,0.83082604,,0.0009035372,0.0009016701,71225.59,7.8390408
base,1240000,,-0.9328958,222520.98,-1.135168,2048.0078,2048.0078,222547.06,-1.135301,-1.1333803,1.0016947,0.9920338,1.0016947,1.2859968,3073.0867,20480000.0,6664.3096,208.25967,3.2540574,-637875.7,23741.604,25.828894,-0.00013176345,1.0518359296451776,,,0.7723708,,0.00089985144,0.0008980269,74315.66,7.4120054
base,1250000,,-0.9330736,222313.48,-1.1341095,2048.0088,2048.0088,222339.5,-1.1342422,-1.1323369,1.0016826,0.99208945,1.0016826,1.2839935,3072.9277,20480000.0,6664.654,208.27043,3.2542255,-637908.7,23742.83,25.759878,-0.00013141138,1.0518077927907306,,,0.81879354,,0.0008962217,0.0008944276,77405.05,7.4345536
base,1260000,,-0.9330745,222286.88,-1.1339736,2048.0068,2048.0068,222312.78,-1.1341058,-1.132196,1.0016868,0.99206936,1.0016868,1.2847124,3073.0166,20480000.0,6664.4614,208.26442,3.2541316,-637890.25,23742.145,25.66977,-0.0001309517,1.0518244219941824,,,0.84052587,,0.0008926151,0.00089087116,80494.62,7.7217293
base,1270000,,-0.9333344,222031.48,-1.1326708,2048.0066,2048.0066,222057.16,-1.1328018,-1.1308936,1.0016873,0.99206835,1.0016873,1.2847567,3073.1233,20480000.0,6664.2295,208.25717,3.2540183,-637868.06,23741.318,25.634645,-0.00013077252,1.0517657548018848,,,0.79938364,,0.00088913785,0.0008873568,83584.664,8.666105
base,1280000,,-0.93348855,221820.72,-1.1315956,2048.007,2048.007,221845.94,-1.1317242,-1.1298218,1.0016838,0.99208397,1.0016838,1.2841911,3072.9963,20480000.0,6664.5054,208.2658,3.254153,-637894.5,23742.3,25.580976,-0.00013049872,1.0518145468859208,,,0.77218723,,0.00088561507,0.00088388385,86675.39,368.0463
base,1290000,,-0.9334929,221780.94,-1.1313927,2048.0051,2048.0051,221805.88,-1.1315199,-1.1296242,1.0016781,0.99211055,1.0016781,1.2832344,3073.0715,20480000.0,6664.3423,208.2607,3.2540734,-637878.9,23741.719,25.512129,-0.00013014751,1.0517666861077497,,,0.776325,,0.00088214036,0.0008804512,90125.6,7.812826
base,1300000,,-0.93338877,221994.27,-1.1324809,2048.007,2048.007,222019.86,-1.1326115,-1.1306918,1.0016978,0.9920184,1.0016978,1.2865459,3073.205,20480000.0,6664.0527,208.25165,3.253932,-637851.1,23740.688,25.64053,-0.00013080252,1.0517024030733484,,,0.78528404,,0.0008787652,0.00087705837,93215.695,7.3795357
base,1310000,,-0.93317866,222184.8,-1.1334529,2048.009,2048.009,222210.78,-1.1335855,-1.1316714,1.0016913,0.9920486,1.0016913,1.2854605,3073.067,20480000.0,6664.352,208.261,3.2540781,-637879.75,23741.754,25.462217,-0.00012989288,1.0517700830720973,,,0.7901869,,0.0008753525,0.0008737044,96305.26,7.39861
base,1320000,,-0.9333022,222029.12,-1.1326588,2048.0076,2048.0076,222054.5,-1.1327882,-1.1308838,1.001684,0.99208367,1.001684,1.284205,3073.0227,20480000.0,6664.448,208.264,3.254125,-637889.0,23742.098,25.34719,-0.00012930609,1.0517384771542528,,,0.76530886,,0.00087205716,0.0008703886,99394.766,7.462948
base,1330000,,-0.93367827,221657.47,-1.1307628,2048.0068,2048.0068,221682.05,-1.1308882,-1.1289905,1.0016809,0.9920981,1.0016809,1.2836843,3073.1855,20480000.0,6664.0947,208.25296,3.2539525,-637855.2,23740.838,25.33574,-0.00012924768,1.0516509814600659,,,0.7572646,,0.0008687168,0.00086711027,102484.44,7.709215
base,1340000,,-0.93385637,221432.5,-1.1296151,2048.0068,2048.0068,221457.45,-1.1297425,-1.1278327,1.0016934,0.9920394,1.0016934,1.2857926,3073.7424,20480000.0,6662.8877,208.21524,3.2533631,-637739.6,23736.54,25.274883,-0.00012893723,1.0515802250308608,,,0.75528026,,0.00086548785,0.0008638687,105574.945,7.3470945
base,1350000,,-0.93373126,221556.22,-1.1302463,2048.0063,2048.0063,221581.77,-1.1303766,-1.1284747,1.0016854,0.9920767,1.0016854,1.2844527,3072.9966,20480000.0,6664.505,208.26578,3.2541528,-637894.44,23742.299,25.220867,-0.00012866166,1.0516606238502793,,,0.8392823,,0.000862281,0.00086066325,108664.38,7.887641
base,1360000,,-0.93400866,221272.16,-1.1287972,2048.006,2048.006,221297.5,-1.1289265,-1.1270351,1.0016781,0.99211067,1.0016781,1.2832326,3073.2043,20480000.0,6664.054,208.2517,3.2539327,-637851.3,23740.693,25.193941,-0.0001285243,1.0516128669561333,,,0.7396121,,0.00085906,0.00085749326,111754.61,7.594075
base,1370000,,-0.9340953,221168.92,-1.1282705,2048.0068,2048.0068,221194.31,-1.1284001,-1.1264892,1.0016963,0.9920255,1.0016963,1.2862923,3072.958,20480000.0,6664.5884,208.26839,3.2541935,-637902.44,23742.596,25.159254,-0.00012834735,1.0516446525447027,,,0.76696587,,0.0008559233,0.000854358,114844.195,7.468458
base,1380000,,-0.93399906,221252.02,-1.1286944,2048.0066,2048.0066,221277.0,-1.1288218,-1.1269296,1.0016791,0.9921062,1.0016791,1.2833917,3072.9446,20480000.0,6664.6177,208.2693,3.2542078,-637905.2,23742.701,25.070206,-0.00012789307,1.0515891851412282,,,0.7721536,,0.0008528227,0.00085125683,117933.66,7.88068
base,1390000,,-0.9343215,220887.58,-1.1268352,2048.0068,2048.0068,220912.69,-1.1269634,-1.1250674,1.0016853,0.99207723,1.0016853,1.2844335,3073.2356,20480000.0,6663.9863,208.24957,3.2538996,-637844.8,23740.451,25.042135,-0.00012774988,1.0516107030126405,,,0.7357912,,0.0008497159,0.00084818923,121023.82,7.433028
base,1400000,,-0.934486,220687.61,-1.1258152,2048.0059,2048.0059,220713.03,-1.1259449,-1.1240618,1.0016751,0.9921241,1.0016751,1.2827477,3073.101,20480000.0,6664.278,208.25868,3.254042,-637872.7,23741.49,25.015013,-0.0001276115,1.0516019940942127,,,0.73793197,,0.00084665354,0.0008451545,124113.445,8.0954685
base,1410000,,-0.934288,220876.61,-1.1267793,2048.0066,2048.0066,220901.31,-1.1269054,-1.1250027,1.0016912,0.99204993,1.0016912,1.2854162,3073.0283,20480000.0,6664.436,208.26363,3.2541192,-637887.8,23742.053,24.951773,-0.0001272889,1.0515213983827592,,,0.8192041,,0.00084364426,0.0008421522,127203.58,7.771409
base,1420000,,-0.93468314,220478.45,-1.1247482,2048.0068,2048.0068,220503.23,-1.1248746,-1.1229873,1.0016807,0.992099,1.0016807,1.2836541,3073.2583,20480000.0,6663.937,208.24803,3.2538755,-637840.06,23740.275,24.900013,-0.00012702486,1.0515677517356334,,,0.74386287,,0.0008406639,0.00083918165,130293.73,7.644421
base,1430000,,-0.9347277,220419.38,-1.1244467,2048.0073,2048.0073,220445.23,-1.1245787,-1.122691,1.0016813,0.99209523,1.0016813,1.2837853,3073.1145,20480000.0,6664.2485,208.25777,3.2540276,-637869.9,23741.387,24.855043,-0.00012679545,1.0515298768156054,,,0.7575476,,0.0008377118,0.00083624234,133383.64,7.3542786
base,1440000,,-0.93399423,221313.92,-1.1290102,2048.008,2048.008,221339.75,-1.1291419,-1.1272404,1.0016869,0.99206924,1.0016869,1.284718,3073.0461,20480000.0,6664.397,208.2624,3.2541,-637884.1,23741.914,25.35587,-0.00012935036,1.0515059013071204,,,0.75677514,,0.0008347885,0.0008333336,136473.08,7.8439693
base,1450000,,-0.93395144,221527.11,-1.1300977,2048.0076,2048.0076,221551.69,-1.1302232,-1.1283183,1.0016882,0.992063,1.0016882,1.2849443,3073.2566,20480000.0,6663.941,208.24815,3.2538774,-637840.44,23740.291,24.859552,-0.00012681846,1.0514509197112718,,,0.8201072,,0.00083189586,0.0008304551,139563.2,7.539914
base,1460000,,-0.9339454,221396.19,-1.1294299,2048.0059,2048.0059,221421.11,-1.129557,-1.1276509,1.0016904,0.9920532,1.0016904,1.285296,3073.0168,20480000.0,6664.461,208.2644,3.2541313,-637890.2,23742.143,24.788242,-0.00012645467,1.0514515949176413,,,0.7600062,,0.0008290311,0.0008276061,142652.9,7.4737215
base,1470000,,-0.9340364,221217.92,-1.1285205,2048.008,2048.008,221242.36,-1.1286452,-1.1267396,1.0016912,0.9920492,1.0016912,1.2854389,3073.008,20480000.0,6664.48,208.265,3.2541406,-637892.06,23742.209,24.70175,-0.00012601343,1.0514602370177786,,,0.7747114,,0.0008261957,0.0008247863,145742.44,7.6900625
base,1480000,,-0.93441796,220852.98,-1.1266588,2048.0076,2048.0076,220877.36,-1.1267831,-1.1248873,1.0016853,0.99207664,1.0016853,1.2844521,3073.139,20480000.0,6664.196,208.25612,3.2540019,-637864.8,23741.197,24.68553,-0.00012593069,1.0514110849426073,,,0.79036474,,0.00082338724,0.00082199514,148832.36,7.5624247
base,1490000,,-0.9344547,220814.89,-1.1264645,2048.0068,2048.0068,220839.95,-1.1265924,-1.1247032,1.0016798,0.992103,1.0016798,1.2835071,3072.9333,20480000.0,6664.6416,208.27005,3.2542195,-637907.5,23742.785,24.677317,-0.0001258888,1.0513946324991732,,,0.8058388,,0.0008206108,0.0008192322,151921.94,7.2201805
base,1500000,,-0.9345822,220646.06,-1.1256032,2048.008,2048.008,220670.64,-1.1257286,-1.1238387,1.0016817,0.992094,1.0016817,1.2838321,3072.9375,20480000.0,6664.6323,208.26976,3.254215,-637906.6,23742.754,24.58941,-0.00012544035,1.0513578578163314,,,0.7862222,,0.00081786537,0.00081649685,155011.19,7.8588552
base,1510000,,-0.9344839,220707.58,-1.1259171,2048.0068,2048.0068,220731.92,-1.1260413,-1.1241388,1.0016923,0.9920445,1.0016923,1.2856116,3073.3276,20480000.0,6663.7866,208.24333,3.253802,-637825.7,23739.74,24.553709,-0.00012525823,1.0513926149417951,,,0.7738979,,0.0008151271,0.00081378873,158101.48,8.490581
base,1520000,,-0.9348043,220387.34,-1.1242834,2048.0068,2048.0068,220411.44,-1.1244063,-1.1225048,1.001694,0.9920364,1.001694,1.2858971,3073.0222,20480000.0,6664.449,208.26404,3.2541256,-637889.06,23742.1,24.54349,-0.00012520609,1.0514134260222074,,,0.76371574,,0.0008124503,0.0008111074,161192.12,7.1607947
base,1530000,,-0.9346575,220515.42,-1.1249368,2048.007,2048.007,220539.92,-1.1250618,-1.1231626,1.001691,0.9920506,1.001691,1.2853898,3073.0151,20480000.0,6664.4644,208.26451,3.254133,-637890.56,23742.154,24.512188,-0.00012504641,1.0513437007560744,,,0.77523446,,0.00080978975,0.0008084524,164281.38,7.432748
base,1540000,,-0.9348125,220378.56,-1.1242386,2048.0083,2048.0083,220403.19,-1.1243643,-1.122472,1.0016857,0.9920749,1.0016857,1.2845168,3073.275,20480000.0,6663.9014,208.24692,3.253858,-637836.6,23740.148,24.487059,-0.00012491821,1.0513401555731359,,,0.7730315,,0.00080713385,0.00080582325,167371.16,8.149244
base,1550000,,-0.93506175,220080.25,-1.1227168,2048.007,2048.007,220104.52,-1.1228406,-1.1209567,1.0016806,0.992099,1.0016806,1.2836521,3072.9973,20480000.0,6664.5034,208.26573,3.254152,-637894.25,23742.293,24.41153,-0.0001245329,1.0513495075204755,,,0.8117726,,0.0008045286,0.0008032196,170461.36,7.7155585
base,1560000,,-0.93495524,220180.22,-1.1232268,2048.007,2048.007,220204.69,-1.1233516,-1.1214594,1.0016874,0.9920676,1.0016874,1.2847805,3073.044,20480000.0,6664.402,208.26256,3.2541025,-637884.56,23741.932,24.375069,-0.0001243469,1.0513248717853962,,,0.760694,,0.0008019294,0.000800641,173551.25,7.3265386
base,1570000,,-0.9349803,220143.25,-1.1230382,2048.0068,2048.0068,220168.22,-1.1231655,-1.1212777,1.0016837,0.9920846,1.0016837,1.2841673,3073.2483,20480000.0,6663.959,208.24872,3.2538862,-637842.2,23740.354,24.316383,-0.00012404754,1.0513355296467615,,,0.75396085,,0.0007993572,0.00079808716,176640.89,7.6845484
base,1580000,,-0.9351849,219901.28,-1.1218038,2048.008,2048.008,219925.3,-1.1219263,-1.1200415,1.0016828,0.9920887,1.0016828,1.2840204,3073.0637,20480000.0,6664.359,208.26122,3.2540815,-637880.44,23741.78,24.306896,-0.00012399914,1.0513250379857035,,,0.8025558,,0.0007968276,0.0007955575,179730.7,7.5344744
base,1590000,,-0.9351057,220022.27,-1.122421,2048.0068,2048.0068,220046.64,-1.1225454,-1.1206673,1.0016757,0.99212134,1.0016757,1.2828461,3073.087,20480000.0,6664.309,208.25966,3.2540572,-637875.7,23741.6,24.298923,-0.00012395847,1.051274317111612,,,0.7858546,,0.0007943035,0.00079305185,182820.4,7.4450865
base,1600000,,-0.93492687,220247.16,-1.1235683,2048.0059,2048.0059,220271.61,-1.123693,-1.1217887,1.0016974,0.9920205,1.0016974,1.2864755,3073.2932,20480000.0,6663.8613,208.24567,3.2538385,-637832.8,23740.006,24.263826,-0.00012377942,1.051325167288668,,,0.7738795,,0.00079179695,0.00079056964,185910.22,7.6508617
base,1610000,,-0.93477273,220391.3,-1.1243036,2048.0076,2048.0076,220415.05,-1.1244247,-1.1225392,1.0016798,0.992103,1.0016798,1.2835081,3073.0942,20480000.0,6664.293,208.25916,3.2540493,-637874.1,23741.543,24.234509,-0.00012362986,1.0512958704738358,,,0.7458904,,0.00078933634,0.0007881106,189000.0,7.78155
base,1620000,,-0.9351679,219879.39,-1.1216922,2048.0068,2048.0068,219904.14,-1.1218184,-1.1199247,1.0016909,0.9920507,1.0016909,1.285385,3073.1184,20480000.0,6664.24,208.2575,3.2540236,-637869.1,23741.355,24.507883,-0.00012502445,1.0512960887928162,,,0.75379395,,0.00078689476,0.00078567443,192089.89,7.3616223
base,1630000,,-0.93522704,219900.64,-1.1218005,2048.0068,2048.0068,219924.81,-1.1219238,-1.1200384,1.0016834,0.9920861,1.0016834,1.2841138,3073.2573,20480000.0,6663.9395,208.24811,3.2538767,-637840.3,23740.283,24.166967,-0.00012328531,1.0512630920479464,,,0.75611854,,0.00078447256,0.0007832607,195179.53,7.6442356
base,1640000,,-0.9354111,219695.61,-1.1207546,2048.0063,2048.0063,219719.44,-1.1208762,-1.1189849,1.0016901,0.9920544,1.0016901,1.2852536,3073.0364,20480000.0,6664.4185,208.26308,3.2541106,-637886.2,23741.99,24.130056,-0.00012309701,1.0512860302264422,,,0.7922766,,0.0007820694,0.00078086904,198269.28,7.561494
base,1650000,,-0.9355225,219586.53,-1.1201981,2048.0059,2048.0059,219610.64,-1.1203212,-1.1184399,1.001682,0.9920922,1.001682,1.2838944,3073.0215,20480000.0,6664.4507,208.26408,3.2541263,-637889.25,23742.105,24.070534,-0.00012279335,1.0512849024031763,,,0.7981913,,0.0007796874,0.0007784992,201358.9,7.6264086
base,1660000,,-0.9348057,220421.31,-1.1244568,2048.006,2048.006,220445.67,-1.1245809,-1.1226774,1.0016955,0.9920296,1.0016955,1.2861452,3073.2034,20480000.0,6664.056,208.25175,3.2539337,-637851.5,23740.7,24.077728,-0.00012283005,1.0512033322025491,,,0.8035352,,0.00077732676,0.00077615073,204448.81,7.3581257
base,1670000,,-0.9348994,220383.89,-1.1242658,2048.0088,2048.0088,220407.98,-1.1243887,-1.1224926,1.0016892,0.9920586,1.0016892,1.2851027,3073.0388,20480000.0,6664.413,208.26291,3.254108,-637885.6,23741.973,24.056875,-0.00012272368,1.0512896803539196,,,0.79239774,,0.00077498663,0.00077382347,207538.34,7.8725634
base,1680000,,-0.93496716,220213.06,-1.1233944,2048.0059,2048.0059,220237.11,-1.1235169,-1.121623,1.0016886,0.992062,1.0016886,1.2849827,3073.0444,20480000.0,6664.4014,208.26254,3.2541022,-637884.5,23741.93,24.052809,-0.00012270293,1.0512078409265637,,,0.80258775,,0.000772665,0.00077151705,210628.45,7.2840395
base,1690000,,-0.93505025,220104.44,-1.1228402,2048.008,2048.008,220128.58,-1.1229633,-1.1210649,1.0016935,0.9920386,1.0016935,1.285819,3073.181,20480000.0,6664.105,208.25328,3.2539575,-637856.1,23740.875,23.962545,-0.00012224246,1.0511932001169295,,,0.7658479,,0.00077035814,0.000769231,213717.95,7.4259796
base,1700000,,-0.9352602,219862.1,-1.1216038,2048.007,2048.007,219886.23,-1.121727,-1.1198397,1.0016854,0.9920764,1.0016854,1.2844607,3073.0266,20480000.0,6664.4395,208.26373,3.2541208,-637888.1,23742.066,23.93217,-0.00012208751,1.0512201195389042,,,0.78574395,,0.00076810434,0.0007669652,216807.44,8.583445
base,1710000,,-0.9353323,219784.03,-1.1212057,2048.008,2048.008,219808.38,-1.1213299,-1.119445,1.0016838,0.9920845,1.0016838,1.2841731,3073.0322,20480000.0,6664.4277,208.26337,3.254115,-637887.0,23742.023,23.896166,-0.00012190384,1.0512545957423876,,,0.776767,,0.00076584175,0.0007647193,219898.1,7.6370935
base,1720000,,-0.9355171,219555.22,-1.1200384,2048.007,2048.007,219579.2,-1.1201608,-1.1182708,1.0016901,0.99205446,1.0016901,1.2852513,3073.1973,20480000.0,6664.0693,208.25217,3.25394,-637852.75,23740.746,23.864616,-0.00012174289,1.0512306528160298,,,0.79330873,,0.0007636021,0.0007624931,222987.97,8.128228
base,1730000,,-0.9355472,219487.56,-1.1196933,2048.0076,2048.0076,219511.95,-1.1198177,-1.1179326,1.0016863,0.99207234,1.0016863,1.2846076,3073.0325,20480000.0,6664.427,208.26334,3.2541146,-637886.94,23742.021,23.827076,-0.000121551384,1.0511810720120345,,,0.7927458,,0.00076139066,0.0007602861,226078.19,7.5123663
base,1740000,,-0.93555504,219558.95,-1.1200575,2048.007,2048.007,219582.31,-1.1201766,-1.1182913,1.001686,0.9920745,1.001686,1.2845335,3073.0408,20480000.0,6664.409,208.26279,3.254106,-637885.25,23741.957,23.820927,-0.00012152001,1.051217411815755,,,1.4831326,,0.00075918884,0.00075809826,229167.81,8.117613
base,1750000,,-0.9353723,219757.61,-1.1210709,2048.0054,2048.0054,219781.34,-1.121192,-1.1192983,1.0016918,0.9920463,1.0016918,1.2855425,3073.2588,20480000.0,6663.9365,208.24802,3.2538753,-637840.0,23740.273,23.807425,-0.00012145113,1.0511612242266835,,,0.79986453,,0.00075700623,0.00075592916,232258.92,7.9364767
base,1760000,,-0.935228,219895.83,-1.121776,2048.0063,2048.0063,219919.02,-1.1218944,-1.1200032,1.0016885,0.9920621,1.0016885,1.2849776,3072.9753,20480000.0,6664.551,208.26721,3.2541752,-637898.8,23742.463,23.722069,-0.00012101569,1.0511538111426735,,,0.81728697,,0.0007548546,0.0007537786,235348.95,7.5136275
base,1770000,,-0.9355232,219545.52,-1.1199889,2048.0059,2048.0059,219569.98,-1.1201137,-1.1182262,1.0016881,0.99206424,1.0016881,1.284901,3072.9832,20480000.0,6664.5337,208.26668,3.2541668,-637897.2,23742.4,23.71726,-0.000120991164,1.051129701770934,,,0.76113987,,0.00075270765,0.00075164624,238438.61,7.2208815
base,1780000,,-0.9356823,219431.58,-1.1194077,2048.008,2048.008,219454.92,-1.1195267,-1.1176274,1.0016994,0.9920113,1.0016994,1.2868047,3073.2725,20480000.0,6663.9067,208.24709,3.2538607,-637837.1,23740.168,23.717812,-0.00012099398,1.0511076317889196,,,0.7391219,,0.00075058796,0.0007495319,241528.12,7.6339874
base,1790000,,-0.9356114,219458.33,-1.1195441,2048.008,2048.008,219481.52,-1.1196624,-1.1177782,1.0016857,0.9920753,1.0016857,1.2845021,3073.0813,20480000.0,6664.321,208.26003,3.254063,-637876.8,23741.643,23.640783,-0.00012060103,1.0511410095277638,,,0.8005512,,0.00074848067,0.0007474353,244617.86,7.421604
base,1800000,,-0.9358084,219262.44,-1.1185448,2048.0076,2048.0076,219285.9,-1.1186645,-1.1167823,1.0016854,0.99207634,1.0016854,1.2844644,3073.0483,20480000.0,6664.3926,208.26227,3.254098,-637883.6,23741.898,23.654905,-0.000120673074,1.0510899656310977,,,0.74811125,,0.0007463927,0.00074535626,247707.47,7.433235
base,1810000,,-0.9358365,219219.75,-1.118327,2048.0073,2048.0073,219243.61,-1.1184489,-1.1165687,1.0016838,0.9920838,1.0016838,1.2841974,3073.3054,20480000.0,6663.835,208.24484,3.2538257,-637830.3,23739.912,23.614449,-0.00012046669,1.0510934632713747,,,0.7688494,,0.0007443258,0.00074329437,250797.28,7.6297436
base,1820000,,-0.9358512,219202.61,-1.1182395,2048.0063,2048.0063,219225.98,-1.1183589,-1.1164823,1.0016807,0.9920981,1.0016807,1.2836831,3073.031,20480000.0,6664.4297,208.26343,3.254116,-637887.25,23742.031,23.581665,-0.00012029944,1.0510814355199178,,,0.76666045,,0.0007422699,0.00074124953,253886.97,7.4639206
base,1830000,,-0.9359034,219075.83,-1.1175928,2048.0076,2048.0076,219098.8,-1.11771,-1.115832,1.0016831,0.9920876,1.0016831,1.2840623,3073.0293,20480000.0,6664.434,208.26357,3.2541182,-637887.6,23742.047,23.552546,-0.000120150886,1.0510743706507857,,,0.7295582,,0.0007402374,0.00073922146,256976.53,7.178385
base,1840000,,-0.9361783,218778.72,-1.1160772,2048.0078,2048.0078,218801.66,-1.1161941,-1.1143209,1.0016811,0.9920962,1.0016811,1.2837487,3073.2612,20480000.0,6663.931,208.24785,3.2538726,-637839.5,23740.254,23.484463,-0.00011980357,1.0510606220747605,,,0.8018527,,0.0007382143,0.00073721,260065.98,7.4239883
base,1850000,,-0.9361532,218842.75,-1.1164038,2048.0059,2048.0059,218866.1,-1.1165229,-1.1146486,1.0016816,0.99209464,1.0016816,1.283806,3073.041,20480000.0,6664.408,208.26276,3.2541056,-637885.2,23741.955,23.516281,-0.00011996589,1.0510900446382472,,,0.79616046,,0.00073621207,0.00073521485,263155.56,7.7317986
base,1860000,,-0.93616474,218822.34,-1.1162997,2048.0068,2048.0068,218845.72,-1.116419,-1.1145438,1.0016824,0.99209017,1.0016824,1.2839675,3072.9434,20480000.0,6664.62,208.26938,3.254209,-637905.44,23742.709,23.459045,-0.00011967392,1.0510374111517826,,,0.7460921,,0.00073421677,0.00073323573,266245.3,7.6679497
base,1870000,,-0.93642724,218541.92,-1.1148691,2048.0068,2048.0068,218565.38,-1.1149888,-1.1131188,1.00168,0.9921018,1.00168,1.2835498,3073.195,20480000.0,6664.074,208.25232,3.2539425,-637853.2,23740.764,23.409859,-0.00011942299,1.050966803537968,,,0.7486181,,0.00073224993,0.0007312726,269335.16,7.783597
base,1880000,,-0.9358388,219221.17,-1.1183343,2048.0066,2048.0066,219245.17,-1.1184567,-1.1165731,1.0016869,0.99206895,1.0016869,1.2847276,3073.0364,20480000.0,6664.4185,208.26308,3.2541106,-637886.2,23741.99,23.426048,-0.000119505574,1.051051512588078,,,0.756861,,0.0007303058,0.0007293251,272425.0,7.6081038
base,1890000,,-0.93546087,219769.8,-1.1211331,2048.0076,2048.0076,219793.06,-1.1212518,-1.1193558,1.0016938,0.99203706,1.0016938,1.2858756,3072.9746,20480000.0,6664.5527,208.26727,3.2541761,-637899.0,23742.469,23.435333,-0.00011955295,1.051003313283601,,,0.7968056,,0.0007283683,0.00072739314,275514.62,7.78531
base,1900000,,-0.9354647,219696.1,-1.1207571,2048.0054,2048.0054,219720.27,-1.1208804,-1.1189886,1.0016906,0.99205154,1.0016906,1.2853506,3073.2026,20480000.0,6664.058,208.25182,3.2539346,-637851.6,23740.707,23.411064,-0.00011942913,1.050962156461101,,,0.7526088,,0.000726443,0.0007254765,278604.7,7.452529
base,1910000,,-0.9353872,219734.94,-1.1209552,2048.008,2048.008,219757.89,-1.1210723,-1.1191806,1.0016904,0.9920534,1.0016904,1.2852893,3073.1028,20480000.0,6664.2744,208.25858,3.2540402,-637872.4,23741.477,23.392694,-0.00011933543,1.0510059877627962,,,0.7942126,,0.0007245341,0.00072357483,281694.25,7.8133187
base,1920000,,-0.9355824,219478.06,-1.1196448,2048.008,2048.008,219501.44,-1.119764,-1.1178745,1.0016903,0.9920536,1.0016903,1.2852814,3072.9866,20480000.0,6664.5264,208.26645,3.2541633,-637896.5,23742.375,23.35099,-0.00011912268,1.0509785658196473,,,0.7677736,,0.0007226429,0.00072168803,284784.16,7.2326684
base,1930000,,-0.9357337,219372.12,-1.1191044,2048.0076,2048.0076,219394.73,-1.1192197,-1.1173314,1.00169,0.9920552,1.00169,1.2852262,3073.2268,20480000.0,6664.0054,208.25017,3.2539089,-637846.6,23740.52,23.286573,-0.00011879406,1.0509512525136753,,,0.7433522,,0.00072075875,0.0007198159,287873.7,7.286954
base,1940000,,-0.93589413,219145.34,-1.1179475,2048.0068,2048.0068,219168.05,-1.1180633,-1.1161754,1.0016913,0.99204904,1.0016913,1.2854471,3072.9404,20480000.0,6664.6265,208.26958,3.2542121,-637906.06,23742.732,23.254852,-0.00011863224,1.0509738112169718,,,0.77622986,,0.0007188838,0.0007179584,290962.9,7.6684065
base,1950000,,-0.93596876,219052.33,-1.117473,2048.0059,2048.0059,219075.47,-1.117591,-1.1157018,1.0016934,0.99203926,1.0016934,1.2857969,3072.982,20480000.0,6664.5366,208.26677,3.2541683,-637897.44,23742.412,23.211515,-0.00011841116,1.0509618979983055,,,0.7720792,,0.00071702193,0.00071611506,294052.75,7.829586
base,1960000,,-0.93611133,218899.98,-1.1166958,2048.008,2048.008,218923.23,-1.1168144,-1.114932,1.0016884,0.99206275,1.0016884,1.284954,3073.2751,20480000.0,6663.9004,208.24689,3.2538576,-637836.56,23740.146,23.208782,-0.00011839722,1.0509651783003355,,,0.73343396,,0.0007151901,0.0007142859,297142.9,7.774313
base,1970000,,-0.9360386,219010.02,-1.1172571,2048.007,2048.007,219032.98,-1.1173743,-1.115501,1.0016793,0.99210465,1.0016793,1.283445,3073.0117,20480000.0,6664.472,208.26476,3.2541368,-637891.3,23742.182,23.194468,-0.0001183242,1.05093862349541,,,0.7256348,,0.0007133783,0.0007124707,300232.72,8.398264
base,1980000,,-0.9361669,218840.55,-1.1163926,2048.0059,2048.0059,218864.1,-1.1165127,-1.1146278,1.0016911,0.9920502,1.0016911,1.2854055,3072.9927,20480000.0,6664.513,208.26604,3.2541568,-637895.25,23742.328,23.166855,-0.00011818333,1.0509826378936509,,,0.7220752,,0.00071157695,0.0007106692,303323.06,7.865411
base,1990000,,-0.93643826,218503.31,-1.1146722,2048.0063,2048.0063,218525.69,-1.1147864,-1.1129085,1.0016873,0.9920674,1.0016873,1.2847852,3073.1802,20480000.0,6664.1064,208.25333,3.2539582,-637856.25,23740.879,23.128984,-0.000117990145,1.050898742274936,,,0.75122476,,0.00070977944,0.0007088814,306413.1,7.3955398
base,2000000,,-0.93639714,218572.25,-1.115024,2048.007,2048.007,218595.1,-1.1151404,-1.1132601,1.001689,0.99206024,1.001689,1.2850465,3073.0378,20480000.0,6664.415,208.26297,3.254109,-637885.8,23741.979,23.103842,-0.00011786188,1.0508947232362524,,,0.7605791,,0.0007079785,0.00070710696,309502.56,7.932203
xl,0,22.21162,2.7579015e-05,2526061.5,10.819549,2048.0,2048.0,2528818.2,10.831355,10.846395,0.9986135,0.99859595,0.9986135,0.9985992,109.56607,204800.0,1869.1918,14.603061,0.9126913,213087.86,1664.7489,2756.568,0.011806847,1.828024591550536,"b""    from __gin__ import dynamic_registration\n    import __main__ as train_script\n    import data.pile.tasks\n    from models.scalable_t5 import network\n    import seqio\n    from t5x import adafactor\n    from t5x import gin_utils\n    from t5x import models\n    from t5x import partitioning\n    from t5x import trainer\n    from t5x import utils\n    \n#### Macros:\n\n    BATCH_SIZE = 2048\n    DROPOUT_RATE = 0.0\n    EVAL_BATCH_SIZE = 256\n    LABEL_SMOOTHING = 0.0\n    LOSS_NORMALIZING_FACTOR = None\n    MIXTURE_OR_TASK_MODULE = None\n    MIXTURE_OR_TASK_NAME = 'pile_r_denoiser'\n    MODEL = @models.EncoderDecoderModel()\n    MODEL_DIR = 'gs://improved-t5/ckpts/v2_xl_mlm/'\n    OPTIMIZER = @adafactor.Adafactor()\n    RANDOM_SEED = None\n    SAVING_PERIOD = 10000\n    SHUFFLE_TRAIN_EXAMPLES = True\n    TASK_FEATURE_LENGTHS = {'inputs': 512, 'targets': 114}\n    TRAIN_STEPS = 1000000\n    USE_CACHED_TASKS = False\n    USE_HARDWARE_RNG = False\n    VOCABULARY = @seqio.SentencePieceVocabulary()\n    Z_LOSS = 0.0001\n    \n#### Parameters for adafactor.Adafactor:\n\n    adafactor.Adafactor.decay_rate = 0.8\n    adafactor.Adafactor.logical_factor_rules = \\\n        @adafactor.standard_logical_factor_rules()\n    adafactor.Adafactor.step_offset = 0\n    \n#### Parameters for utils.CheckpointConfig:\n\n    utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()\n    utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()\n    \n#### Parameters for utils.create_learning_rate_scheduler:\n\n    utils.create_learning_rate_scheduler.base_learning_rate = 1.0\n    utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'\n    utils.create_learning_rate_scheduler.warmup_steps = 10000\n    \n#### Parameters for infer_eval/utils.DatasetConfig:\n\n    infer_eval/utils.DatasetConfig.batch_size = %EVAL_BATCH_SIZE\n    infer_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    infer_eval/utils.DatasetConfig.seed = 0\n    infer_eval/utils.DatasetConfig.shuffle = False\n    infer_eval/utils.DatasetConfig.split = 'validation'\n    infer_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    \n#### Parameters for train/utils.DatasetConfig:\n\n    train/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train/utils.DatasetConfig.pack = True\n    train/utils.DatasetConfig.seed = None\n    train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES\n    train/utils.DatasetConfig.split = 'train'\n    train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for train_eval/utils.DatasetConfig:\n\n    train_eval/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train_eval/utils.DatasetConfig.pack = True\n    train_eval/utils.DatasetConfig.seed = 42\n    train_eval/utils.DatasetConfig.shuffle = False\n    train_eval/utils.DatasetConfig.split = 'validation'\n    train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for models.EncoderDecoderModel:\n\n    models.EncoderDecoderModel.input_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.label_smoothing = %LABEL_SMOOTHING\n    models.EncoderDecoderModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR\n    models.EncoderDecoderModel.module = @network.Transformer()\n    models.EncoderDecoderModel.optimizer_def = %OPTIMIZER\n    models.EncoderDecoderModel.output_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.z_loss = %Z_LOSS\n    \n#### Parameters for seqio.Evaluator:\n\n    seqio.Evaluator.logger_cls = \\\n        [@seqio.PyLoggingLogger, @seqio.TensorBoardLogger, @seqio.JSONLogger]\n    seqio.Evaluator.num_examples = 2048\n    \n#### Parameters for partitioning.PjitPartitioner:\n\n    partitioning.PjitPartitioner.logical_axis_rules = \\\n        @partitioning.standard_logical_axis_rules()\n    partitioning.PjitPartitioner.model_parallel_submesh = (1, 1, 2, 1)\n    partitioning.PjitPartitioner.num_partitions = 1\n    \n#### Parameters for utils.RestoreCheckpointConfig:\n\n    utils.RestoreCheckpointConfig.path = []\n    \n#### Parameters for utils.SaveCheckpointConfig:\n\n    utils.SaveCheckpointConfig.dtype = 'float32'\n    utils.SaveCheckpointConfig.keep = None\n    utils.SaveCheckpointConfig.period = %SAVING_PERIOD\n    utils.SaveCheckpointConfig.save_dataset = False\n    \n#### Parameters for seqio.SentencePieceVocabulary:\n\n    seqio.SentencePieceVocabulary.extra_ids = 100\n    seqio.SentencePieceVocabulary.sentencepiece_model_file = \\\n        'gs://improved-t5/vocabs/tokenizer.model'\n    \n#### Parameters for network.T5Config:\n\n    network.T5Config.dropout_rate = %DROPOUT_RATE\n    network.T5Config.dtype = 'bfloat16'\n    network.T5Config.emb_dim = 2048\n    network.T5Config.head_dim = 64\n    network.T5Config.logits_via_embedding = False\n    network.T5Config.mlp_activations = ('gelu', 'linear')\n    network.T5Config.mlp_dim = 5120\n    network.T5Config.num_decoder_layers = 24\n    network.T5Config.num_encoder_layers = 24\n    network.T5Config.num_heads = 32\n    network.T5Config.remat_policy = 'minimal'\n    network.T5Config.scan_layers = True\n    network.T5Config.vocab_size = 32128\n    \n#### Parameters for train_script.train:\n\n    train_script.train.checkpoint_cfg = @utils.CheckpointConfig()\n    train_script.train.eval_period = %SAVING_PERIOD\n    train_script.train.eval_steps = 100\n    train_script.train.infer_eval_dataset_cfg = @infer_eval/utils.DatasetConfig()\n    train_script.train.inference_evaluator_cls = @seqio.Evaluator\n    train_script.train.model = %MODEL\n    train_script.train.model_dir = %MODEL_DIR\n    train_script.train.partitioner = @partitioning.PjitPartitioner()\n    train_script.train.random_seed = %RANDOM_SEED\n    train_script.train.run_eval_before_training = True\n    train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config\n    train_script.train.total_steps = %TRAIN_STEPS\n    train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()\n    train_script.train.train_eval_dataset_cfg = @train_eval/utils.DatasetConfig()\n    train_script.train.trainer_cls = @trainer.Trainer\n    train_script.train.use_hardware_rng = %USE_HARDWARE_RNG\n    \n#### Parameters for trainer.Trainer:\n\n    trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()\n    trainer.Trainer.num_microbatches = None\n    \n#### Parameters for network.Transformer:\n\n    network.Transformer.config = @network.T5Config()""",4.6731806,10.037601,1.4543533e-05,,,,
xl,1240000,25.28532,-0.987344,160501.88,-0.8187838,2048.007,2048.007,160518.67,-0.81886953,-0.817492,1.0016851,0.9920779,1.0016851,1.2844076,9926.884,20480000.0,2063.0845,16.117847,1.0073655,-197468.53,1837.4346,16.418945,-8.375956e-05,1.0367821547593794,"b""    from __gin__ import dynamic_registration\n    import __main__ as train_script\n    import data.pile.tasks\n    from models.scalable_t5 import network\n    import seqio\n    from t5x import adafactor\n    from t5x import gin_utils\n    from t5x import models\n    from t5x import partitioning\n    from t5x import trainer\n    from t5x import utils\n    \n#### Macros:\n\n    BATCH_SIZE = 2048\n    DROPOUT_RATE = 0.0\n    EVAL_BATCH_SIZE = 256\n    LABEL_SMOOTHING = 0.0\n    LOSS_NORMALIZING_FACTOR = None\n    MIXTURE_OR_TASK_MODULE = None\n    MIXTURE_OR_TASK_NAME = 'pile_r_denoiser'\n    MODEL = @models.EncoderDecoderModel()\n    MODEL_DIR = 'gs://improved-t5/ckpts/v2_xl_mlm/'\n    OPTIMIZER = @adafactor.Adafactor()\n    RANDOM_SEED = None\n    SAVING_PERIOD = 10000\n    SHUFFLE_TRAIN_EXAMPLES = True\n    TASK_FEATURE_LENGTHS = {'inputs': 512, 'targets': 114}\n    TRAIN_STEPS = 2000000\n    USE_CACHED_TASKS = False\n    USE_HARDWARE_RNG = False\n    VOCABULARY = @seqio.SentencePieceVocabulary()\n    Z_LOSS = 0.0001\n    \n#### Parameters for adafactor.Adafactor:\n\n    adafactor.Adafactor.decay_rate = 0.8\n    adafactor.Adafactor.logical_factor_rules = \\\n        @adafactor.standard_logical_factor_rules()\n    adafactor.Adafactor.step_offset = 0\n    \n#### Parameters for utils.CheckpointConfig:\n\n    utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()\n    utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()\n    \n#### Parameters for utils.create_learning_rate_scheduler:\n\n    utils.create_learning_rate_scheduler.base_learning_rate = 1.0\n    utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'\n    utils.create_learning_rate_scheduler.warmup_steps = 10000\n    \n#### Parameters for infer_eval/utils.DatasetConfig:\n\n    infer_eval/utils.DatasetConfig.batch_size = %EVAL_BATCH_SIZE\n    infer_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    infer_eval/utils.DatasetConfig.seed = 0\n    infer_eval/utils.DatasetConfig.shuffle = False\n    infer_eval/utils.DatasetConfig.split = 'validation'\n    infer_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    \n#### Parameters for train/utils.DatasetConfig:\n\n    train/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train/utils.DatasetConfig.pack = True\n    train/utils.DatasetConfig.seed = None\n    train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES\n    train/utils.DatasetConfig.split = 'train'\n    train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for train_eval/utils.DatasetConfig:\n\n    train_eval/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train_eval/utils.DatasetConfig.pack = True\n    train_eval/utils.DatasetConfig.seed = 42\n    train_eval/utils.DatasetConfig.shuffle = False\n    train_eval/utils.DatasetConfig.split = 'validation'\n    train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for models.EncoderDecoderModel:\n\n    models.EncoderDecoderModel.input_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.label_smoothing = %LABEL_SMOOTHING\n    models.EncoderDecoderModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR\n    models.EncoderDecoderModel.module = @network.Transformer()\n    models.EncoderDecoderModel.optimizer_def = %OPTIMIZER\n    models.EncoderDecoderModel.output_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.z_loss = %Z_LOSS\n    \n#### Parameters for seqio.Evaluator:\n\n    seqio.Evaluator.logger_cls = \\\n        [@seqio.PyLoggingLogger, @seqio.TensorBoardLogger, @seqio.JSONLogger]\n    seqio.Evaluator.num_examples = 2048\n    \n#### Parameters for partitioning.PjitPartitioner:\n\n    partitioning.PjitPartitioner.logical_axis_rules = \\\n        @partitioning.standard_logical_axis_rules()\n    partitioning.PjitPartitioner.model_parallel_submesh = None\n    partitioning.PjitPartitioner.num_partitions = 2\n    \n#### Parameters for utils.RestoreCheckpointConfig:\n\n    utils.RestoreCheckpointConfig.path = []\n    \n#### Parameters for utils.SaveCheckpointConfig:\n\n    utils.SaveCheckpointConfig.dtype = 'float32'\n    utils.SaveCheckpointConfig.keep = None\n    utils.SaveCheckpointConfig.period = %SAVING_PERIOD\n    utils.SaveCheckpointConfig.save_dataset = False\n    \n#### Parameters for seqio.SentencePieceVocabulary:\n\n    seqio.SentencePieceVocabulary.extra_ids = 100\n    seqio.SentencePieceVocabulary.sentencepiece_model_file = \\\n        'gs://improved-t5/vocabs/tokenizer.model'\n    \n#### Parameters for network.T5Config:\n\n    network.T5Config.dropout_rate = %DROPOUT_RATE\n    network.T5Config.dtype = 'bfloat16'\n    network.T5Config.emb_dim = 2048\n    network.T5Config.head_dim = 64\n    network.T5Config.logits_via_embedding = False\n    network.T5Config.mlp_activations = ('gelu', 'linear')\n    network.T5Config.mlp_dim = 5120\n    network.T5Config.num_decoder_layers = 24\n    network.T5Config.num_encoder_layers = 24\n    network.T5Config.num_heads = 32\n    network.T5Config.remat_policy = 'minimal'\n    network.T5Config.scan_layers = True\n    network.T5Config.vocab_size = 32128\n    \n#### Parameters for train_script.train:\n\n    train_script.train.checkpoint_cfg = @utils.CheckpointConfig()\n    train_script.train.eval_period = %SAVING_PERIOD\n    train_script.train.eval_steps = 100\n    train_script.train.infer_eval_dataset_cfg = @infer_eval/utils.DatasetConfig()\n    train_script.train.inference_evaluator_cls = @seqio.Evaluator\n    train_script.train.model = %MODEL\n    train_script.train.model_dir = %MODEL_DIR\n    train_script.train.partitioner = @partitioning.PjitPartitioner()\n    train_script.train.random_seed = %RANDOM_SEED\n    train_script.train.run_eval_before_training = True\n    train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config\n    train_script.train.total_steps = %TRAIN_STEPS\n    train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()\n    train_script.train.train_eval_dataset_cfg = @train_eval/utils.DatasetConfig()\n    train_script.train.trainer_cls = @trainer.Trainer\n    train_script.train.use_hardware_rng = %USE_HARDWARE_RNG\n    \n#### Parameters for trainer.Trainer:\n\n    trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()\n    trainer.Trainer.num_microbatches = None\n    \n#### Parameters for network.Transformer:\n\n    network.Transformer.config = @network.T5Config()""",34.85002,0.89001155,6.67572e-06,0.00089985144,0.0008980269,91303.836,15.373458
xl,1250000,,-0.98651755,161493.7,-0.82384354,2048.007,2048.007,161510.1,-0.8239271,-0.82252735,1.0017018,0.99199945,1.0017018,1.2872257,6049.4316,20480000.0,3385.442,13.224382,1.6530478,-324038.25,1507.5796,16.617884,-8.477443e-05,1.0367769725617018,,,0.8567767,,0.0008962217,0.0008944276,6189.552,27.207767
xl,1260000,,-0.9862905,161664.64,-0.82471555,2048.008,2048.008,161681.28,-0.82480043,-0.82340235,1.0016979,0.99201745,1.0016979,1.2865785,6047.8203,20480000.0,3386.344,13.227906,1.6534883,-324124.6,1507.9813,16.492134,-8.413293e-05,1.0367277522501708,,,0.8556111,,0.0008926151,0.00089087116,12282.3545,24.568642
xl,1270000,,-0.9867547,161227.39,-0.82248497,2048.0068,2048.0068,161244.34,-0.82257146,-0.82118464,1.0016888,0.9920602,1.0016888,1.2850446,6047.8306,20480000.0,3386.3384,13.227884,1.6534855,-324124.06,1507.9788,16.372879,-8.352456e-05,1.0367125990943458,,,0.8285456,,0.00088913785,0.0008873568,18372.555,369.77438
xl,1280000,,-0.98689663,161070.69,-0.8216856,2048.006,2048.006,161087.58,-0.82177174,-0.82038486,1.0016905,0.9920526,1.0016905,1.2853156,6047.81,20480000.0,3386.3496,13.227928,1.653491,-324125.12,1507.9839,16.288095,-8.309204e-05,1.0367600540787323,,,1.0611055,,0.00088561507,0.00088388385,24807.898,25.255548
xl,1290000,,-0.9868851,161072.89,-0.8216968,2048.0073,2048.0073,161088.88,-0.82177836,-0.8203909,1.0016913,0.992049,1.0016913,1.2854486,6047.842,20480000.0,3386.3318,13.227859,1.6534823,-324123.44,1507.9758,16.18637,-8.2573104e-05,1.0367036330609904,,,0.8562963,,0.00088214036,0.0008804512,30898.916,25.47253
xl,1300000,,-0.987051,160892.02,-0.8207741,2048.0063,2048.0063,160907.89,-0.8208551,-0.8194673,1.0016935,0.99203867,1.0016935,1.2858193,6047.8496,20480000.0,3386.3276,13.227842,1.6534803,-324123.03,1507.974,16.085455,-8.20583e-05,1.0367228638509365,,,0.85368824,,0.0008787652,0.00087705837,36989.98,23.938221
xl,1310000,,-0.98712105,160831.31,-0.8204644,2048.0063,2048.0063,160847.19,-0.82054543,-0.8191603,1.0016909,0.99205035,1.0016909,1.2853963,6047.8247,20480000.0,3386.3413,13.227896,1.653487,-324124.34,1507.9801,16.02944,-8.1772545e-05,1.0366858304771978,,,0.84653306,,0.0008753525,0.0008737044,43079.49,24.702856
xl,1320000,,-0.98724926,160691.7,-0.8197522,2048.0068,2048.0068,160707.78,-0.81983423,-0.81845057,1.0016906,0.99205256,1.0016906,1.2853215,6047.8345,20480000.0,3386.336,13.227875,1.6534843,-324123.8,1507.9778,15.932099,-8.127597e-05,1.0367041317193728,,,0.84111905,,0.00087205716,0.0008703886,49169.79,25.07867
xl,1330000,,-0.9871621,160719.62,-0.8198947,2048.0083,2048.0083,160735.16,-0.8199739,-0.8185809,1.0017016,0.99200076,1.0017016,1.2871811,6047.8447,20480000.0,3386.3303,13.227853,1.6534816,-324123.28,1507.9752,15.832529,-8.076802e-05,1.0366821086029152,,,0.8314223,,0.0008687168,0.00086711027,55260.445,370.10522
xl,1340000,,-0.98738617,160547.53,-0.81901675,2048.0068,2048.0068,160562.98,-0.8190956,-0.8177078,1.0016973,0.99202156,1.0016973,1.2864367,6047.806,20480000.0,3386.3518,13.227937,1.6534921,-324125.34,1507.9847,15.769635,-8.044718e-05,1.0366364454222703,,,0.85917044,,0.00086548785,0.0008638687,61696.113,24.213394
xl,1350000,,-0.98752475,160367.78,-0.81809974,2048.0076,2048.0076,160383.42,-0.81817955,-0.81679726,1.0016923,0.9920438,1.0016923,1.2856318,6047.8276,20480000.0,3386.3398,13.22789,1.6534863,-324124.2,1507.9795,15.702915,-8.010681e-05,1.0365975599011377,,,0.8474598,,0.000862281,0.00086066325,67785.98,24.204985
xl,1360000,,-0.98767954,160242.9,-0.8174627,2048.0068,2048.0068,160258.25,-0.817541,-0.8161589,1.0016934,0.99203926,1.0016934,1.2857988,6047.843,20480000.0,3386.3313,13.227857,1.6534821,-324123.38,1507.9756,15.646558,-7.981931e-05,1.0366003518980031,,,0.8459282,,0.00085906,0.00085749326,73875.766,370.95355
xl,1370000,,-0.9877035,160207.9,-0.81728417,2048.0068,2048.0068,160223.61,-0.8173643,-0.8159852,1.0016901,0.9920545,1.0016901,1.2852478,6047.9087,20480000.0,3386.2944,13.227713,1.6534641,-324119.88,1507.9592,15.584912,-7.9504825e-05,1.0366063853142542,,,0.82339454,,0.0008559233,0.000854358,80312.37,24.37364
xl,1380000,,-0.98793256,159992.38,-0.8161847,2048.007,2048.007,160008.16,-0.81626517,-0.8148937,1.0016829,0.99208826,1.0016829,1.2840376,6047.835,20480000.0,3386.3357,13.227874,1.6534842,-324123.78,1507.9777,15.503444,-7.908922e-05,1.0365863814766907,,,0.84423876,,0.0008528227,0.00085125683,86402.31,24.92425
xl,1390000,,-0.9876331,160365.19,-0.81808656,2048.0083,2048.0083,160381.16,-0.818168,-0.8167856,1.0016925,0.9920434,1.0016925,1.2856492,6047.865,20480000.0,3386.319,13.227809,1.6534761,-324122.2,1507.9702,15.487807,-7.900946e-05,1.0364909539668499,,,0.8460493,,0.0008497159,0.00084818923,92492.91,24.64522
xl,1400000,,-0.9903976,157291.0,-0.8024039,2048.0063,2048.0063,157305.89,-0.80247986,-0.8011475,1.0016631,0.99218196,1.0016631,1.2806745,6047.8066,20480000.0,3386.3516,13.227936,1.653492,-324125.3,1507.9846,15.244369,-7.776758e-05,1.0364950359423553,,,0.85058856,,0.00084665354,0.0008451545,98583.125,24.385553
xl,1410000,,-0.99067616,157049.9,-0.8011739,2048.0063,2048.0063,157065.17,-0.8012518,-0.7999198,1.0016652,0.9921714,1.0016652,1.2810524,6047.8135,20480000.0,3386.348,13.2279215,1.6534902,-324124.97,1507.983,15.195495,-7.7518256e-05,1.036473951286587,,,0.8475926,,0.00084364426,0.0008421522,104673.21,24.438715
xl,1420000,,-0.9901094,157727.45,-0.80463046,2048.0076,2048.0076,157742.83,-0.80470884,-0.8033714,1.0016648,0.99217355,1.0016648,1.2809743,6047.868,20480000.0,3386.3171,13.227801,1.6534752,-324122.03,1507.9694,15.227175,-7.767987e-05,1.0364670867400607,,,0.84482837,,0.0008406639,0.00083918165,110763.28,25.386581
xl,1430000,,-0.9886822,159171.17,-0.8119954,2048.0068,2048.0068,159186.42,-0.81207323,-0.8107134,1.0016773,0.9921146,1.0016773,1.283091,6047.995,20480000.0,3386.2458,13.227523,1.6534404,-324115.22,1507.9376,15.295155,-7.802666e-05,1.036435878980205,,,0.82694626,,0.0008377118,0.00083624234,116854.4,24.80046
xl,1440000,,-0.9888369,158981.03,-0.8110254,2048.006,2048.006,158996.61,-0.81110483,-0.8097415,1.0016837,0.9920843,1.0016837,1.2841767,6047.822,20480000.0,3386.343,13.227902,1.6534878,-324124.5,1507.981,15.244559,-7.776855e-05,1.0364558940189914,,,0.85550785,,0.0008347885,0.0008333336,122944.73,23.906113
xl,1450000,,-0.9890279,158800.61,-0.810105,2048.0063,2048.0063,158816.08,-0.8101839,-0.8088218,1.0016841,0.99208313,1.0016841,1.2842233,6047.8623,20480000.0,3386.3206,13.227815,1.6534768,-324122.34,1507.9708,15.186117,-7.747042e-05,1.0364394036100644,,,0.8402879,,0.00083189586,0.0008304551,129034.305,24.014122
xl,1460000,,-0.9890384,158779.83,-0.809999,2048.0073,2048.0073,158795.02,-0.8100765,-0.8087138,1.001685,0.9920787,1.001685,1.2843806,6047.824,20480000.0,3386.3416,13.227897,1.6534871,-324124.38,1507.9802,15.155002,-7.7311684e-05,1.0363804573471984,,,0.8432157,,0.0008290311,0.0008276061,135123.88,24.42931
xl,1470000,,-0.9892576,158556.55,-0.80885994,2048.007,2048.007,158571.31,-0.8089353,-0.80757564,1.0016836,0.9920855,1.0016836,1.2841367,6047.817,20480000.0,3386.3457,13.227913,1.6534891,-324124.78,1507.982,15.0771885,-7.691473e-05,1.0364666106055689,,,0.8653238,,0.0008261957,0.0008247863,141213.83,24.815008
xl,1480000,,-0.9894063,158408.11,-0.8081027,2048.008,2048.008,158422.88,-0.80817807,-0.80682105,1.0016819,0.99209285,1.0016819,1.2838733,6047.855,20480000.0,3386.3247,13.227831,1.6534789,-324122.75,1507.9727,15.028022,-7.6663906e-05,1.0364017112413748,,,0.8510113,,0.00082338724,0.00082199514,147304.39,23.081106
xl,1490000,,-0.9893591,158458.67,-0.80836064,2048.0059,2048.0059,158473.42,-0.8084359,-0.80707735,1.0016834,0.9920865,1.0016834,1.2841018,6047.818,20480000.0,3386.3452,13.227911,1.6534889,-324124.72,1507.9818,15.0115185,-7.6579716e-05,1.0363569874390974,,,0.8383782,,0.0008206108,0.0008192322,153393.06,370.5795
xl,1500000,,-0.9893999,158414.89,-0.8081373,2048.0063,2048.0063,158430.11,-0.80821496,-0.80685693,1.0016831,0.9920872,1.0016831,1.2840734,6047.8193,20480000.0,3386.3447,13.227909,1.6534886,-324124.66,1507.9816,14.922341,-7.6124794e-05,1.0363438677895915,,,0.8584163,,0.00081786537,0.00081649685,159829.17,23.310432
xl,1510000,,-0.9895401,158239.45,-0.80724233,2048.0078,2048.0078,158254.28,-0.807318,-0.8059567,1.001689,0.9920602,1.001689,1.2850477,6047.8423,20480000.0,3386.3315,13.227858,1.6534822,-324123.4,1507.9757,14.888616,-7.595274e-05,1.0363129206073287,,,0.852175,,0.0008151271,0.00081378873,165918.1,23.192698
xl,1520000,,-0.9896345,158211.9,-0.8071018,2048.0059,2048.0059,158226.98,-0.80717874,-0.80582094,1.0016849,0.992079,1.0016849,1.2843708,6047.8076,20480000.0,3386.351,13.227934,1.6534917,-324125.28,1507.9845,14.845309,-7.573182e-05,1.0363359665767005,,,0.8380227,,0.0008124503,0.0008111074,172006.83,23.964544
xl,1530000,,-0.9897603,158029.34,-0.80617046,2048.0066,2048.0066,158044.47,-0.80624765,-0.80489755,1.0016774,0.9921146,1.0016774,1.2830914,6047.8228,20480000.0,3386.3425,13.2279005,1.6534876,-324124.44,1507.9807,14.779395,-7.539557e-05,1.0363222503827128,,,0.84408236,,0.00080978975,0.0008084524,178096.61,25.261333
xl,1540000,,-0.9898026,158023.97,-0.80614305,2048.0068,2048.0068,158038.83,-0.80621886,-0.80487126,1.0016743,0.9921288,1.0016743,1.2825825,6047.853,20480000.0,3386.3254,13.227834,1.6534792,-324122.8,1507.9731,14.757931,-7.5286065e-05,1.0362630287026195,,,0.8490999,,0.00080713385,0.00080582325,184187.5,22.909676
xl,1550000,,-0.989878,157914.33,-0.8055837,2048.0063,2048.0063,157929.03,-0.80565876,-0.804311,1.0016757,0.9921221,1.0016757,1.2828207,6047.8115,20480000.0,3386.349,13.227926,1.6534908,-324125.1,1507.9835,14.719973,-7.509243e-05,1.0362334838010117,,,0.849509,,0.0008045286,0.0008032196,190275.97,23.661552
xl,1560000,,-0.9902066,157609.67,-0.8040296,2048.0056,2048.0056,157624.52,-0.8041052,-0.8027561,1.0016806,0.9920994,1.0016806,1.28364,6047.8174,20480000.0,3386.3455,13.227912,1.653489,-324124.75,1507.9819,14.668562,-7.483016e-05,1.0362621163202976,,,0.8286643,,0.0008019294,0.000800641,196365.17,370.3208
xl,1570000,,-0.9859524,162183.44,-0.8273621,2048.0068,2048.0068,162198.4,-0.82743853,-0.8260149,1.0017235,0.991898,1.0017235,1.2908707,6047.848,20480000.0,3386.3286,13.227846,1.6534808,-324123.12,1507.9744,14.946701,-7.624906e-05,1.0363191661620152,,,0.8403995,,0.0007993572,0.00079808716,202801.06,23.345608
xl,1580000,,-0.98569787,162482.6,-0.82888824,2048.008,2048.008,162497.16,-0.82896256,-0.8275288,1.0017327,0.99185437,1.0017327,1.2924336,6048.0957,20480000.0,3386.1897,13.2273035,1.6534129,-324109.8,1507.9126,14.9721775,-7.637903e-05,1.0362980848640848,,,0.8543825,,0.0007968276,0.0007955575,208890.64,23.168024
xl,1590000,,-0.98630744,161781.88,-0.8253136,2048.0073,2048.0073,161797.16,-0.8253916,-0.82397354,1.001721,0.991909,1.001721,1.2904705,6047.819,20480000.0,3386.345,13.22791,1.6534888,-324124.7,1507.9817,14.841827,-7.571406e-05,1.0363063011427585,,,0.8485799,,0.0007943035,0.00079305185,214979.33,25.558676
xl,1600000,,-0.9878955,160113.78,-0.816804,2048.0076,2048.0076,160128.16,-0.81687737,-0.8154897,1.0017016,0.9920002,1.0017016,1.2871964,6047.839,20480000.0,3386.3335,13.227865,1.6534832,-324123.6,1507.9767,14.769584,-7.534552e-05,1.0362056393573917,,,0.85092354,,0.00079179695,0.00079056964,221070.48,25.000137
xl,1610000,,-0.9883014,159533.92,-0.81384593,2048.007,2048.007,159548.53,-0.81392044,-0.8125357,1.0017042,0.99198866,1.0017042,1.2876155,6047.8257,20480000.0,3386.341,13.227895,1.6534868,-324124.3,1507.98,14.701933,-7.50004e-05,1.0361950166210576,,,0.8565011,,0.00078933634,0.0007881106,227161.1,370.8244
xl,1620000,,-0.9885538,159311.0,-0.8127087,2048.0076,2048.0076,159325.52,-0.81278276,-0.811405,1.0016979,0.99201787,1.0016979,1.2865661,6047.81,20480000.0,3386.3496,13.227928,1.653491,-324125.12,1507.9839,14.64942,-7.473251e-05,1.0362346478595006,,,0.8483019,,0.00078689476,0.00078567443,233597.48,23.210463
xl,1630000,,-0.98889565,158965.0,-0.8109436,2048.0054,2048.0054,158979.61,-0.8110181,-0.80964196,1.0016997,0.99200916,1.0016997,1.2868775,6048.1323,20480000.0,3386.1694,13.227224,1.653403,-324107.88,1507.9036,14.6067,-7.451458e-05,1.0361980491946814,,,0.85049295,,0.00078447256,0.0007832607,239686.69,24.622063
xl,1640000,,-0.98890394,158969.14,-0.81096476,2048.0068,2048.0068,158983.48,-0.8110379,-0.809666,1.0016944,0.9920343,1.0016944,1.285975,6048.3496,20480000.0,3386.0479,13.226749,1.6533437,-324096.25,1507.8494,14.542148,-7.418528e-05,1.0361714002646871,,,0.85024214,,0.0007820694,0.00078086904,245777.47,23.667637
xl,1650000,,-0.98912007,158772.16,-0.8099598,2048.007,2048.007,158786.56,-0.8100334,-0.8086637,1.0016937,0.9920377,1.0016937,1.2858531,6047.8125,20480000.0,3386.3484,13.227923,1.6534904,-324125.03,1507.9833,14.500449,-7.397255e-05,1.036183243864435,,,0.82818556,,0.0007796874,0.0007784992,251866.69,23.287413
xl,1660000,,-0.98914593,158717.9,-0.80968314,2048.0056,2048.0056,158732.67,-0.8097584,-0.8083883,1.0016949,0.99203247,1.0016949,1.286043,6047.834,20480000.0,3386.3362,13.227876,1.6534845,-324123.84,1507.9779,14.48228,-7.387986e-05,1.0361501585331514,,,0.84808826,,0.00077732676,0.00077615073,257955.56,23.613914
xl,1670000,,-0.989199,158645.28,-0.80931264,2048.0073,2048.0073,158659.52,-0.80938524,-0.80801684,1.0016935,0.9920388,1.0016935,1.2858142,6047.8154,20480000.0,3386.3467,13.227917,1.6534896,-324124.84,1507.9825,14.418987,-7.3556985e-05,1.0362044042372176,,,0.8526242,,0.00077498663,0.00077382347,264044.75,23.615229
xl,1680000,,-0.9892564,158569.19,-0.80892444,2048.0076,2048.0076,158583.16,-0.8089957,-0.80762875,1.0016925,0.9920432,1.0016925,1.285656,6047.821,20480000.0,3386.3438,13.227905,1.6534882,-324124.56,1507.9812,14.408649,-7.350425e-05,1.0361047082125743,,,0.8434024,,0.000772665,0.00077151705,270133.94,23.175373
xl,1690000,,-0.98933256,158520.98,-0.8086785,2048.0068,2048.0068,158535.83,-0.80875427,-0.80738974,1.00169,0.99205536,1.00169,1.2852212,6048.1396,20480000.0,3386.1653,13.227208,1.653401,-324107.47,1507.9017,14.375568,-7.333548e-05,1.0361593954440615,,,0.8512647,,0.00077035814,0.000769231,276222.97,23.532753
xl,1700000,,-0.9895094,158361.25,-0.80786365,2048.008,2048.008,158375.44,-0.807936,-0.8065836,1.0016767,0.9921172,1.0016767,1.2829951,6047.805,20480000.0,3386.3523,13.227939,1.6534923,-324125.4,1507.985,14.334749,-7.312725e-05,1.0361733691072368,,,0.831444,,0.00076810434,0.0007669652,282312.1,27.940739
xl,1710000,,-0.98953676,158267.4,-0.80738497,2048.0068,2048.0068,158281.83,-0.80745846,-0.8060927,1.0016943,0.9920348,1.0016943,1.2859586,6047.8193,20480000.0,3386.3447,13.227909,1.6534886,-324124.66,1507.9816,14.274243,-7.2818584e-05,1.036152218418693,,,0.86458707,,0.00076584175,0.0007647193,288405.56,25.070679
xl,1720000,,-0.98985267,157962.69,-0.8058304,2048.0066,2048.0066,157976.92,-0.805903,-0.80454314,1.0016903,0.99205405,1.0016903,1.2852685,6047.8457,20480000.0,3386.3296,13.22785,1.6534812,-324123.22,1507.975,14.230387,-7.2594856e-05,1.036145353303698,,,0.8484323,,0.0007636021,0.0007624931,294496.22,23.308935
xl,1730000,,-0.989841,157962.58,-0.8058299,2048.007,2048.007,157976.56,-0.80590117,-0.80454415,1.0016867,0.99207026,1.0016867,1.2846833,6047.8115,20480000.0,3386.349,13.227926,1.6534908,-324125.1,1507.9835,14.218966,-7.253659e-05,1.0361611308587737,,,0.98416543,,0.00076139066,0.0007602861,300585.06,26.238695
xl,1740000,,-0.9903426,157524.98,-0.80359757,2048.007,2048.007,157539.3,-0.8036705,-0.80231965,1.0016838,0.9920843,1.0016838,1.2841796,6048.419,20480000.0,3386.0088,13.226597,1.6533246,-324092.5,1507.832,14.146367,-7.2166236e-05,1.0361415093838289,,,0.8593631,,0.00075918884,0.00075809826,306677.6,25.181797
xl,1750000,,-0.9913832,156324.0,-0.79747087,2048.0068,2048.0068,156338.2,-0.79754335,-0.7962055,1.0016803,0.9921008,1.0016803,1.2835857,6047.8486,20480000.0,3386.3281,13.227844,1.6534805,-324123.1,1507.9742,14.048415,-7.1666545e-05,1.0361247055024834,,,0.84891605,,0.00075700623,0.00075592916,312768.38,23.292315
xl,1760000,,-0.9921126,155598.52,-0.79376984,2048.0063,2048.0063,155612.64,-0.7938419,-0.7925214,1.0016662,0.9921674,1.0016662,1.2811987,6047.8193,20480000.0,3386.3447,13.227909,1.6534886,-324124.66,1507.9816,14.00774,-7.145905e-05,1.0360780757616037,,,0.86313295,,0.0007548546,0.0007537786,318857.22,23.305449
xl,1770000,,-0.9908762,156970.9,-0.80077094,2048.0063,2048.0063,156985.27,-0.8008442,-0.79949903,1.0016825,0.99209046,1.0016825,1.2839607,6047.816,20480000.0,3386.3464,13.227916,1.6534895,-324124.8,1507.9824,14.083595,-7.1846014e-05,1.0360636607380451,,,0.8637078,,0.00075270765,0.00075164624,324946.2,24.676208
xl,1780000,,-0.9896765,158181.61,-0.8069472,2048.0063,2048.0063,158195.95,-0.8070204,-0.80565625,1.0016932,0.9920395,1.0016932,1.2857847,6047.8438,20480000.0,3386.3308,13.227855,1.6534818,-324123.34,1507.9755,14.13621,-7.2114424e-05,1.036054682012166,,,0.8265381,,0.00075058796,0.0007495319,331036.44,370.69583
xl,1790000,,-0.9895745,158206.66,-0.80707496,2048.007,2048.007,158220.86,-0.80714744,-0.8057787,1.0016987,0.9920146,1.0016987,1.2866851,6048.364,20480000.0,3386.0398,13.226718,1.6533397,-324095.47,1507.8458,14.108478,-7.197295e-05,1.036045495264198,,,0.8487661,,0.00074848067,0.0007474353,337473.4,372.44604
xl,1800000,,-0.99016625,157675.47,-0.8043652,2048.0063,2048.0063,157689.56,-0.8044371,-0.8030759,1.0016949,0.99203163,1.0016949,1.286071,6047.817,20480000.0,3386.3457,13.227913,1.6534891,-324124.78,1507.982,14.07717,-7.1813236e-05,1.0360358050667033,,,0.84579396,,0.0007463927,0.00074535626,343911.44,23.575989
xl,1810000,,-0.9900713,157743.02,-0.8047098,2048.0059,2048.0059,157756.47,-0.80477846,-0.8034227,1.0016874,0.99206656,1.0016874,1.2848133,6047.8447,20480000.0,3386.3303,13.227853,1.6534816,-324123.28,1507.9752,14.043772,-7.1642855e-05,1.0359968341434085,,,0.8728821,,0.0007443258,0.00074329437,350000.56,24.639503
xl,1820000,,-0.9904004,157413.75,-0.80303013,2048.0059,2048.0059,157427.97,-0.8031026,-0.8017478,1.0016898,0.9920554,1.0016898,1.2852129,6047.8184,20480000.0,3386.345,13.22791,1.6534888,-324124.7,1507.9817,14.019015,-7.1516566e-05,1.0359730683946906,,,0.8459511,,0.0007422699,0.00074124953,356090.88,22.537096
xl,1830000,,-0.99060696,157264.25,-0.80226743,2048.0068,2048.0068,157278.36,-0.8023394,-0.8009872,1.0016881,0.992064,1.0016881,1.2849102,6047.8076,20480000.0,3386.351,13.227934,1.6534917,-324125.28,1507.9845,13.966188,-7.124708e-05,1.0359568097765004,,,0.8545582,,0.0007402374,0.00073922146,362179.03,24.844002
xl,1840000,,-0.99053866,157261.94,-0.80225563,2048.006,2048.006,157275.98,-0.80232733,-0.8009733,1.0016905,0.99205303,1.0016905,1.2853041,6048.14,20480000.0,3386.1648,13.227206,1.6534008,-324107.44,1507.9016,13.941228,-7.111974e-05,1.0359169709658975,,,0.85624576,,0.0007382143,0.00073721,368269.88,23.008665
xl,1850000,,-0.990425,157381.33,-0.8028647,2048.0063,2048.0063,157395.19,-0.80293536,-0.80158025,1.0016905,0.9920522,1.0016905,1.2853318,6047.821,20480000.0,3386.3438,13.227905,1.6534882,-324124.56,1507.9812,13.913933,-7.0980495e-05,1.0359555290803868,,,0.84609056,,0.00073621207,0.00073521485,374358.44,371.15625
xl,1860000,23.109877,-0.9906071,157170.48,-0.80178905,2048.0085,2048.0085,157184.19,-0.801859,-0.80050784,1.0016879,0.9920654,1.0016879,1.28486,6047.812,20480000.0,3386.3489,13.227925,1.6534907,-324125.06,1507.9834,13.876768,-7.0790906e-05,1.0359495783870205,"b""    from __gin__ import dynamic_registration\n    import __main__ as train_script\n    import data.pile.tasks\n    from models.scalable_t5 import network\n    import seqio\n    from t5x import adafactor\n    from t5x import gin_utils\n    from t5x import models\n    from t5x import partitioning\n    from t5x import trainer\n    from t5x import utils\n    \n#### Macros:\n\n    BATCH_SIZE = 2048\n    DROPOUT_RATE = 0.0\n    EVAL_BATCH_SIZE = 256\n    LABEL_SMOOTHING = 0.0\n    LOSS_NORMALIZING_FACTOR = None\n    MIXTURE_OR_TASK_MODULE = None\n    MIXTURE_OR_TASK_NAME = 'pile_r_denoiser'\n    MODEL = @models.EncoderDecoderModel()\n    MODEL_DIR = 'gs://improved-t5/ckpts/v2_xl_mlm/'\n    OPTIMIZER = @adafactor.Adafactor()\n    RANDOM_SEED = None\n    SAVING_PERIOD = 10000\n    SHUFFLE_TRAIN_EXAMPLES = True\n    TASK_FEATURE_LENGTHS = {'inputs': 512, 'targets': 114}\n    TRAIN_STEPS = 2000000\n    USE_CACHED_TASKS = False\n    USE_HARDWARE_RNG = False\n    VOCABULARY = @seqio.SentencePieceVocabulary()\n    Z_LOSS = 0.0001\n    \n#### Parameters for adafactor.Adafactor:\n\n    adafactor.Adafactor.decay_rate = 0.8\n    adafactor.Adafactor.logical_factor_rules = \\\n        @adafactor.standard_logical_factor_rules()\n    adafactor.Adafactor.step_offset = 0\n    \n#### Parameters for utils.CheckpointConfig:\n\n    utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()\n    utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()\n    \n#### Parameters for utils.create_learning_rate_scheduler:\n\n    utils.create_learning_rate_scheduler.base_learning_rate = 1.0\n    utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'\n    utils.create_learning_rate_scheduler.warmup_steps = 10000\n    \n#### Parameters for infer_eval/utils.DatasetConfig:\n\n    infer_eval/utils.DatasetConfig.batch_size = %EVAL_BATCH_SIZE\n    infer_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    infer_eval/utils.DatasetConfig.seed = 0\n    infer_eval/utils.DatasetConfig.shuffle = False\n    infer_eval/utils.DatasetConfig.split = 'validation'\n    infer_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    \n#### Parameters for train/utils.DatasetConfig:\n\n    train/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train/utils.DatasetConfig.pack = True\n    train/utils.DatasetConfig.seed = None\n    train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES\n    train/utils.DatasetConfig.split = 'train'\n    train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for train_eval/utils.DatasetConfig:\n\n    train_eval/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train_eval/utils.DatasetConfig.pack = True\n    train_eval/utils.DatasetConfig.seed = 42\n    train_eval/utils.DatasetConfig.shuffle = False\n    train_eval/utils.DatasetConfig.split = 'validation'\n    train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for models.EncoderDecoderModel:\n\n    models.EncoderDecoderModel.input_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.label_smoothing = %LABEL_SMOOTHING\n    models.EncoderDecoderModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR\n    models.EncoderDecoderModel.module = @network.Transformer()\n    models.EncoderDecoderModel.optimizer_def = %OPTIMIZER\n    models.EncoderDecoderModel.output_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.z_loss = %Z_LOSS\n    \n#### Parameters for seqio.Evaluator:\n\n    seqio.Evaluator.logger_cls = \\\n        [@seqio.PyLoggingLogger, @seqio.TensorBoardLogger, @seqio.JSONLogger]\n    seqio.Evaluator.num_examples = 2048\n    \n#### Parameters for partitioning.PjitPartitioner:\n\n    partitioning.PjitPartitioner.logical_axis_rules = \\\n        @partitioning.standard_logical_axis_rules()\n    partitioning.PjitPartitioner.model_parallel_submesh = None\n    partitioning.PjitPartitioner.num_partitions = 2\n    \n#### Parameters for utils.RestoreCheckpointConfig:\n\n    utils.RestoreCheckpointConfig.path = []\n    \n#### Parameters for utils.SaveCheckpointConfig:\n\n    utils.SaveCheckpointConfig.dtype = 'float32'\n    utils.SaveCheckpointConfig.keep = None\n    utils.SaveCheckpointConfig.period = %SAVING_PERIOD\n    utils.SaveCheckpointConfig.save_dataset = False\n    \n#### Parameters for seqio.SentencePieceVocabulary:\n\n    seqio.SentencePieceVocabulary.extra_ids = 100\n    seqio.SentencePieceVocabulary.sentencepiece_model_file = \\\n        'gs://improved-t5/vocabs/tokenizer.model'\n    \n#### Parameters for network.T5Config:\n\n    network.T5Config.dropout_rate = %DROPOUT_RATE\n    network.T5Config.dtype = 'bfloat16'\n    network.T5Config.emb_dim = 2048\n    network.T5Config.head_dim = 64\n    network.T5Config.logits_via_embedding = False\n    network.T5Config.mlp_activations = ('gelu', 'linear')\n    network.T5Config.mlp_dim = 5120\n    network.T5Config.num_decoder_layers = 24\n    network.T5Config.num_encoder_layers = 24\n    network.T5Config.num_heads = 32\n    network.T5Config.remat_policy = 'minimal'\n    network.T5Config.scan_layers = True\n    network.T5Config.vocab_size = 32128\n    \n#### Parameters for train_script.train:\n\n    train_script.train.checkpoint_cfg = @utils.CheckpointConfig()\n    train_script.train.eval_period = %SAVING_PERIOD\n    train_script.train.eval_steps = 100\n    train_script.train.infer_eval_dataset_cfg = @infer_eval/utils.DatasetConfig()\n    train_script.train.inference_evaluator_cls = @seqio.Evaluator\n    train_script.train.model = %MODEL\n    train_script.train.model_dir = %MODEL_DIR\n    train_script.train.partitioner = @partitioning.PjitPartitioner()\n    train_script.train.random_seed = %RANDOM_SEED\n    train_script.train.run_eval_before_training = True\n    train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config\n    train_script.train.total_steps = %TRAIN_STEPS\n    train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()\n    train_script.train.train_eval_dataset_cfg = @train_eval/utils.DatasetConfig()\n    train_script.train.trainer_cls = @trainer.Trainer\n    train_script.train.use_hardware_rng = %USE_HARDWARE_RNG\n    \n#### Parameters for trainer.Trainer:\n\n    trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()\n    trainer.Trainer.num_microbatches = None\n    \n#### Parameters for network.Transformer:\n\n    network.Transformer.config = @network.T5Config()""",32.119843,6.244545,3.194809e-05,0.00073421677,0.00073323573,380795.16,23.218313
xl,750000,20.214558,-0.9830668,165024.14,-0.8418537,2048.007,2048.007,165044.9,-0.84195966,-0.84055036,1.0016767,0.99211735,1.0016767,1.28299,10142.194,20480000.0,2019.2869,15.775679,0.9859799,-193276.45,1798.4274,20.697758,-0.00010558748,1.0379300779906087,"b""    from __gin__ import dynamic_registration\n    import __main__ as train_script\n    import data.pile.tasks\n    from models.scalable_t5 import network\n    import seqio\n    from t5x import adafactor\n    from t5x import gin_utils\n    from t5x import models\n    from t5x import partitioning\n    from t5x import trainer\n    from t5x import utils\n    \n#### Macros:\n\n    BATCH_SIZE = 2048\n    DROPOUT_RATE = 0.0\n    EVAL_BATCH_SIZE = 256\n    LABEL_SMOOTHING = 0.0\n    LOSS_NORMALIZING_FACTOR = None\n    MIXTURE_OR_TASK_MODULE = None\n    MIXTURE_OR_TASK_NAME = 'pile_r_denoiser'\n    MODEL = @models.EncoderDecoderModel()\n    MODEL_DIR = 'gs://improved-t5/ckpts/v2_xl_mlm/'\n    OPTIMIZER = @adafactor.Adafactor()\n    RANDOM_SEED = None\n    SAVING_PERIOD = 10000\n    SHUFFLE_TRAIN_EXAMPLES = True\n    TASK_FEATURE_LENGTHS = {'inputs': 512, 'targets': 114}\n    TRAIN_STEPS = 1000000\n    USE_CACHED_TASKS = False\n    USE_HARDWARE_RNG = False\n    VOCABULARY = @seqio.SentencePieceVocabulary()\n    Z_LOSS = 0.0001\n    \n#### Parameters for adafactor.Adafactor:\n\n    adafactor.Adafactor.decay_rate = 0.8\n    adafactor.Adafactor.logical_factor_rules = \\\n        @adafactor.standard_logical_factor_rules()\n    adafactor.Adafactor.step_offset = 0\n    \n#### Parameters for utils.CheckpointConfig:\n\n    utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()\n    utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()\n    \n#### Parameters for utils.create_learning_rate_scheduler:\n\n    utils.create_learning_rate_scheduler.base_learning_rate = 1.0\n    utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'\n    utils.create_learning_rate_scheduler.warmup_steps = 10000\n    \n#### Parameters for infer_eval/utils.DatasetConfig:\n\n    infer_eval/utils.DatasetConfig.batch_size = %EVAL_BATCH_SIZE\n    infer_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    infer_eval/utils.DatasetConfig.seed = 0\n    infer_eval/utils.DatasetConfig.shuffle = False\n    infer_eval/utils.DatasetConfig.split = 'validation'\n    infer_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    \n#### Parameters for train/utils.DatasetConfig:\n\n    train/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train/utils.DatasetConfig.pack = True\n    train/utils.DatasetConfig.seed = None\n    train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES\n    train/utils.DatasetConfig.split = 'train'\n    train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for train_eval/utils.DatasetConfig:\n\n    train_eval/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train_eval/utils.DatasetConfig.pack = True\n    train_eval/utils.DatasetConfig.seed = 42\n    train_eval/utils.DatasetConfig.shuffle = False\n    train_eval/utils.DatasetConfig.split = 'validation'\n    train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for models.EncoderDecoderModel:\n\n    models.EncoderDecoderModel.input_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.label_smoothing = %LABEL_SMOOTHING\n    models.EncoderDecoderModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR\n    models.EncoderDecoderModel.module = @network.Transformer()\n    models.EncoderDecoderModel.optimizer_def = %OPTIMIZER\n    models.EncoderDecoderModel.output_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.z_loss = %Z_LOSS\n    \n#### Parameters for seqio.Evaluator:\n\n    seqio.Evaluator.logger_cls = \\\n        [@seqio.PyLoggingLogger, @seqio.TensorBoardLogger, @seqio.JSONLogger]\n    seqio.Evaluator.num_examples = 2048\n    \n#### Parameters for partitioning.PjitPartitioner:\n\n    partitioning.PjitPartitioner.logical_axis_rules = \\\n        @partitioning.standard_logical_axis_rules()\n    partitioning.PjitPartitioner.model_parallel_submesh = (1, 1, 2, 1)\n    partitioning.PjitPartitioner.num_partitions = 1\n    \n#### Parameters for utils.RestoreCheckpointConfig:\n\n    utils.RestoreCheckpointConfig.path = []\n    \n#### Parameters for utils.SaveCheckpointConfig:\n\n    utils.SaveCheckpointConfig.dtype = 'float32'\n    utils.SaveCheckpointConfig.keep = None\n    utils.SaveCheckpointConfig.period = %SAVING_PERIOD\n    utils.SaveCheckpointConfig.save_dataset = False\n    \n#### Parameters for seqio.SentencePieceVocabulary:\n\n    seqio.SentencePieceVocabulary.extra_ids = 100\n    seqio.SentencePieceVocabulary.sentencepiece_model_file = \\\n        'gs://improved-t5/vocabs/tokenizer.model'\n    \n#### Parameters for network.T5Config:\n\n    network.T5Config.dropout_rate = %DROPOUT_RATE\n    network.T5Config.dtype = 'bfloat16'\n    network.T5Config.emb_dim = 2048\n    network.T5Config.head_dim = 64\n    network.T5Config.logits_via_embedding = False\n    network.T5Config.mlp_activations = ('gelu', 'linear')\n    network.T5Config.mlp_dim = 5120\n    network.T5Config.num_decoder_layers = 24\n    network.T5Config.num_encoder_layers = 24\n    network.T5Config.num_heads = 32\n    network.T5Config.remat_policy = 'minimal'\n    network.T5Config.scan_layers = True\n    network.T5Config.vocab_size = 32128\n    \n#### Parameters for train_script.train:\n\n    train_script.train.checkpoint_cfg = @utils.CheckpointConfig()\n    train_script.train.eval_period = %SAVING_PERIOD\n    train_script.train.eval_steps = 100\n    train_script.train.infer_eval_dataset_cfg = @infer_eval/utils.DatasetConfig()\n    train_script.train.inference_evaluator_cls = @seqio.Evaluator\n    train_script.train.model = %MODEL\n    train_script.train.model_dir = %MODEL_DIR\n    train_script.train.partitioner = @partitioning.PjitPartitioner()\n    train_script.train.random_seed = %RANDOM_SEED\n    train_script.train.run_eval_before_training = True\n    train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config\n    train_script.train.total_steps = %TRAIN_STEPS\n    train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()\n    train_script.train.train_eval_dataset_cfg = @train_eval/utils.DatasetConfig()\n    train_script.train.trainer_cls = @trainer.Trainer\n    train_script.train.use_hardware_rng = %USE_HARDWARE_RNG\n    \n#### Parameters for trainer.Trainer:\n\n    trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()\n    trainer.Trainer.num_microbatches = None\n    \n#### Parameters for network.Transformer:\n\n    network.Transformer.config = @network.T5Config()""",11.149113,0.8651593,1.1920929e-05,0.0011585751,0.0011547012,285462.0,16.077015
xl,760000,,-0.98277074,165414.44,-0.8438448,2048.0056,2048.0056,165435.55,-0.8439525,-0.8425294,1.0016891,0.99205965,1.0016891,1.2850673,10146.096,20480000.0,2018.5104,15.769612,0.98560077,-193202.12,1797.7358,20.645767,-0.00010532226,1.0378763645581301,,,0.8471608,,0.0011508791,0.0011470794,10276.589,19.848938
xl,770000,,-0.9825706,165523.33,-0.8444002,2048.0068,2048.0068,165543.62,-0.8445038,-0.8430789,1.00169,0.9920541,1.00169,1.2852604,10142.438,20480000.0,2019.2383,15.775299,0.9859562,-193271.8,1798.3842,20.527367,-0.00010471825,1.0379477652816038,,,0.8737128,,0.0011433268,0.0011396065,20471.46,14.757067
xl,780000,,-0.9827456,165369.28,-0.8436144,2048.007,2048.007,165389.61,-0.84371805,-0.84229445,1.0016901,0.99205405,1.0016901,1.2852646,10142.49,20480000.0,2019.2279,15.775218,0.9859511,-193270.8,1798.3749,20.396555,-0.00010405092,1.0378930648597453,,,0.8432193,,0.0011359173,0.0011322778,30661.248,19.22353
xl,790000,,-0.9831577,164982.95,-0.8416436,2048.008,2048.008,165002.97,-0.8417457,-0.8403355,1.0016781,0.9921107,1.0016781,1.2832295,10142.471,20480000.0,2019.2319,15.7752495,0.9859531,-193271.19,1798.3784,20.196795,-0.000103031874,1.0378551785561685,,,0.8529196,,0.0011286756,0.0011250885,40855.527,16.298912
xl,800000,,-0.98329717,164802.34,-0.8407222,2048.0076,2048.0076,164822.89,-0.84082705,-0.8394088,1.0016897,0.9920565,1.0016897,1.2851768,10142.439,20480000.0,2019.238,15.775297,0.9859561,-193271.77,1798.3839,20.062649,-0.000102347534,1.0378332748250247,,,0.86340475,,0.001121544,0.0011180347,51046.832,15.679903
xl,810000,,-0.9836279,164500.55,-0.8391827,2048.005,2048.005,164520.23,-0.8392831,-0.83786976,1.0016868,0.99207,1.0016868,1.2846935,10142.413,20480000.0,2019.2434,15.775339,0.9859587,-193272.28,1798.3887,19.928995,-0.00010166572,1.0377597915627566,,,0.8534739,,0.0011145591,0.0011111118,61237.45,21.522898
xl,820000,,-0.98377126,164275.2,-0.838033,2048.0068,2048.0068,164295.7,-0.8381376,-0.8367226,1.0016912,0.99204975,1.0016912,1.2854213,10142.376,20480000.0,2019.2509,15.775397,0.98596233,-193273.0,1798.3953,19.802572,-0.000101020785,1.0377607953069288,,,0.82925725,,0.001107703,0.001104316,71433.86,15.194734
xl,830000,,-0.9838035,164261.42,-0.8379628,2048.0063,2048.0063,164281.62,-0.8380658,-0.8366544,1.0016869,0.99206936,1.0016869,1.2847159,10142.769,20480000.0,2019.1726,15.774786,0.9859241,-193265.52,1798.3256,19.69554,-0.00010047477,1.0376807670607842,,,0.8364935,,0.0011009688,0.0010976433,81624.31,15.0726185
xl,840000,,-0.9842763,163771.98,-0.83546597,2048.0063,2048.0063,163792.53,-0.8355708,-0.83416396,1.0016865,0.9920714,1.0016865,1.2846427,10142.38,20480000.0,2019.25,15.775391,0.9859619,-193272.9,1798.3945,19.58289,-9.9900106e-05,1.037575109997224,,,0.8348336,,0.0010943569,0.0010910901,91814.26,19.094538
xl,850000,,-0.98485136,163185.22,-0.8324727,2048.0068,2048.0068,163204.83,-0.8325727,-0.83116966,1.001688,0.99206436,1.001688,1.2848961,10142.402,20480000.0,2019.2455,15.775355,0.9859597,-193272.48,1798.3905,19.453453,-9.923979e-05,1.0374665284035136,,,0.8573127,,0.0010878694,0.001084653,102008.26,13.873287
xl,860000,,-0.9850177,163019.98,-0.8316297,2048.0078,2048.0078,163039.52,-0.8317293,-0.8303374,1.0016763,0.992119,1.0016763,1.2829331,10142.431,20480000.0,2019.2399,15.775311,0.98595697,-193271.94,1798.3855,19.302761,-9.847105e-05,1.0374394129939306,,,0.8498163,,0.0010814735,0.0010783284,112197.08,15.604634
xl,870000,,-0.9852817,162759.22,-0.83029944,2048.0063,2048.0063,162778.1,-0.83039576,-0.82900155,1.0016818,0.99209386,1.0016818,1.2838367,10142.369,20480000.0,2019.2521,15.775407,0.9859629,-193273.11,1798.3964,19.182705,-9.78586e-05,1.0374362883330355,,,0.8686757,,0.0010752105,0.0010721132,122387.58,15.214074
xl,880000,,-0.98532283,162709.14,-0.830044,2048.0076,2048.0076,162728.67,-0.83014363,-0.82875186,1.0016793,0.9921052,1.0016793,1.2834301,10142.398,20480000.0,2019.2462,15.775361,0.98596007,-193272.56,1798.3911,19.083054,-9.735024e-05,1.0374034607226812,,,0.82182145,,0.0010690391,0.0010660042,132577.73,16.190725
xl,890000,,-0.98537385,162641.95,-0.82970124,2048.0073,2048.0073,162660.28,-0.8297947,-0.8284016,1.0016817,0.9920942,1.0016817,1.2838246,10142.458,20480000.0,2019.2343,15.775268,0.9859542,-193271.42,1798.3805,18.984837,-9.6849195e-05,1.037447582911137,,,0.8328073,,0.0010630098,0.0010599985,142768.84,22.054445
xl,900000,,-0.98546696,162597.5,-0.82947445,2048.0059,2048.0059,162616.47,-0.82957125,-0.82818156,1.001678,0.9921118,1.001678,1.2831936,10142.366,20480000.0,2019.2527,15.775412,0.9859632,-193273.17,1798.3969,18.87907,-9.630964e-05,1.0373072592983268,,,0.85121393,,0.0010570436,0.0010540931,152965.73,16.647179
xl,910000,,-0.9852125,162823.16,-0.83062565,2048.0068,2048.0068,162841.84,-0.8307209,-0.8293197,1.0016896,0.9920572,1.0016896,1.2851524,10142.428,20480000.0,2019.2404,15.775315,0.9859572,-193272.0,1798.386,18.82304,-9.60238e-05,1.0372997272065112,,,0.84122777,,0.0010511758,0.0010482854,163157.36,15.974821
xl,920000,,-0.98522174,162752.69,-0.8302661,2048.0066,2048.0066,162771.34,-0.83036125,-0.8289631,1.0016866,0.99207133,1.0016866,1.2846487,10142.444,20480000.0,2019.2372,15.7752905,0.98595566,-193271.69,1798.383,18.722874,-9.551281e-05,1.0373250683458646,,,0.8215797,,0.0010454139,0.0010425727,173348.27,17.025448
xl,930000,,-0.98538846,162613.12,-0.8295542,2048.008,2048.008,162631.73,-0.8296491,-0.8282534,1.0016851,0.9920774,1.0016851,1.2844236,10142.409,20480000.0,2019.2441,15.775345,0.98595905,-193272.36,1798.3893,18.636364,-9.50715e-05,1.0373187985287748,,,0.82228756,,0.0010397567,0.0010369522,183540.17,20.449297
xl,940000,,-0.985531,162512.08,-0.82903874,2048.0068,2048.0068,162530.78,-0.82913405,-0.8277411,1.0016829,0.9920892,1.0016829,1.2840064,10142.389,20480000.0,2019.2482,15.775376,0.985961,-193272.73,1798.393,18.527761,-9.4517476e-05,1.037271343747828,,,0.830168,,0.001034198,0.0010314218,193735.48,14.509451
xl,950000,,-0.98568314,162333.66,-0.82812846,2048.0068,2048.0068,162352.36,-0.8282239,-0.826831,1.0016847,0.9920801,1.0016847,1.2843298,10142.421,20480000.0,2019.2417,15.775326,0.98595786,-193272.12,1798.3872,18.432594,-9.403199e-05,1.037265266006078,,,0.84931993,,0.0010286918,0.0010259789,203924.92,15.0931425
xl,960000,,-0.98593986,162069.0,-0.82677835,2048.0054,2048.0054,162087.17,-0.8268711,-0.8254845,1.0016797,0.99210316,1.0016797,1.2835014,10142.379,20480000.0,2019.2502,15.775393,0.98596203,-193272.94,1798.3948,18.342068,-9.3570176e-05,1.037204526334804,,,0.8502848,,0.0010232817,0.0010206213,214114.9,18.880733
xl,970000,,-0.9860018,161957.8,-0.82621104,2048.007,2048.007,161975.81,-0.826303,-0.8249112,1.0016872,0.9920678,1.0016872,1.2847698,10142.372,20480000.0,2019.2516,15.775403,0.9859627,-193273.06,1798.396,18.269402,-9.319947e-05,1.0372980258954783,,,0.8365121,,0.0010179855,0.0010153466,224308.67,14.60672
xl,980000,,-0.98586833,162118.98,-0.82703334,2048.008,2048.008,162137.61,-0.82712835,-0.82574326,1.0016774,0.99211377,1.0016774,1.2831188,10142.436,20480000.0,2019.2389,15.775304,0.9859565,-193271.86,1798.3846,18.19876,-9.28391e-05,1.0372156723763468,,,0.84567976,,0.0010127369,0.001010153,234498.22,16.270006
xl,990000,,-0.9859957,161968.84,-0.8262674,2048.0063,2048.0063,161987.38,-0.82636195,-0.82497233,1.0016844,0.9920812,1.0016844,1.2842894,10142.359,20480000.0,2019.254,15.775422,0.9859639,-193273.3,1798.3981,18.111769,-9.239533e-05,1.0372102417470408,,,0.8360567,,0.0010075895,0.0010050383,244689.36,15.376198
xl,1000000,25.89449,-0.9862328,161688.78,-0.8248387,2048.0059,2048.0059,161706.53,-0.82492924,-0.823541,1.0016857,0.9920754,1.0016857,1.2845019,10142.44,20480000.0,2019.2379,15.775296,0.985956,-193271.75,1798.3838,18.02051,-9.192978e-05,1.0371612624330924,"b""    from __gin__ import dynamic_registration\n    import __main__ as train_script\n    import data.pile.tasks\n    from models.scalable_t5 import network\n    import seqio\n    from t5x import adafactor\n    from t5x import gin_utils\n    from t5x import models\n    from t5x import partitioning\n    from t5x import trainer\n    from t5x import utils\n    \n#### Macros:\n\n    BATCH_SIZE = 2048\n    DROPOUT_RATE = 0.0\n    EVAL_BATCH_SIZE = 256\n    LABEL_SMOOTHING = 0.0\n    LOSS_NORMALIZING_FACTOR = None\n    MIXTURE_OR_TASK_MODULE = None\n    MIXTURE_OR_TASK_NAME = 'pile_r_denoiser'\n    MODEL = @models.EncoderDecoderModel()\n    MODEL_DIR = 'gs://improved-t5/ckpts/v2_xl_mlm/'\n    OPTIMIZER = @adafactor.Adafactor()\n    RANDOM_SEED = None\n    SAVING_PERIOD = 10000\n    SHUFFLE_TRAIN_EXAMPLES = True\n    TASK_FEATURE_LENGTHS = {'inputs': 512, 'targets': 114}\n    TRAIN_STEPS = 2000000\n    USE_CACHED_TASKS = False\n    USE_HARDWARE_RNG = False\n    VOCABULARY = @seqio.SentencePieceVocabulary()\n    Z_LOSS = 0.0001\n    \n#### Parameters for adafactor.Adafactor:\n\n    adafactor.Adafactor.decay_rate = 0.8\n    adafactor.Adafactor.logical_factor_rules = \\\n        @adafactor.standard_logical_factor_rules()\n    adafactor.Adafactor.step_offset = 0\n    \n#### Parameters for utils.CheckpointConfig:\n\n    utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()\n    utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()\n    \n#### Parameters for utils.create_learning_rate_scheduler:\n\n    utils.create_learning_rate_scheduler.base_learning_rate = 1.0\n    utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'\n    utils.create_learning_rate_scheduler.warmup_steps = 10000\n    \n#### Parameters for infer_eval/utils.DatasetConfig:\n\n    infer_eval/utils.DatasetConfig.batch_size = %EVAL_BATCH_SIZE\n    infer_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    infer_eval/utils.DatasetConfig.seed = 0\n    infer_eval/utils.DatasetConfig.shuffle = False\n    infer_eval/utils.DatasetConfig.split = 'validation'\n    infer_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    \n#### Parameters for train/utils.DatasetConfig:\n\n    train/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train/utils.DatasetConfig.pack = True\n    train/utils.DatasetConfig.seed = None\n    train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES\n    train/utils.DatasetConfig.split = 'train'\n    train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for train_eval/utils.DatasetConfig:\n\n    train_eval/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train_eval/utils.DatasetConfig.pack = True\n    train_eval/utils.DatasetConfig.seed = 42\n    train_eval/utils.DatasetConfig.shuffle = False\n    train_eval/utils.DatasetConfig.split = 'validation'\n    train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for models.EncoderDecoderModel:\n\n    models.EncoderDecoderModel.input_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.label_smoothing = %LABEL_SMOOTHING\n    models.EncoderDecoderModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR\n    models.EncoderDecoderModel.module = @network.Transformer()\n    models.EncoderDecoderModel.optimizer_def = %OPTIMIZER\n    models.EncoderDecoderModel.output_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.z_loss = %Z_LOSS\n    \n#### Parameters for seqio.Evaluator:\n\n    seqio.Evaluator.logger_cls = \\\n        [@seqio.PyLoggingLogger, @seqio.TensorBoardLogger, @seqio.JSONLogger]\n    seqio.Evaluator.num_examples = 2048\n    \n#### Parameters for partitioning.PjitPartitioner:\n\n    partitioning.PjitPartitioner.logical_axis_rules = \\\n        @partitioning.standard_logical_axis_rules()\n    partitioning.PjitPartitioner.model_parallel_submesh = (1, 1, 2, 1)\n    partitioning.PjitPartitioner.num_partitions = 1\n    \n#### Parameters for utils.RestoreCheckpointConfig:\n\n    utils.RestoreCheckpointConfig.path = []\n    \n#### Parameters for utils.SaveCheckpointConfig:\n\n    utils.SaveCheckpointConfig.dtype = 'float32'\n    utils.SaveCheckpointConfig.keep = None\n    utils.SaveCheckpointConfig.period = %SAVING_PERIOD\n    utils.SaveCheckpointConfig.save_dataset = False\n    \n#### Parameters for seqio.SentencePieceVocabulary:\n\n    seqio.SentencePieceVocabulary.extra_ids = 100\n    seqio.SentencePieceVocabulary.sentencepiece_model_file = \\\n        'gs://improved-t5/vocabs/tokenizer.model'\n    \n#### Parameters for network.T5Config:\n\n    network.T5Config.dropout_rate = %DROPOUT_RATE\n    network.T5Config.dtype = 'bfloat16'\n    network.T5Config.emb_dim = 2048\n    network.T5Config.head_dim = 64\n    network.T5Config.logits_via_embedding = False\n    network.T5Config.mlp_activations = ('gelu', 'linear')\n    network.T5Config.mlp_dim = 5120\n    network.T5Config.num_decoder_layers = 24\n    network.T5Config.num_encoder_layers = 24\n    network.T5Config.num_heads = 32\n    network.T5Config.remat_policy = 'minimal'\n    network.T5Config.scan_layers = True\n    network.T5Config.vocab_size = 32128\n    \n#### Parameters for train_script.train:\n\n    train_script.train.checkpoint_cfg = @utils.CheckpointConfig()\n    train_script.train.eval_period = %SAVING_PERIOD\n    train_script.train.eval_steps = 100\n    train_script.train.infer_eval_dataset_cfg = @infer_eval/utils.DatasetConfig()\n    train_script.train.inference_evaluator_cls = @seqio.Evaluator\n    train_script.train.model = %MODEL\n    train_script.train.model_dir = %MODEL_DIR\n    train_script.train.partitioner = @partitioning.PjitPartitioner()\n    train_script.train.random_seed = %RANDOM_SEED\n    train_script.train.run_eval_before_training = True\n    train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config\n    train_script.train.total_steps = %TRAIN_STEPS\n    train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()\n    train_script.train.train_eval_dataset_cfg = @train_eval/utils.DatasetConfig()\n    train_script.train.trainer_cls = @trainer.Trainer\n    train_script.train.use_hardware_rng = %USE_HARDWARE_RNG\n    \n#### Parameters for trainer.Trainer:\n\n    trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()\n    trainer.Trainer.num_microbatches = None\n    \n#### Parameters for network.Transformer:\n\n    network.Transformer.config = @network.T5Config()""",15.026348,8.568999,1.5735626e-05,0.0010025135,0.0010000005,254879.69,14.732311
xl,470000,20.525229,-0.9771977,171251.8,-0.87362343,2048.005,2048.005,171278.14,-0.87375784,-0.87229294,1.0016794,0.9921046,1.0016794,1.28345,10142.211,20480000.0,2019.2836,15.775653,0.9859783,-193276.14,1798.4244,26.85219,-0.00013698368,1.0393732283094823,"b""    from __gin__ import dynamic_registration\n    import __main__ as train_script\n    import data.pile.tasks\n    from models.scalable_t5 import network\n    import seqio\n    from t5x import adafactor\n    from t5x import gin_utils\n    from t5x import models\n    from t5x import partitioning\n    from t5x import trainer\n    from t5x import utils\n    \n#### Macros:\n\n    BATCH_SIZE = 2048\n    DROPOUT_RATE = 0.0\n    EVAL_BATCH_SIZE = 256\n    LABEL_SMOOTHING = 0.0\n    LOSS_NORMALIZING_FACTOR = None\n    MIXTURE_OR_TASK_MODULE = None\n    MIXTURE_OR_TASK_NAME = 'pile_r_denoiser'\n    MODEL = @models.EncoderDecoderModel()\n    MODEL_DIR = 'gs://improved-t5/ckpts/v2_xl_mlm/'\n    OPTIMIZER = @adafactor.Adafactor()\n    RANDOM_SEED = None\n    SAVING_PERIOD = 10000\n    SHUFFLE_TRAIN_EXAMPLES = True\n    TASK_FEATURE_LENGTHS = {'inputs': 512, 'targets': 114}\n    TRAIN_STEPS = 1000000\n    USE_CACHED_TASKS = False\n    USE_HARDWARE_RNG = False\n    VOCABULARY = @seqio.SentencePieceVocabulary()\n    Z_LOSS = 0.0001\n    \n#### Parameters for adafactor.Adafactor:\n\n    adafactor.Adafactor.decay_rate = 0.8\n    adafactor.Adafactor.logical_factor_rules = \\\n        @adafactor.standard_logical_factor_rules()\n    adafactor.Adafactor.step_offset = 0\n    \n#### Parameters for utils.CheckpointConfig:\n\n    utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()\n    utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()\n    \n#### Parameters for utils.create_learning_rate_scheduler:\n\n    utils.create_learning_rate_scheduler.base_learning_rate = 1.0\n    utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'\n    utils.create_learning_rate_scheduler.warmup_steps = 10000\n    \n#### Parameters for infer_eval/utils.DatasetConfig:\n\n    infer_eval/utils.DatasetConfig.batch_size = %EVAL_BATCH_SIZE\n    infer_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    infer_eval/utils.DatasetConfig.seed = 0\n    infer_eval/utils.DatasetConfig.shuffle = False\n    infer_eval/utils.DatasetConfig.split = 'validation'\n    infer_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    \n#### Parameters for train/utils.DatasetConfig:\n\n    train/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train/utils.DatasetConfig.pack = True\n    train/utils.DatasetConfig.seed = None\n    train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES\n    train/utils.DatasetConfig.split = 'train'\n    train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for train_eval/utils.DatasetConfig:\n\n    train_eval/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train_eval/utils.DatasetConfig.pack = True\n    train_eval/utils.DatasetConfig.seed = 42\n    train_eval/utils.DatasetConfig.shuffle = False\n    train_eval/utils.DatasetConfig.split = 'validation'\n    train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for models.EncoderDecoderModel:\n\n    models.EncoderDecoderModel.input_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.label_smoothing = %LABEL_SMOOTHING\n    models.EncoderDecoderModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR\n    models.EncoderDecoderModel.module = @network.Transformer()\n    models.EncoderDecoderModel.optimizer_def = %OPTIMIZER\n    models.EncoderDecoderModel.output_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.z_loss = %Z_LOSS\n    \n#### Parameters for seqio.Evaluator:\n\n    seqio.Evaluator.logger_cls = \\\n        [@seqio.PyLoggingLogger, @seqio.TensorBoardLogger, @seqio.JSONLogger]\n    seqio.Evaluator.num_examples = 2048\n    \n#### Parameters for partitioning.PjitPartitioner:\n\n    partitioning.PjitPartitioner.logical_axis_rules = \\\n        @partitioning.standard_logical_axis_rules()\n    partitioning.PjitPartitioner.model_parallel_submesh = (1, 1, 2, 1)\n    partitioning.PjitPartitioner.num_partitions = 1\n    \n#### Parameters for utils.RestoreCheckpointConfig:\n\n    utils.RestoreCheckpointConfig.path = []\n    \n#### Parameters for utils.SaveCheckpointConfig:\n\n    utils.SaveCheckpointConfig.dtype = 'float32'\n    utils.SaveCheckpointConfig.keep = None\n    utils.SaveCheckpointConfig.period = %SAVING_PERIOD\n    utils.SaveCheckpointConfig.save_dataset = False\n    \n#### Parameters for seqio.SentencePieceVocabulary:\n\n    seqio.SentencePieceVocabulary.extra_ids = 100\n    seqio.SentencePieceVocabulary.sentencepiece_model_file = \\\n        'gs://improved-t5/vocabs/tokenizer.model'\n    \n#### Parameters for network.T5Config:\n\n    network.T5Config.dropout_rate = %DROPOUT_RATE\n    network.T5Config.dtype = 'bfloat16'\n    network.T5Config.emb_dim = 2048\n    network.T5Config.head_dim = 64\n    network.T5Config.logits_via_embedding = False\n    network.T5Config.mlp_activations = ('gelu', 'linear')\n    network.T5Config.mlp_dim = 5120\n    network.T5Config.num_decoder_layers = 24\n    network.T5Config.num_encoder_layers = 24\n    network.T5Config.num_heads = 32\n    network.T5Config.remat_policy = 'minimal'\n    network.T5Config.scan_layers = True\n    network.T5Config.vocab_size = 32128\n    \n#### Parameters for train_script.train:\n\n    train_script.train.checkpoint_cfg = @utils.CheckpointConfig()\n    train_script.train.eval_period = %SAVING_PERIOD\n    train_script.train.eval_steps = 100\n    train_script.train.infer_eval_dataset_cfg = @infer_eval/utils.DatasetConfig()\n    train_script.train.inference_evaluator_cls = @seqio.Evaluator\n    train_script.train.model = %MODEL\n    train_script.train.model_dir = %MODEL_DIR\n    train_script.train.partitioner = @partitioning.PjitPartitioner()\n    train_script.train.random_seed = %RANDOM_SEED\n    train_script.train.run_eval_before_training = True\n    train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config\n    train_script.train.total_steps = %TRAIN_STEPS\n    train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()\n    train_script.train.train_eval_dataset_cfg = @train_eval/utils.DatasetConfig()\n    train_script.train.trainer_cls = @trainer.Trainer\n    train_script.train.use_hardware_rng = %USE_HARDWARE_RNG\n    \n#### Parameters for trainer.Trainer:\n\n    trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()\n    trainer.Trainer.num_microbatches = None\n    \n#### Parameters for network.Transformer:\n\n    network.Transformer.config = @network.T5Config()""",10.075423,9.833323,1.4543533e-05,0.0014665002,0.0014586514,479051.3,
xl,480000,,-0.9776167,170854.69,-0.87159765,2048.0078,2048.0078,170881.08,-0.8717323,-0.87026364,1.0016876,0.9920661,1.0016876,1.2848331,10145.466,20480000.0,2018.6357,15.770592,0.985662,-193214.12,1797.8474,26.595503,-0.00013567423,1.0392934218853827,,,0.8596349,,0.0014509773,0.0014433772,10280.778,15.22047
xl,490000,,-0.9775381,170829.95,-0.8714715,2048.0083,2048.0083,170855.81,-0.8716034,-0.8701311,1.001692,0.99204487,1.001692,1.2855933,10142.18,20480000.0,2019.2897,15.775701,0.9859813,-193276.72,1798.4299,26.297386,-0.00013415342,1.039174523946391,,,0.85787916,,0.0014359363,0.0014285729,20470.777,26.702728
xl,500000,,-0.97785574,170541.06,-0.86999774,2048.0056,2048.0056,170566.7,-0.8701285,-0.86865795,1.0016929,0.99204135,1.0016929,1.2857223,10142.156,20480000.0,2019.2946,15.775739,0.98598367,-193277.17,1798.4342,25.936985,-0.00013231486,1.039136776008758,,,0.85476375,,0.0014213494,0.001414215,30672.475,18.037113
xl,510000,,-0.97824365,170229.67,-0.8684092,2048.0059,2048.0059,170254.83,-0.86853755,-0.8670844,1.0016758,0.9921211,1.0016758,1.2828572,10142.211,20480000.0,2019.2836,15.775653,0.9859783,-193276.14,1798.4244,25.610664,-0.00013065017,1.0390263548502139,,,0.85914516,,0.0014072084,0.0014002814,40865.285,14.9929495
xl,520000,,-0.9784049,169972.25,-0.86709595,2048.0063,2048.0063,169997.75,-0.86722606,-0.8657603,1.001693,0.9920408,1.001693,1.2857416,10142.209,20480000.0,2019.2839,15.775656,0.9859785,-193276.17,1798.4247,25.346888,-0.00012930454,1.0389789765581634,,,0.863482,,0.0013934822,0.0013867519,51055.043,16.141514
xl,530000,,-0.9787847,169623.22,-0.86531544,2048.0063,2048.0063,169648.3,-0.86544335,-0.8639874,1.0016851,0.99207795,1.0016851,1.2844089,10142.158,20480000.0,2019.2941,15.775735,0.98598343,-193277.14,1798.4337,25.038431,-0.00012773098,1.0389005765781385,,,0.8726833,,0.0013801508,0.001373607,61245.918,17.30845
xl,540000,,-0.9790752,169259.34,-0.8634591,2048.0068,2048.0068,169284.33,-0.86358666,-0.86213094,1.0016885,0.9920623,1.0016885,1.2849718,10142.203,20480000.0,2019.2852,15.775665,0.9859791,-193276.28,1798.4258,24.764107,-0.00012633155,1.0388428684141908,,,0.8454542,,0.0013671798,0.001360829,71438.016,16.197357
xl,550000,,-0.97912014,169201.31,-0.8631631,2048.0083,2048.0083,169226.34,-0.8632908,-0.8618377,1.0016861,0.9920732,1.0016861,1.2845771,10142.493,20480000.0,2019.2273,15.775213,0.9859508,-193270.75,1798.3743,24.510761,-0.00012503914,1.0387638851712304,,,0.86102366,,0.001354586,0.001348401,81629.25,17.219374
xl,560000,,-0.9792654,169036.69,-0.8623233,2048.0056,2048.0056,169061.61,-0.8624505,-0.860996,1.0016893,0.99205834,1.0016893,1.285111,10142.163,20480000.0,2019.2932,15.775728,0.985983,-193277.05,1798.433,24.318144,-0.00012405652,1.0387896512792472,,,0.8656523,,0.0013423256,0.0013363075,91821.195,16.62696
xl,570000,,-0.97981083,168468.89,-0.85942674,2048.0063,2048.0063,168493.6,-0.85955274,-0.8581062,1.0016857,0.9920747,1.0016857,1.2845223,10142.219,20480000.0,2019.282,15.7756405,0.98597753,-193275.98,1798.423,24.095133,-0.00012291885,1.0386818564985976,,,0.8887081,,0.0013303938,0.0013245335,102012.586,17.34763
xl,580000,,-0.9800036,168264.95,-0.8583864,2048.0073,2048.0073,168289.31,-0.8585107,-0.8570725,1.001678,0.992111,1.001678,1.2832172,10142.17,20480000.0,2019.2916,15.775716,0.98598224,-193276.9,1798.4316,23.841335,-0.00012162412,1.0386660030224362,,,0.8657901,,0.0013187655,0.0013130654,112204.72,16.384674
xl,590000,,-0.9803157,167956.19,-0.8568113,2048.0068,2048.0068,167980.05,-0.85693294,-0.85549176,1.0016847,0.99208045,1.0016847,1.2843195,10142.141,20480000.0,2019.2976,15.775763,0.98598516,-193277.47,1798.437,23.596622,-0.00012037575,1.0386375485075021,,,1.1585333,,0.0013074477,0.0013018901,122395.81,16.550411
xl,600000,,-0.9804774,167787.42,-0.8559503,2048.0059,2048.0059,167811.34,-0.85607237,-0.8546387,1.0016775,0.99211365,1.0016775,1.2831261,10142.184,20480000.0,2019.289,15.775695,0.9859809,-193276.66,1798.4292,23.324768,-0.00011898891,1.0386116735477606,,,0.8885598,,0.0012964226,0.0012909955,132587.4,18.862782
xl,610000,,-0.9807709,167459.19,-0.8542758,2048.0068,2048.0068,167481.73,-0.85439086,-0.8529556,1.0016828,0.9920893,1.0016828,1.2840015,10142.214,20480000.0,2019.2831,15.775649,0.98597807,-193276.08,1798.424,23.248783,-0.00011860128,1.0385028634012474,,,0.8572478,,0.0012856664,0.0012803698,142781.06,16.874521
xl,620000,,-0.98109347,167177.33,-0.852838,2048.0068,2048.0068,167199.95,-0.85295343,-0.8515257,1.0016767,0.9921179,1.0016767,1.2829745,10142.161,20480000.0,2019.2935,15.77573,0.98598313,-193277.08,1798.4332,22.766176,-0.00011613931,1.0384246099497545,,,0.8553722,,0.0012751668,0.0012700022,152972.67,15.769588
xl,630000,,-0.98079693,167456.92,-0.85426426,2048.0059,2048.0059,167479.69,-0.8543804,-0.8529388,1.0016903,0.99205416,1.0016903,1.285264,10142.205,20480000.0,2019.2847,15.775661,0.98597884,-193276.23,1798.4254,22.587048,-0.000115225506,1.0383754819559887,,,0.8589728,,0.0012649208,0.0012598827,163163.19,17.77734
xl,640000,,-0.98077774,167392.36,-0.85393494,2048.0063,2048.0063,167414.66,-0.85404867,-0.8526089,1.0016886,0.99206203,1.0016886,1.2849821,10142.209,20480000.0,2019.2839,15.775656,0.9859785,-193276.17,1798.4247,22.453915,-0.00011454634,1.038419626792063,,,1.1615965,,0.0012549129,0.0012500009,173355.72,17.100193
xl,650000,,-0.98105603,167150.95,-0.85270345,2048.0095,2048.0095,167173.2,-0.85281694,-0.85138315,1.0016841,0.9920823,1.0016841,1.2842463,10142.148,20480000.0,2019.296,15.77575,0.9859844,-193277.31,1798.4355,22.30004,-0.00011376136,1.0383669276623266,,,0.84075356,,0.0012451629,0.0012403483,183547.8,17.465576
xl,660000,,-0.9813271,166914.23,-0.8514958,2048.0068,2048.0068,166936.61,-0.85160995,-0.8501758,1.0016869,0.99207014,1.0016869,1.2846931,10142.197,20480000.0,2019.2863,15.775674,0.9859796,-193276.39,1798.4269,22.136593,-0.000112927555,1.03829303622851,,,0.8648014,,0.0012356176,0.0012309158,193739.98,16.94054
xl,670000,,-0.98161477,166601.69,-0.8499014,2048.0068,2048.0068,166623.08,-0.8500105,-0.84858024,1.0016855,0.9920762,1.0016855,1.2844708,10142.211,20480000.0,2019.2836,15.775653,0.9859783,-193276.14,1798.4244,21.957499,-0.00011201393,1.0382609424588352,,,0.8477082,,0.0012262804,0.0012216953,203931.69,17.96322
xl,680000,,-0.9818427,166355.05,-0.8486432,2048.0068,2048.0068,166376.42,-0.8487522,-0.8473293,1.0016793,0.992105,1.0016793,1.2834346,10142.145,20480000.0,2019.2968,15.775756,0.98598474,-193277.39,1798.4362,21.773764,-0.00011107662,1.0382175053582512,,,0.8598542,,0.0012171788,0.001212679,214124.34,15.067378
xl,690000,,-0.9820971,166061.72,-0.8471468,2048.007,2048.007,166083.28,-0.84725684,-0.84583056,1.0016862,0.9920726,1.0016862,1.2845988,10142.182,20480000.0,2019.2894,15.775699,0.98598117,-193276.69,1798.4297,21.614079,-0.00011026201,1.0380797851224852,,,0.85716724,,0.0012082481,0.0012038593,224314.17,16.3726
xl,700000,,-0.98203385,166123.47,-0.8474618,2048.0068,2048.0068,166144.81,-0.8475707,-0.8461493,1.0016798,0.99210256,1.0016798,1.2835222,10142.204,20480000.0,2019.2848,15.775662,0.9859789,-193276.25,1798.4255,21.483166,-0.00010959417,1.038229817916529,,,0.88252425,,0.0011995289,0.0011952295,234505.28,17.723316
xl,710000,,-0.98223233,165910.0,-0.84637284,2048.0068,2048.0068,165931.23,-0.84648114,-0.8450544,1.0016884,0.9920629,1.0016884,1.2849476,10142.141,20480000.0,2019.2976,15.775763,0.98598516,-193277.47,1798.437,21.334768,-0.00010883713,1.0380690355466435,,,0.8462155,,0.0011909944,0.0011867824,244697.75,15.247545
xl,720000,,-0.9825582,165546.98,-0.844521,2048.0056,2048.0056,165567.97,-0.844628,-0.8432049,1.0016878,0.99206597,1.0016878,1.2848383,10142.162,20480000.0,2019.2933,15.775729,0.9859831,-193277.06,1798.4331,21.144516,-0.00010786658,1.038077143741093,,,0.854841,,0.0011826245,0.0011785121,254887.69,15.685025
xl,730000,,-0.9828178,165303.45,-0.8432786,2048.0059,2048.0059,165324.22,-0.8433845,-0.8419722,1.0016774,0.9921143,1.0016774,1.2831041,10142.151,20480000.0,2019.2954,15.775745,0.9859841,-193277.27,1798.4349,21.007277,-0.00010716646,1.0380751714472212,,,0.8603296,,0.0011744511,0.0011704123,265078.06,17.36555
xl,740000,,-0.9829519,165181.02,-0.842654,2048.0078,2048.0078,165201.98,-0.8427609,-0.841348,1.0016794,0.99210495,1.0016794,1.2834389,10142.149,20480000.0,2019.2959,15.775749,0.9859843,-193277.31,1798.4353,20.829653,-0.00010626033,1.0380308367926923,,,0.8632946,,0.0011664189,0.0011624771,275270.16,17.071726
xl,1010000,,-0.98594135,162111.23,-0.8269938,2048.0083,2048.0083,162129.45,-0.82708675,-0.825693,1.001688,0.9920648,1.001688,1.2848809,9930.304,20480000.0,2062.374,16.112297,1.0070186,-197400.53,1836.8018,18.002296,-9.183687e-05,1.0371564762215004,,,0.92396784,,0.0009975114,0.0009950376,10083.199,372.2471
xl,1020000,,-0.98556215,162389.66,-0.82841414,2048.0059,2048.0059,162408.16,-0.8285085,-0.8271103,1.0016905,0.9920524,1.0016905,1.2853235,9927.003,20480000.0,2063.0598,16.117655,1.0073534,-197466.19,1837.4126,17.899923,-9.131462e-05,1.0371544581757746,,,0.89677787,,0.000992588,0.000990148,20414.95,371.1518
xl,1030000,,-0.98584145,162116.95,-0.82702297,2048.0068,2048.0068,162134.9,-0.8271146,-0.8257185,1.0016906,0.9920519,1.0016906,1.2853427,9927.022,20480000.0,2063.0557,16.117622,1.0073514,-197465.78,1837.4089,17.834824,-9.098252e-05,1.0371309365671362,,,0.8964815,,0.0009877331,0.0009853297,30745.586,20.491663
xl,1040000,,-0.9860581,161934.22,-0.82609075,2048.008,2048.008,161952.36,-0.8261833,-0.8247985,1.001679,0.9921072,1.001679,1.2833575,9927.013,20480000.0,2063.0576,16.117638,1.0073524,-197465.98,1837.4108,17.729883,-9.044718e-05,1.03717379160987,,,0.90401816,,0.0009829515,0.0009805812,40725.49,370.87628
xl,1050000,,-0.98604417,161892.66,-0.82587874,2048.008,2048.008,161910.38,-0.8259691,-0.8245762,1.0016893,0.992058,1.0016893,1.2851214,9926.993,20480000.0,2063.0618,16.11767,1.0073544,-197466.38,1837.4143,17.64239,-9.000084e-05,1.037118522283804,,,0.9052105,,0.0009782402,0.0009759005,51055.844,17.795202
xl,1060000,,-0.9863795,161607.7,-0.8244251,2048.0076,2048.0076,161625.27,-0.8245147,-0.8231277,1.001685,0.9920785,1.001685,1.28439,9927.008,20480000.0,2063.0588,16.117647,1.007353,-197466.08,1837.4117,17.565783,-8.961003e-05,1.0370486919982451,,,0.8984449,,0.0009735984,0.0009712863,61033.1,370.94116
xl,1070000,,-0.9864918,161428.84,-0.8235127,2048.009,2048.009,161446.38,-0.8236021,-0.8222131,1.0016893,0.9920586,1.0016893,1.2851048,9926.984,20480000.0,2063.0635,16.117683,1.0073552,-197466.53,1837.416,17.502447,-8.9286936e-05,1.0370574328215432,,,0.8982525,,0.0009689927,0.000966737,71363.49,17.15672
xl,1080000,,-0.9862953,161615.56,-0.82446516,2048.007,2048.007,161632.78,-0.824553,-0.82316494,1.0016862,0.9920726,1.0016862,1.2845986,9926.996,20480000.0,2063.061,16.117664,1.007354,-197466.31,1837.4138,17.443344,-8.8985435e-05,1.037035651313589,,,0.921082,,0.0009644823,0.0009622509,81340.06,16.706678
xl,1090000,,-0.9863081,161612.2,-0.82444805,2048.0056,2048.0056,161629.52,-0.8245364,-0.8231479,1.0016868,0.9920701,1.0016868,1.2846893,9927.012,20480000.0,2063.058,16.117641,1.0073526,-197466.02,1837.411,17.352364,-8.85213e-05,1.0370667149173107,,,0.9198127,,0.00096004637,0.0009578268,91316.24,17.637486
xl,1100000,,-0.98662615,161272.3,-0.8227141,2048.0076,2048.0076,161290.19,-0.8228053,-0.8214216,1.0016844,0.9920807,1.0016844,1.2843065,9927.019,20480000.0,2063.0566,16.11763,1.0073519,-197465.88,1837.4097,17.299381,-8.8251014e-05,1.0370480342905928,,,0.9288945,,0.0009556196,0.000953463,101293.39,18.215113
xl,1110000,,-0.9866908,161215.28,-0.82242316,2048.0044,2048.0044,161232.3,-0.82250994,-0.82113016,1.0016804,0.9921,1.0016804,1.2836162,9927.022,20480000.0,2063.0557,16.117622,1.0073514,-197465.78,1837.4089,17.171745,-8.7599896e-05,1.0370405265715996,,,0.916193,,0.00095131237,0.00094915845,111271.06,19.069096
xl,1120000,,-0.9868722,161044.5,-0.821552,2048.0068,2048.0068,161061.92,-0.82164085,-0.82025766,1.0016863,0.99207276,1.0016863,1.2845961,9927.031,20480000.0,2063.054,16.117609,1.0073506,-197465.61,1837.4073,17.087612,-8.71707e-05,1.0370170428145338,,,0.9174359,,0.0009470065,0.00094491156,121249.6,370.81192
xl,1130000,,-0.986911,160991.78,-0.82128304,2048.0059,2048.0059,161009.11,-0.82137144,-0.81999487,1.0016788,0.9921079,1.0016788,1.2833327,9927.061,20480000.0,2063.0479,16.117561,1.0073476,-197465.03,1837.4019,17.007364,-8.676133e-05,1.0370309044346797,,,0.89538646,,0.00094282895,0.0009407213,131579.95,370.47937
xl,1140000,,-0.98705906,160834.34,-0.82047987,2048.0063,2048.0063,160851.58,-0.8205678,-0.8191889,1.0016834,0.9920865,1.0016834,1.2840999,9926.997,20480000.0,2063.061,16.117664,1.007354,-197466.3,1837.4137,16.942486,-8.643035e-05,1.0370307076150727,,,0.9337456,,0.00093864615,0.0009365862,141909.89,35.75077
xl,1150000,25.054787,-0.987161,160782.61,-0.820216,2048.008,2048.008,160799.48,-0.82030207,-0.8189294,1.0016762,0.9921199,1.0016762,1.2829019,9927.025,20480000.0,2063.055,16.117617,1.007351,-197465.72,1837.4084,16.89404,-8.61832e-05,1.0369392821127064,"b""    from __gin__ import dynamic_registration\n    import __main__ as train_script\n    import data.pile.tasks\n    from models.scalable_t5 import network\n    import seqio\n    from t5x import adafactor\n    from t5x import gin_utils\n    from t5x import models\n    from t5x import partitioning\n    from t5x import trainer\n    from t5x import utils\n    \n#### Macros:\n\n    BATCH_SIZE = 2048\n    DROPOUT_RATE = 0.0\n    EVAL_BATCH_SIZE = 256\n    LABEL_SMOOTHING = 0.0\n    LOSS_NORMALIZING_FACTOR = None\n    MIXTURE_OR_TASK_MODULE = None\n    MIXTURE_OR_TASK_NAME = 'pile_r_denoiser'\n    MODEL = @models.EncoderDecoderModel()\n    MODEL_DIR = 'gs://improved-t5/ckpts/v2_xl_mlm/'\n    OPTIMIZER = @adafactor.Adafactor()\n    RANDOM_SEED = None\n    SAVING_PERIOD = 10000\n    SHUFFLE_TRAIN_EXAMPLES = True\n    TASK_FEATURE_LENGTHS = {'inputs': 512, 'targets': 114}\n    TRAIN_STEPS = 2000000\n    USE_CACHED_TASKS = False\n    USE_HARDWARE_RNG = False\n    VOCABULARY = @seqio.SentencePieceVocabulary()\n    Z_LOSS = 0.0001\n    \n#### Parameters for adafactor.Adafactor:\n\n    adafactor.Adafactor.decay_rate = 0.8\n    adafactor.Adafactor.logical_factor_rules = \\\n        @adafactor.standard_logical_factor_rules()\n    adafactor.Adafactor.step_offset = 0\n    \n#### Parameters for utils.CheckpointConfig:\n\n    utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()\n    utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()\n    \n#### Parameters for utils.create_learning_rate_scheduler:\n\n    utils.create_learning_rate_scheduler.base_learning_rate = 1.0\n    utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'\n    utils.create_learning_rate_scheduler.warmup_steps = 10000\n    \n#### Parameters for infer_eval/utils.DatasetConfig:\n\n    infer_eval/utils.DatasetConfig.batch_size = %EVAL_BATCH_SIZE\n    infer_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    infer_eval/utils.DatasetConfig.seed = 0\n    infer_eval/utils.DatasetConfig.shuffle = False\n    infer_eval/utils.DatasetConfig.split = 'validation'\n    infer_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    \n#### Parameters for train/utils.DatasetConfig:\n\n    train/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train/utils.DatasetConfig.pack = True\n    train/utils.DatasetConfig.seed = None\n    train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES\n    train/utils.DatasetConfig.split = 'train'\n    train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for train_eval/utils.DatasetConfig:\n\n    train_eval/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train_eval/utils.DatasetConfig.pack = True\n    train_eval/utils.DatasetConfig.seed = 42\n    train_eval/utils.DatasetConfig.shuffle = False\n    train_eval/utils.DatasetConfig.split = 'validation'\n    train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for models.EncoderDecoderModel:\n\n    models.EncoderDecoderModel.input_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.label_smoothing = %LABEL_SMOOTHING\n    models.EncoderDecoderModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR\n    models.EncoderDecoderModel.module = @network.Transformer()\n    models.EncoderDecoderModel.optimizer_def = %OPTIMIZER\n    models.EncoderDecoderModel.output_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.z_loss = %Z_LOSS\n    \n#### Parameters for seqio.Evaluator:\n\n    seqio.Evaluator.logger_cls = \\\n        [@seqio.PyLoggingLogger, @seqio.TensorBoardLogger, @seqio.JSONLogger]\n    seqio.Evaluator.num_examples = 2048\n    \n#### Parameters for partitioning.PjitPartitioner:\n\n    partitioning.PjitPartitioner.logical_axis_rules = \\\n        @partitioning.standard_logical_axis_rules()\n    partitioning.PjitPartitioner.model_parallel_submesh = (1, 1, 2, 1)\n    partitioning.PjitPartitioner.num_partitions = 1\n    \n#### Parameters for utils.RestoreCheckpointConfig:\n\n    utils.RestoreCheckpointConfig.path = []\n    \n#### Parameters for utils.SaveCheckpointConfig:\n\n    utils.SaveCheckpointConfig.dtype = 'float32'\n    utils.SaveCheckpointConfig.keep = None\n    utils.SaveCheckpointConfig.period = %SAVING_PERIOD\n    utils.SaveCheckpointConfig.save_dataset = False\n    \n#### Parameters for seqio.SentencePieceVocabulary:\n\n    seqio.SentencePieceVocabulary.extra_ids = 100\n    seqio.SentencePieceVocabulary.sentencepiece_model_file = \\\n        'gs://improved-t5/vocabs/tokenizer.model'\n    \n#### Parameters for network.T5Config:\n\n    network.T5Config.dropout_rate = %DROPOUT_RATE\n    network.T5Config.dtype = 'bfloat16'\n    network.T5Config.emb_dim = 2048\n    network.T5Config.head_dim = 64\n    network.T5Config.logits_via_embedding = False\n    network.T5Config.mlp_activations = ('gelu', 'linear')\n    network.T5Config.mlp_dim = 5120\n    network.T5Config.num_decoder_layers = 24\n    network.T5Config.num_encoder_layers = 24\n    network.T5Config.num_heads = 32\n    network.T5Config.remat_policy = 'minimal'\n    network.T5Config.scan_layers = True\n    network.T5Config.vocab_size = 32128\n    \n#### Parameters for train_script.train:\n\n    train_script.train.checkpoint_cfg = @utils.CheckpointConfig()\n    train_script.train.eval_period = %SAVING_PERIOD\n    train_script.train.eval_steps = 100\n    train_script.train.infer_eval_dataset_cfg = @infer_eval/utils.DatasetConfig()\n    train_script.train.inference_evaluator_cls = @seqio.Evaluator\n    train_script.train.model = %MODEL\n    train_script.train.model_dir = %MODEL_DIR\n    train_script.train.partitioner = @partitioning.PjitPartitioner()\n    train_script.train.random_seed = %RANDOM_SEED\n    train_script.train.run_eval_before_training = True\n    train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config\n    train_script.train.total_steps = %TRAIN_STEPS\n    train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()\n    train_script.train.train_eval_dataset_cfg = @train_eval/utils.DatasetConfig()\n    train_script.train.trainer_cls = @trainer.Trainer\n    train_script.train.use_hardware_rng = %USE_HARDWARE_RNG\n    \n#### Parameters for trainer.Trainer:\n\n    trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()\n    trainer.Trainer.num_microbatches = None\n    \n#### Parameters for network.Transformer:\n\n    network.Transformer.config = @network.T5Config()""",13.7947645,11.865018,1.835823e-05,0.0009345193,0.00093250524,151905.11,17.49115
xl,1950000,22.244398,-0.99054,157256.6,-0.8022284,2048.0068,2048.0068,157270.7,-0.80230033,-0.8009393,1.0016993,0.99201185,1.0016993,1.2867836,6047.667,20480000.0,3386.43,13.228242,1.6535302,-324132.8,1508.0195,13.676034,-6.976688e-05,1.0357763609666566,"b""    from __gin__ import dynamic_registration\n    import __main__ as train_script\n    import data.pile.tasks\n    from models.scalable_t5 import network\n    import seqio\n    from t5x import adafactor\n    from t5x import gin_utils\n    from t5x import models\n    from t5x import partitioning\n    from t5x import trainer\n    from t5x import utils\n    \n#### Macros:\n\n    BATCH_SIZE = 2048\n    DROPOUT_RATE = 0.0\n    EVAL_BATCH_SIZE = 256\n    LABEL_SMOOTHING = 0.0\n    LOSS_NORMALIZING_FACTOR = None\n    MIXTURE_OR_TASK_MODULE = None\n    MIXTURE_OR_TASK_NAME = 'pile_r_denoiser'\n    MODEL = @models.EncoderDecoderModel()\n    MODEL_DIR = 'gs://improved-t5/ckpts/v2_xl_mlm/'\n    OPTIMIZER = @adafactor.Adafactor()\n    RANDOM_SEED = None\n    SAVING_PERIOD = 10000\n    SHUFFLE_TRAIN_EXAMPLES = True\n    TASK_FEATURE_LENGTHS = {'inputs': 512, 'targets': 114}\n    TRAIN_STEPS = 2000000\n    USE_CACHED_TASKS = False\n    USE_HARDWARE_RNG = False\n    VOCABULARY = @seqio.SentencePieceVocabulary()\n    Z_LOSS = 0.0001\n    \n#### Parameters for adafactor.Adafactor:\n\n    adafactor.Adafactor.decay_rate = 0.8\n    adafactor.Adafactor.logical_factor_rules = \\\n        @adafactor.standard_logical_factor_rules()\n    adafactor.Adafactor.step_offset = 0\n    \n#### Parameters for utils.CheckpointConfig:\n\n    utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()\n    utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()\n    \n#### Parameters for utils.create_learning_rate_scheduler:\n\n    utils.create_learning_rate_scheduler.base_learning_rate = 1.0\n    utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'\n    utils.create_learning_rate_scheduler.warmup_steps = 10000\n    \n#### Parameters for infer_eval/utils.DatasetConfig:\n\n    infer_eval/utils.DatasetConfig.batch_size = %EVAL_BATCH_SIZE\n    infer_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    infer_eval/utils.DatasetConfig.seed = 0\n    infer_eval/utils.DatasetConfig.shuffle = False\n    infer_eval/utils.DatasetConfig.split = 'validation'\n    infer_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    \n#### Parameters for train/utils.DatasetConfig:\n\n    train/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train/utils.DatasetConfig.pack = True\n    train/utils.DatasetConfig.seed = None\n    train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES\n    train/utils.DatasetConfig.split = 'train'\n    train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for train_eval/utils.DatasetConfig:\n\n    train_eval/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train_eval/utils.DatasetConfig.pack = True\n    train_eval/utils.DatasetConfig.seed = 42\n    train_eval/utils.DatasetConfig.shuffle = False\n    train_eval/utils.DatasetConfig.split = 'validation'\n    train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for models.EncoderDecoderModel:\n\n    models.EncoderDecoderModel.input_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.label_smoothing = %LABEL_SMOOTHING\n    models.EncoderDecoderModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR\n    models.EncoderDecoderModel.module = @network.Transformer()\n    models.EncoderDecoderModel.optimizer_def = %OPTIMIZER\n    models.EncoderDecoderModel.output_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.z_loss = %Z_LOSS\n    \n#### Parameters for seqio.Evaluator:\n\n    seqio.Evaluator.logger_cls = \\\n        [@seqio.PyLoggingLogger, @seqio.TensorBoardLogger, @seqio.JSONLogger]\n    seqio.Evaluator.num_examples = 2048\n    \n#### Parameters for partitioning.PjitPartitioner:\n\n    partitioning.PjitPartitioner.logical_axis_rules = \\\n        @partitioning.standard_logical_axis_rules()\n    partitioning.PjitPartitioner.model_parallel_submesh = None\n    partitioning.PjitPartitioner.num_partitions = 2\n    \n#### Parameters for utils.RestoreCheckpointConfig:\n\n    utils.RestoreCheckpointConfig.path = []\n    \n#### Parameters for utils.SaveCheckpointConfig:\n\n    utils.SaveCheckpointConfig.dtype = 'float32'\n    utils.SaveCheckpointConfig.keep = None\n    utils.SaveCheckpointConfig.period = %SAVING_PERIOD\n    utils.SaveCheckpointConfig.save_dataset = False\n    \n#### Parameters for seqio.SentencePieceVocabulary:\n\n    seqio.SentencePieceVocabulary.extra_ids = 100\n    seqio.SentencePieceVocabulary.sentencepiece_model_file = \\\n        'gs://improved-t5/vocabs/tokenizer.model'\n    \n#### Parameters for network.T5Config:\n\n    network.T5Config.dropout_rate = %DROPOUT_RATE\n    network.T5Config.dtype = 'bfloat16'\n    network.T5Config.emb_dim = 2048\n    network.T5Config.head_dim = 64\n    network.T5Config.logits_via_embedding = False\n    network.T5Config.mlp_activations = ('gelu', 'linear')\n    network.T5Config.mlp_dim = 5120\n    network.T5Config.num_decoder_layers = 24\n    network.T5Config.num_encoder_layers = 24\n    network.T5Config.num_heads = 32\n    network.T5Config.remat_policy = 'minimal'\n    network.T5Config.scan_layers = True\n    network.T5Config.vocab_size = 32128\n    \n#### Parameters for train_script.train:\n\n    train_script.train.checkpoint_cfg = @utils.CheckpointConfig()\n    train_script.train.eval_period = %SAVING_PERIOD\n    train_script.train.eval_steps = 100\n    train_script.train.infer_eval_dataset_cfg = @infer_eval/utils.DatasetConfig()\n    train_script.train.inference_evaluator_cls = @seqio.Evaluator\n    train_script.train.model = %MODEL\n    train_script.train.model_dir = %MODEL_DIR\n    train_script.train.partitioner = @partitioning.PjitPartitioner()\n    train_script.train.random_seed = %RANDOM_SEED\n    train_script.train.run_eval_before_training = True\n    train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config\n    train_script.train.total_steps = %TRAIN_STEPS\n    train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()\n    train_script.train.train_eval_dataset_cfg = @train_eval/utils.DatasetConfig()\n    train_script.train.trainer_cls = @trainer.Trainer\n    train_script.train.use_hardware_rng = %USE_HARDWARE_RNG\n    \n#### Parameters for trainer.Trainer:\n\n    trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()\n    trainer.Trainer.num_microbatches = None\n    \n#### Parameters for network.Transformer:\n\n    network.Transformer.config = @network.T5Config()""",32.037136,0.8341563,1.3828278e-05,0.00071702193,0.00071611506,55253.652,25.861855
xl,1960000,,-0.99014896,157732.98,-0.80465865,2048.007,2048.007,157747.14,-0.80473083,-0.8033617,1.0017042,0.99198794,1.0017042,1.2876377,6063.0557,20480000.0,3377.835,13.194668,1.6493335,-323310.12,1504.1921,13.746493,-7.012633e-05,1.0358147812750953,,,0.8238752,,0.0007151901,0.0007142859,6199.7905,26.548052
xl,1970000,,-0.9898625,157926.52,-0.8056459,2048.007,2048.007,157940.27,-0.805716,-0.80435014,1.0016981,0.99201655,1.0016981,1.2866104,6061.8804,20480000.0,3378.4895,13.197225,1.6496531,-323372.8,1504.4836,13.718604,-6.998405e-05,1.0357750061119455,,,1.3649127,,0.0007133783,0.0007124707,12305.973,24.681602
xl,1980000,24.190514,-0.9903011,157529.0,-0.803618,2048.0066,2048.0066,157542.58,-0.8036873,-0.8023271,1.0016953,0.9920303,1.0016953,1.2861193,6061.8413,20480000.0,3378.5115,13.19731,1.6496638,-323374.9,1504.4933,13.672372,-6.9748196e-05,1.035777857333587,"b""    from __gin__ import dynamic_registration\n    import __main__ as train_script\n    import data.pile.tasks\n    from models.scalable_t5 import network\n    import seqio\n    from t5x import adafactor\n    from t5x import gin_utils\n    from t5x import models\n    from t5x import partitioning\n    from t5x import trainer\n    from t5x import utils\n    \n#### Macros:\n\n    BATCH_SIZE = 2048\n    DROPOUT_RATE = 0.0\n    EVAL_BATCH_SIZE = 256\n    LABEL_SMOOTHING = 0.0\n    LOSS_NORMALIZING_FACTOR = None\n    MIXTURE_OR_TASK_MODULE = None\n    MIXTURE_OR_TASK_NAME = 'pile_r_denoiser'\n    MODEL = @models.EncoderDecoderModel()\n    MODEL_DIR = 'gs://improved-t5/ckpts/v2_xl_mlm/'\n    OPTIMIZER = @adafactor.Adafactor()\n    RANDOM_SEED = None\n    SAVING_PERIOD = 10000\n    SHUFFLE_TRAIN_EXAMPLES = True\n    TASK_FEATURE_LENGTHS = {'inputs': 512, 'targets': 114}\n    TRAIN_STEPS = 2000000\n    USE_CACHED_TASKS = False\n    USE_HARDWARE_RNG = False\n    VOCABULARY = @seqio.SentencePieceVocabulary()\n    Z_LOSS = 0.0001\n    \n#### Parameters for adafactor.Adafactor:\n\n    adafactor.Adafactor.decay_rate = 0.8\n    adafactor.Adafactor.logical_factor_rules = \\\n        @adafactor.standard_logical_factor_rules()\n    adafactor.Adafactor.step_offset = 0\n    \n#### Parameters for utils.CheckpointConfig:\n\n    utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()\n    utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()\n    \n#### Parameters for utils.create_learning_rate_scheduler:\n\n    utils.create_learning_rate_scheduler.base_learning_rate = 1.0\n    utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'\n    utils.create_learning_rate_scheduler.warmup_steps = 10000\n    \n#### Parameters for infer_eval/utils.DatasetConfig:\n\n    infer_eval/utils.DatasetConfig.batch_size = %EVAL_BATCH_SIZE\n    infer_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    infer_eval/utils.DatasetConfig.seed = 0\n    infer_eval/utils.DatasetConfig.shuffle = False\n    infer_eval/utils.DatasetConfig.split = 'validation'\n    infer_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    \n#### Parameters for train/utils.DatasetConfig:\n\n    train/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train/utils.DatasetConfig.pack = True\n    train/utils.DatasetConfig.seed = None\n    train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES\n    train/utils.DatasetConfig.split = 'train'\n    train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for train_eval/utils.DatasetConfig:\n\n    train_eval/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train_eval/utils.DatasetConfig.pack = True\n    train_eval/utils.DatasetConfig.seed = 42\n    train_eval/utils.DatasetConfig.shuffle = False\n    train_eval/utils.DatasetConfig.split = 'validation'\n    train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for models.EncoderDecoderModel:\n\n    models.EncoderDecoderModel.input_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.label_smoothing = %LABEL_SMOOTHING\n    models.EncoderDecoderModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR\n    models.EncoderDecoderModel.module = @network.Transformer()\n    models.EncoderDecoderModel.optimizer_def = %OPTIMIZER\n    models.EncoderDecoderModel.output_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.z_loss = %Z_LOSS\n    \n#### Parameters for seqio.Evaluator:\n\n    seqio.Evaluator.logger_cls = \\\n        [@seqio.PyLoggingLogger, @seqio.TensorBoardLogger, @seqio.JSONLogger]\n    seqio.Evaluator.num_examples = 2048\n    \n#### Parameters for partitioning.PjitPartitioner:\n\n    partitioning.PjitPartitioner.logical_axis_rules = \\\n        @partitioning.standard_logical_axis_rules()\n    partitioning.PjitPartitioner.model_parallel_submesh = None\n    partitioning.PjitPartitioner.num_partitions = 1\n    \n#### Parameters for utils.RestoreCheckpointConfig:\n\n    utils.RestoreCheckpointConfig.path = []\n    \n#### Parameters for utils.SaveCheckpointConfig:\n\n    utils.SaveCheckpointConfig.dtype = 'float32'\n    utils.SaveCheckpointConfig.keep = None\n    utils.SaveCheckpointConfig.period = %SAVING_PERIOD\n    utils.SaveCheckpointConfig.save_dataset = False\n    \n#### Parameters for seqio.SentencePieceVocabulary:\n\n    seqio.SentencePieceVocabulary.extra_ids = 100\n    seqio.SentencePieceVocabulary.sentencepiece_model_file = \\\n        'gs://improved-t5/vocabs/tokenizer.model'\n    \n#### Parameters for network.T5Config:\n\n    network.T5Config.dropout_rate = %DROPOUT_RATE\n    network.T5Config.dtype = 'bfloat16'\n    network.T5Config.emb_dim = 2048\n    network.T5Config.head_dim = 64\n    network.T5Config.logits_via_embedding = False\n    network.T5Config.mlp_activations = ('gelu', 'linear')\n    network.T5Config.mlp_dim = 5120\n    network.T5Config.num_decoder_layers = 24\n    network.T5Config.num_encoder_layers = 24\n    network.T5Config.num_heads = 32\n    network.T5Config.remat_policy = 'minimal'\n    network.T5Config.scan_layers = True\n    network.T5Config.vocab_size = 32128\n    \n#### Parameters for train_script.train:\n\n    train_script.train.checkpoint_cfg = @utils.CheckpointConfig()\n    train_script.train.eval_period = %SAVING_PERIOD\n    train_script.train.eval_steps = 100\n    train_script.train.infer_eval_dataset_cfg = @infer_eval/utils.DatasetConfig()\n    train_script.train.inference_evaluator_cls = @seqio.Evaluator\n    train_script.train.model = %MODEL\n    train_script.train.model_dir = %MODEL_DIR\n    train_script.train.partitioner = @partitioning.PjitPartitioner()\n    train_script.train.random_seed = %RANDOM_SEED\n    train_script.train.run_eval_before_training = True\n    train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config\n    train_script.train.total_steps = %TRAIN_STEPS\n    train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()\n    train_script.train.train_eval_dataset_cfg = @train_eval/utils.DatasetConfig()\n    train_script.train.trainer_cls = @trainer.Trainer\n    train_script.train.use_hardware_rng = %USE_HARDWARE_RNG\n    \n#### Parameters for trainer.Trainer:\n\n    trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()\n    trainer.Trainer.num_microbatches = None\n    \n#### Parameters for network.Transformer:\n\n    network.Transformer.config = @network.T5Config()""",32.721107,5.931676,1.5974045e-05,0.00071157695,0.0007106692,18410.82,23.994118
xl,1870000,,-0.9896376,158280.11,-0.80744976,2048.0063,2048.0063,158293.89,-0.80752003,-0.8061526,1.0016962,0.99202484,1.0016962,1.2863103,6048.081,20480000.0,3386.198,13.227336,1.653417,-324110.62,1507.9163,13.99535,-7.1395836e-05,1.0359146941679898,,,0.8245454,,0.00073224993,0.0007312726,6185.4727,27.649652
xl,1880000,,-0.9894012,158405.27,-0.80808824,2048.0063,2048.0063,158419.36,-0.8081601,-0.80678374,1.001706,0.9919802,1.001706,1.2879187,6047.6455,20480000.0,3386.4417,13.228288,1.653536,-324133.94,1508.0249,13.96665,-7.124943e-05,1.0359542882304846,,,0.8262465,,0.0007303058,0.0007293251,12278.558,370.89642
xl,1890000,,-0.9897075,158133.11,-0.8066998,2048.0085,2048.0085,158146.73,-0.8067693,-0.8054062,1.0016924,0.99204296,1.0016924,1.28566,6047.6562,20480000.0,3386.4358,13.228265,1.6535331,-324133.38,1508.0222,13.918897,-7.100582e-05,1.0358953725785205,,,0.8403897,,0.0007283683,0.00072739314,18714.893,24.904491
xl,1900000,,-0.98997265,157855.9,-0.8052857,2048.0068,2048.0068,157869.53,-0.80535525,-0.8039864,1.0017025,0.9919962,1.0017025,1.287343,6047.622,20480000.0,3386.4548,13.228339,1.6535424,-324135.22,1508.0306,13.881323,-7.0814145e-05,1.0358614550486103,,,0.82135963,,0.000726443,0.0007254765,24805.184,25.862402
xl,1910000,,-0.9901085,157715.48,-0.80456936,2048.0068,2048.0068,157729.42,-0.8046404,-0.80327666,1.0016978,0.9920183,1.0016978,1.2865479,6047.6567,20480000.0,3386.4355,13.228264,1.653533,-324133.34,1508.0221,13.838428,-7.059532e-05,1.0358184396820527,,,0.8313,,0.0007245341,0.00072357483,30896.453,23.477362
xl,1920000,,-0.9905109,157348.05,-0.80269486,2048.0088,2048.0088,157362.06,-0.80276644,-0.80140895,1.0016938,0.99203694,1.0016938,1.2858807,6047.6567,20480000.0,3386.4355,13.228264,1.653533,-324133.34,1508.0221,13.776128,-7.02775e-05,1.0358057733035095,,,0.83920383,,0.0007226429,0.00072168803,36985.324,24.49174
xl,1930000,,-0.9905035,157337.84,-0.8026428,2048.0063,2048.0063,157351.62,-0.80271316,-0.80135494,1.0016949,0.99203193,1.0016949,1.2860589,6047.622,20480000.0,3386.4548,13.228339,1.6535424,-324135.22,1508.0306,13.771223,-7.025248e-05,1.0358447383366882,,,0.82202506,,0.00072075875,0.0007198159,43075.215,24.316633
xl,1940000,,-0.9906887,157139.44,-0.80163074,2048.0073,2048.0073,157153.14,-0.8017006,-0.8003448,1.001694,0.9920361,1.001694,1.2859093,6047.6597,20480000.0,3386.434,13.228258,1.6535323,-324133.22,1508.0214,13.70669,-6.992327e-05,1.0358665727685,,,0.83487344,,0.0007188838,0.0007179584,49165.06,23.146788
xl,10000,,-0.72481126,486026.28,-2.479413,2048.007,2048.007,488510.03,-2.4920838,-2.487883,1.0016885,0.9920622,1.0016885,1.2849739,10142.281,20480000.0,2019.2697,15.775544,0.9859715,-193274.8,1798.412,2484.3,-0.012673401,1.0699406727154108,,,0.8447592,,0.010000295,0.01,10290.866,15.794447
xl,20000,,-0.88748145,272948.72,-1.3924199,2048.0068,2048.0068,273519.62,-1.3953323,-1.3929796,1.001689,0.99205935,1.001689,1.285072,10142.227,20480000.0,2019.2805,15.775629,0.9859768,-193275.84,1798.4216,570.10803,-0.0029083476,1.0606152536257074,,,0.8618605,,0.008284418,0.0070712445,20481.404,15.687461
xl,30000,,-0.9085842,247844.66,-1.264354,2048.0054,2048.0054,248135.31,-1.2658368,-1.2637008,1.0016903,0.9920536,1.0016903,1.2852818,10142.232,20480000.0,2019.2793,15.7756195,0.9859762,-193275.72,1798.4207,290.822,-0.0014835985,1.0562206039890898,,,0.86072445,,0.00635681,0.0057735993,30671.887,17.372564
xl,40000,,-0.921299,233220.81,-1.189752,2048.0076,2048.0076,233393.22,-1.1906315,-1.1886362,1.0016787,0.99210835,1.0016787,1.2833163,10142.211,20480000.0,2019.2836,15.775653,0.9859783,-193276.14,1798.4244,172.09619,-0.0008779311,1.0536260069601555,,,0.8420079,,0.005359021,0.0050000623,40864.0,16.763824
xl,50000,,-0.9292623,224006.73,-1.1427473,2048.008,2048.008,224141.4,-1.1434344,-1.1415067,1.0016887,0.9920606,1.0016887,1.2850283,10142.195,20480000.0,2019.2867,15.775678,0.98597986,-193276.44,1798.4272,134.87364,-0.00068804406,1.0515546251177943,,,0.85517144,,0.004721385,0.0044721807,51055.5,15.68714
xl,60000,,-0.93558973,216929.05,-1.1066413,2048.0066,2048.0066,217041.2,-1.1072134,-1.1053511,1.0016848,0.99207914,1.0016848,1.2843636,10142.203,20480000.0,2019.2852,15.775665,0.9859791,-193276.28,1798.4258,112.30871,-0.00057293137,1.0500635700365717,,,0.8607292,,0.0042684553,0.004082517,61245.934,13.797989
xl,70000,,-0.9403329,211606.44,-1.0794885,2048.007,2048.007,211710.44,-1.080019,-1.0782012,1.0016861,0.9920736,1.0016861,1.2845633,10142.215,20480000.0,2019.2828,15.775647,0.98597795,-193276.06,1798.4238,104.01058,-0.0005305993,1.048801311684206,,,0.86509657,,0.003925243,0.0037796716,71434.47,15.365773
xl,80000,,-0.9438691,207604.19,-1.0590714,2048.0068,2048.0068,207694.73,-1.0595334,-1.0577526,1.0016836,0.9920848,1.0016836,1.2841599,10142.225,20480000.0,2019.2809,15.775632,0.985977,-193275.88,1798.4221,90.38423,-0.0004610859,1.047829064227353,,,0.8684659,,0.0036535263,0.003535556,81624.58,15.275057
xl,90000,,-0.9468807,204236.83,-1.0418932,2048.0073,2048.0073,204319.4,-1.0423144,-1.0405596,1.0016863,0.99207187,1.0016863,1.2846256,10142.149,20480000.0,2019.2959,15.775749,0.9859843,-193277.31,1798.4353,81.98814,-0.00041825406,1.0471854331775454,,,0.85918474,,0.0034314673,0.0033333518,91814.57,15.695104
xl,100000,,-0.94992095,200895.5,-1.0248477,2048.0059,2048.0059,200970.78,-1.0252318,-1.0235068,1.0016855,0.9920762,1.0016855,1.2844695,10142.211,20480000.0,2019.2836,15.775653,0.9859783,-193276.14,1798.4244,75.010216,-0.00038265693,1.0464024507920746,,,0.8462126,,0.0032455528,0.0031622935,102005.0,15.364279
xl,110000,,-0.9522259,198373.39,-1.0119815,2048.008,2048.008,198442.97,-1.0123364,-1.0106419,1.0016766,0.99211764,1.0016766,1.2829813,10142.192,20480000.0,2019.2874,15.775682,0.98598015,-193276.5,1798.4277,69.2769,-0.000353409,1.0458267605013791,,,0.8288269,,0.0030869492,0.0030151273,112195.07,22.620417
xl,120000,,-0.9543503,196049.14,-1.0001246,2048.0078,2048.0078,196113.53,-1.000453,-0.9987725,1.0016826,0.99208987,1.0016826,1.2839818,10142.226,20480000.0,2019.2806,15.77563,0.9859769,-193275.84,1798.4219,64.31887,-0.0003281161,1.0453172741595662,,,0.84959245,,0.0029495438,0.0028867633,122392.42,16.321407
xl,130000,,-0.95599806,194223.69,-0.9908122,2048.008,2048.008,194284.67,-0.99112326,-0.9894625,1.0016785,0.99210924,1.0016785,1.2832844,10142.204,20480000.0,2019.2848,15.775662,0.9859789,-193276.25,1798.4255,61.147087,-0.00031193558,1.0448574472413032,,,0.85331297,,0.0028289973,0.002773512,132583.48,15.101031
xl,140000,,-0.95781046,192230.47,-0.980644,2048.0059,2048.0059,192288.48,-0.9809399,-0.97929144,1.0016834,0.99208623,1.0016834,1.2841108,10142.233,20480000.0,2019.2792,15.775619,0.98597616,-193275.7,1798.4205,57.775585,-0.0002947362,1.044437779462567,,,0.87148094,,0.0027221239,0.002672622,142773.33,14.733263
xl,150000,,-0.95921123,190747.06,-0.9730766,2048.0059,2048.0059,190802.48,-0.9733593,-0.9717258,1.001681,0.99209744,1.001681,1.2837093,10142.2295,20480000.0,2019.2799,15.775624,0.9859765,-193275.78,1798.4211,55.317307,-0.00028219554,1.0441262751232312,,,0.86549735,,0.0026265252,0.0025819975,152962.84,20.608501
xl,160000,,-0.9599675,189913.56,-0.9688245,2048.0083,2048.0083,189966.23,-0.9690932,-0.9674586,1.0016896,0.99205714,1.0016896,1.2851546,10142.173,20480000.0,2019.291,15.775711,0.98598194,-193276.84,1798.4312,53.05083,-0.00027063335,1.0437125625408958,,,0.85332584,,0.0025403376,0.0025000079,163158.16,22.957792
xl,170000,,-0.961037,188674.03,-0.96250117,2048.0076,2048.0076,188724.36,-0.96275795,-0.96113765,1.0016859,0.9920751,1.0016859,1.2845125,10142.176,20480000.0,2019.2906,15.775708,0.98598176,-193276.81,1798.4307,50.801792,-0.00025916012,1.0435330145123465,,,0.8505397,,0.0024621193,0.0024253633,173355.81,14.539906
xl,180000,,-0.96228033,187326.66,-0.9556277,2048.0083,2048.0083,187375.48,-0.95587677,-0.9542682,1.0016856,0.992075,1.0016856,1.2845105,10142.209,20480000.0,2019.2839,15.775656,0.9859785,-193276.17,1798.4247,48.753273,-0.0002487098,1.0432358472064198,,,0.85220623,,0.0023907027,0.0023570291,183545.1,13.712679
xl,190000,,-0.9634439,186137.42,-0.94956094,2048.0056,2048.0056,186183.8,-0.9497975,-0.94820005,1.0016847,0.99208057,1.0016847,1.2843176,10142.216,20480000.0,2019.2826,15.775645,0.9859778,-193276.03,1798.4236,47.076267,-0.00024015474,1.0429247647205997,,,0.861608,,0.0023251679,0.0022941635,193733.53,13.924585
xl,200000,,-0.9643974,185075.47,-0.9441435,2048.008,2048.008,185121.62,-0.9443789,-0.9427924,1.0016828,0.99208903,1.0016828,1.2840106,10142.214,20480000.0,2019.2831,15.775649,0.98597807,-193276.08,1798.424,45.814278,-0.00023371683,1.0426820128874494,,,0.8352463,,0.002264747,0.0022360736,203922.2,13.931416
xl,210000,,-0.96546227,183923.62,-0.93826747,2048.009,2048.009,183968.02,-0.93849397,-0.9369234,1.0016763,0.99211866,1.0016763,1.2829419,10142.172,20480000.0,2019.2914,15.775714,0.9859821,-193276.88,1798.4314,43.99283,-0.00022442489,1.0423722450618274,,,0.8441932,,0.0022087973,0.002182184,214110.88,13.412248
xl,220000,,-0.966308,182975.4,-0.93343025,2048.0063,2048.0063,183019.02,-0.9336527,-0.93208045,1.0016868,0.9920697,1.0016868,1.284703,10142.227,20480000.0,2019.2805,15.775629,0.9859768,-193275.84,1798.4216,42.781685,-0.00021824638,1.04224665202791,,,0.85733724,,0.0021567962,0.002132012,224298.98,13.895521
xl,230000,,-0.9668606,182358.03,-0.9302808,2048.0063,2048.0063,182399.84,-0.93049407,-0.9289343,1.0016791,0.9921059,1.0016791,1.2834026,10142.734,20480000.0,2019.1794,15.774839,0.98592746,-193266.17,1798.3317,41.821957,-0.00021335043,1.0420141151527826,,,0.8518579,,0.0021083185,0.0020851486,234488.14,13.75277
xl,240000,,-0.96768683,181476.95,-0.925786,2048.0068,2048.0068,181517.44,-0.92599255,-0.9244338,1.0016861,0.99207354,1.0016861,1.2845683,10142.212,20480000.0,2019.2833,15.775651,0.9859782,-193276.11,1798.4242,40.273254,-0.00020544988,1.0418371632945609,,,0.86199045,,0.0020629603,0.0020412458,244676.62,13.089963
xl,250000,,-0.9684982,180579.62,-0.92120844,2048.0056,2048.0056,180619.02,-0.92140937,-0.9198577,1.0016869,0.9920697,1.0016869,1.2847031,10142.201,20480000.0,2019.2854,15.775667,0.9859792,-193276.31,1798.426,39.34203,-0.00020069933,1.041603148181146,,,0.8407304,,0.00202042,0.002000004,254864.44,13.038272
xl,260000,,-0.9692842,179750.25,-0.91697747,2048.0068,2048.0068,179789.22,-0.91717625,-0.9156387,1.0016792,0.9921062,1.0016792,1.2833964,10142.196,20480000.0,2019.2865,15.775676,0.98597974,-193276.4,1798.427,38.44096,-0.0001961026,1.0414820944807965,,,0.8517945,,0.001980391,0.001961165,265052.2,12.774878
xl,270000,,-0.969926,179098.2,-0.91365105,2048.008,2048.008,179135.3,-0.9138403,-0.91230613,1.0016816,0.99209476,1.0016816,1.2838058,10142.208,20480000.0,2019.284,15.775657,0.98597854,-193276.17,1798.4249,37.389748,-0.00019073994,1.0413994118793317,,,0.8578651,,0.0019426537,0.0019245044,275239.7,13.12675
xl,280000,,-0.97039723,178540.69,-0.91080695,2048.008,2048.008,178576.77,-0.910991,-0.90946454,1.0016785,0.99210924,1.0016785,1.2832837,10142.501,20480000.0,2019.2258,15.775202,0.9859501,-193270.61,1798.3729,36.624413,-0.00018683568,1.0411672717984566,,,0.84692454,,0.0019070015,0.0018898257,285427.88,14.683627
xl,290000,,-0.97066665,178295.39,-0.9095556,2048.006,2048.006,178331.77,-0.90974116,-0.9082135,1.001682,0.99209213,1.001682,1.2838974,10142.167,20480000.0,2019.2922,15.775721,0.98598254,-193276.97,1798.4321,36.291348,-0.0001851366,1.0410266094247784,,,0.83623266,,0.001873255,0.0018569566,295617.25,18.161077
xl,300000,,-0.9708836,178146.92,-0.9087983,2048.007,2048.007,178181.53,-0.90897477,-0.9074456,1.0016851,0.9920782,1.0016851,1.2844007,10142.193,20480000.0,2019.2871,15.775681,0.98598003,-193276.47,1798.4275,35.104836,-0.00017908371,1.040902587311013,,,0.8653178,,0.0018412152,0.001825745,305810.12,14.735535
xl,310000,,-0.9712422,177705.12,-0.90654445,2048.0068,2048.0068,177740.42,-0.9067245,-0.9052036,1.0016803,0.9921002,1.0016803,1.2836043,10143.234,20480000.0,2019.0798,15.774061,0.9858788,-193256.62,1798.2429,34.427826,-0.00017563,1.0407861563343952,,,0.8684828,,0.0018107702,0.0017960559,316000.62,14.868103
xl,320000,,-0.97165126,177183.66,-0.9038842,2048.007,2048.007,177216.73,-0.904053,-0.90252507,1.001693,0.99204195,1.001693,1.2857058,10142.176,20480000.0,2019.2906,15.775708,0.98598176,-193276.81,1798.4307,33.702904,-0.0001719319,1.040611135561417,,,0.84199786,,0.001781797,0.0017677698,326190.2,15.98136
xl,330000,,-0.9721832,176630.14,-0.9010605,2048.008,2048.008,176663.97,-0.9012331,-0.89971745,1.0016845,0.9920802,1.0016845,1.284326,10142.22,20480000.0,2019.2819,15.77564,0.9859775,-193275.97,1798.4229,33.123547,-0.00016897637,1.0404950271309406,,,0.8403959,,0.0017541745,0.0017407791,336380.9,15.428511
xl,340000,,-0.97279906,176029.11,-0.8979944,2048.008,2048.008,176061.39,-0.89815915,-0.8966476,1.0016857,0.9920749,1.0016857,1.2845165,10142.191,20480000.0,2019.2875,15.775683,0.9859802,-193276.5,1798.428,32.57707,-0.00016618856,1.0404257196119273,,,0.84605956,,0.0017277872,0.0017149884,346571.03,14.745711
xl,350000,,-0.9732891,175510.2,-0.8953473,2048.0068,2048.0068,175541.39,-0.8955063,-0.89399844,1.0016867,0.99207085,1.0016867,1.2846658,10142.217,20480000.0,2019.2825,15.775644,0.98597777,-193276.02,1798.4235,32.027016,-0.00016338253,1.0403133368411461,,,0.84400773,,0.001702562,0.0016903109,356760.5,14.80155
xl,360000,,-0.97390157,174806.78,-0.89175886,2048.0059,2048.0059,174838.1,-0.8919186,-0.8904152,1.0016885,0.9920628,1.0016885,1.2849537,10142.155,20480000.0,2019.2947,15.77574,0.9859837,-193277.19,1798.4343,31.46649,-0.00016052306,1.0402449005829957,,,0.8483579,,0.001678393,0.001666669,366950.0,15.300888
xl,370000,,-0.97421557,174457.11,-0.889975,2048.0068,2048.0068,174487.62,-0.8901307,-0.8886314,1.0016872,0.99206907,1.0016872,1.2847323,10142.199,20480000.0,2019.2859,15.775671,0.98597944,-193276.34,1798.4265,30.847233,-0.00015736399,1.040072980113488,,,0.8419428,,0.0016552461,0.001643992,377140.03,15.456884
xl,380000,,-0.97425103,174385.88,-0.88961166,2048.008,2048.008,174416.22,-0.8897664,-0.88826317,1.0016924,0.99204445,1.0016924,1.2856134,10142.219,20480000.0,2019.282,15.7756405,0.98597753,-193275.98,1798.423,30.373098,-0.00015494524,1.0399848886392196,,,0.8728852,,0.0016330457,0.0016222164,387330.25,15.052973
xl,390000,,-0.9749061,173687.78,-0.88605034,2048.0068,2048.0068,173717.52,-0.88620204,-0.884704,1.0016932,0.9920401,1.0016932,1.2857689,10142.183,20480000.0,2019.2893,15.775698,0.9859811,-193276.67,1798.4296,29.985655,-0.00015296874,1.039910097165902,,,0.8668456,,0.001611688,0.0016012837,397520.03,17.34965
xl,400000,,-0.9752879,173275.06,-0.883945,2048.0068,2048.0068,173304.6,-0.8840956,-0.88260704,1.0016866,0.9920713,1.0016866,1.284648,10142.228,20480000.0,2019.2804,15.775628,0.98597676,-193275.83,1798.4215,29.525234,-0.00015061995,1.0397729051448257,,,0.8638339,,0.0015911432,0.0015811408,407712.16,17.738256
xl,410000,,-0.97575516,172819.66,-0.8816217,2048.0068,2048.0068,172848.83,-0.88177055,-0.88029015,1.0016817,0.99209356,1.0016817,1.2838447,10142.207,20480000.0,2019.2844,15.77566,0.9859787,-193276.2,1798.4252,29.174328,-0.00014882983,1.039722021104144,,,0.84727955,,0.0015713765,0.0015617395,417904.66,15.855498
xl,420000,,-0.97609144,172443.17,-0.87970114,2048.0068,2048.0068,172471.23,-0.87984425,-0.87836194,1.0016876,0.9920666,1.0016876,1.2848164,10142.17,20480000.0,2019.2916,15.775716,0.98598224,-193276.9,1798.4316,28.518415,-0.00014548376,1.0397206103358954,,,0.8583336,,0.0015523254,0.0015430354,428095.22,25.134449
xl,430000,,-0.9764216,172075.38,-0.87782484,2048.0083,2048.0083,172103.3,-0.87796724,-0.8764908,1.0016845,0.9920804,1.0016845,1.2843188,10142.207,20480000.0,2019.2844,15.77566,0.9859787,-193276.2,1798.4252,28.14644,-0.00014358618,1.0395883576262681,,,0.86247826,,0.0015339577,0.0015249875,438295.1,14.105903
xl,440000,,-0.9768431,171640.89,-0.8756084,2048.008,2048.008,171669.1,-0.8757522,-0.8742839,1.0016794,0.9921043,1.0016794,1.2834593,10142.492,20480000.0,2019.2275,15.775215,0.98595095,-193270.77,1798.3746,27.827005,-0.00014195661,1.039525201495982,,,0.8641875,,0.0015162186,0.0015075585,448484.22,14.41836
xl,450000,,-0.9768195,171709.11,-0.87595636,2048.0063,2048.0063,171736.47,-0.87609595,-0.87462384,1.0016831,0.9920873,1.0016831,1.2840723,10142.166,20480000.0,2019.2924,15.775722,0.9859826,-193276.98,1798.4323,27.535667,-0.00014047038,1.0394428402813727,,,0.8418944,,0.0014990865,0.0014907136,458673.34,14.312069
xl,460000,,-0.9768256,171644.6,-0.8756273,2048.007,2048.007,171671.67,-0.87576544,-0.87428886,1.0016888,0.9920602,1.0016888,1.2850451,10142.17,20480000.0,2019.2916,15.775716,0.98598224,-193276.9,1798.4316,27.184153,-0.00013867716,1.0393307798347569,,,0.8576088,,0.0014825218,0.0014744212,468862.3,14.257998
xl,1160000,,-0.9869645,161000.3,-0.8213265,2048.008,2048.008,161017.58,-0.82141465,-0.82003015,1.0016884,0.99206305,1.0016884,1.2849433,9931.556,20480000.0,2062.114,16.110266,1.0068916,-197375.66,1836.5703,16.948643,-8.646176e-05,1.0369198206889518,,,0.9082203,,0.00093050546,0.0009284771,10083.272,15.647011
xl,1170000,,-0.98658884,161280.61,-0.82275647,2048.009,2048.009,161297.11,-0.82284063,-0.821455,1.0016868,0.9920696,1.0016868,1.2847048,9926.857,20480000.0,2063.0898,16.11789,1.0073681,-197469.06,1837.4395,16.828989,-8.585136e-05,1.036878319128007,,,0.8928499,,0.00092649757,0.0009245007,20058.297,370.3396
xl,1180000,,-0.9868177,161073.48,-0.8216998,2048.0059,2048.0059,161090.44,-0.82178634,-0.82040036,1.0016894,0.9920576,1.0016894,1.2851365,9926.902,20480000.0,2063.0806,16.117817,1.0073636,-197468.17,1837.4313,16.781445,-8.5608815e-05,1.0368698107066787,,,0.8928282,,0.00092254323,0.00092057505,30387.984,371.08194
xl,1190000,,-0.98711395,160825.73,-0.820436,2048.0076,2048.0076,160842.12,-0.82051957,-0.819143,1.0016805,0.99209976,1.0016805,1.2836239,9926.884,20480000.0,2063.0845,16.117847,1.0073655,-197468.53,1837.4346,16.734507,-8.536937e-05,1.0368411972458553,,,0.9133105,,0.0009186377,0.0009166989,40718.348,371.07684
xl,1200000,,-0.9872135,160669.75,-0.8196402,2048.0068,2048.0068,160686.83,-0.81972736,-0.8183436,1.001691,0.99205035,1.001691,1.2853966,9926.888,20480000.0,2063.0837,16.117842,1.0073651,-197468.47,1837.4338,16.637308,-8.487352e-05,1.0368244064277616,,,0.89561224,,0.0009147812,0.0009128713,51048.71,370.78012
xl,1210000,,-0.98744017,160486.39,-0.8187048,2048.006,2048.006,160502.78,-0.81878847,-0.817411,1.0016851,0.99207795,1.0016851,1.2844076,9926.87,20480000.0,2063.0872,16.117868,1.0073668,-197468.81,1837.437,16.55416,-8.444934e-05,1.036840993945042,,,0.938406,,0.0009109817,0.00090909126,61378.836,15.196424
xl,1220000,,-0.98756784,160284.97,-0.81767726,2048.0068,2048.0068,160301.69,-0.8177626,-0.81638235,1.0016907,0.992052,1.0016907,1.2853421,9926.878,20480000.0,2063.0857,16.117857,1.0073661,-197468.66,1837.4358,16.514488,-8.424697e-05,1.03671394066407,,,0.92344594,,0.00090723217,0.0009053578,71353.38,15.740917
xl,1230000,,-0.98744816,160413.23,-0.8183316,2048.0083,2048.0083,160429.75,-0.8184159,-0.8170383,1.001686,0.9920735,1.001686,1.2845669,9926.861,20480000.0,2063.089,16.117884,1.0073677,-197468.98,1837.4388,16.454493,-8.39409e-05,1.036736595548099,,,0.9099469,,0.0009035372,0.0009016701,81328.49,16.03135
xl,1990000,,-0.9910076,156815.4,-0.7999777,2048.008,2048.008,156829.33,-0.8000487,-0.7986974,1.0016919,0.99204606,1.0016919,1.2855521,4953.484,20480000.0,4134.464,16.15025,2.0187812,-395731.03,1841.1285,13.637145,-6.9568494e-05,1.0358043131638381,,,0.88074446,,0.00070977944,0.0007088814,5103.42,24.859346
xl,2000000,,-0.9908163,156942.31,-0.8006251,2048.0085,2048.0085,156956.42,-0.8006971,-0.79934925,1.0016861,0.992073,1.0016861,1.2845849,4976.729,20480000.0,4115.153,16.074816,2.009352,-393882.66,1832.5289,13.613112,-6.9445894e-05,1.03579441444418,,,0.86980724,,0.0007079785,0.00070710696,10116.362,22.55346
